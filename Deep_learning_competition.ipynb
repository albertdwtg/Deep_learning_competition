{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "705954-Albert-DeWatrigant.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "92qjV9yy_z45"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import EarlyStopping  \n",
        "from keras.models import load_model\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "SDU5YkADB6Iz",
        "outputId": "e376c3bb-9938-4aaa-ba93-d8ef20b49603"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6402122a-53a6-4ce5-899d-a50e494fbd93\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6402122a-53a6-4ce5-899d-a50e494fbd93\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving heart_train_4.txt to heart_train_4.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('heart_train_4.txt', sep=\" \", header=None)"
      ],
      "metadata": {
        "id": "97yl4U-jBKUF"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns = [\"age\", \"sex\", \"chest_pain_type\", \"resting_blood_pressure\",\"serum_cholesteral\",\n",
        "                \"blood_sugar\",\"electro_results\",\"max_heart_rate\",\"angina\",\"oldpeak\",\"slope_peak\",\n",
        "                \"major_vessels\",\"thal\",\"disease\"]"
      ],
      "metadata": {
        "id": "Gn-FmEUOCPLu"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "R7DRvIBJCEES",
        "outputId": "792ffdc9-7deb-443d-c9a2-7d9a5ee54f4b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-70afd83d-6465-4196-9e1e-72b71a5a1a73\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>chest_pain_type</th>\n",
              "      <th>resting_blood_pressure</th>\n",
              "      <th>serum_cholesteral</th>\n",
              "      <th>blood_sugar</th>\n",
              "      <th>electro_results</th>\n",
              "      <th>max_heart_rate</th>\n",
              "      <th>angina</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope_peak</th>\n",
              "      <th>major_vessels</th>\n",
              "      <th>thal</th>\n",
              "      <th>disease</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>71.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>265.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>55.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>180.0</td>\n",
              "      <td>327.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>42.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>136.0</td>\n",
              "      <td>315.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.8</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>59.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>178.0</td>\n",
              "      <td>270.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>145.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.2</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>67.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>152.0</td>\n",
              "      <td>277.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>172.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-70afd83d-6465-4196-9e1e-72b71a5a1a73')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-70afd83d-6465-4196-9e1e-72b71a5a1a73 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-70afd83d-6465-4196-9e1e-72b71a5a1a73');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    age  sex  chest_pain_type  ...  major_vessels  thal  disease\n",
              "0  71.0  0.0              3.0  ...            1.0   3.0        1\n",
              "1  55.0  0.0              4.0  ...            0.0   3.0        2\n",
              "2  42.0  1.0              4.0  ...            0.0   6.0        2\n",
              "3  59.0  1.0              1.0  ...            0.0   7.0        1\n",
              "4  67.0  0.0              3.0  ...            1.0   3.0        1\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"disease\"]=data[\"disease\"].replace(2,0)"
      ],
      "metadata": {
        "id": "Rb9VqjOCEJCu"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "on remplace les 2 par des 0 pour pouvoir utiliser la sigmoid"
      ],
      "metadata": {
        "id": "dLz9XtgJaRuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "IzjW7b43EVZA",
        "outputId": "fa78df07-8bd0-45b2-8961-220460bdb883"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-3ff13620-2abe-4207-95b2-8369e1038721\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>chest_pain_type</th>\n",
              "      <th>resting_blood_pressure</th>\n",
              "      <th>serum_cholesteral</th>\n",
              "      <th>blood_sugar</th>\n",
              "      <th>electro_results</th>\n",
              "      <th>max_heart_rate</th>\n",
              "      <th>angina</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope_peak</th>\n",
              "      <th>major_vessels</th>\n",
              "      <th>thal</th>\n",
              "      <th>disease</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>71.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>265.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>55.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>180.0</td>\n",
              "      <td>327.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>42.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>136.0</td>\n",
              "      <td>315.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.8</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>59.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>178.0</td>\n",
              "      <td>270.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>145.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.2</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>67.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>152.0</td>\n",
              "      <td>277.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>172.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3ff13620-2abe-4207-95b2-8369e1038721')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3ff13620-2abe-4207-95b2-8369e1038721 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3ff13620-2abe-4207-95b2-8369e1038721');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    age  sex  chest_pain_type  ...  major_vessels  thal  disease\n",
              "0  71.0  0.0              3.0  ...            1.0   3.0        1\n",
              "1  55.0  0.0              4.0  ...            0.0   3.0        0\n",
              "2  42.0  1.0              4.0  ...            0.0   6.0        0\n",
              "3  59.0  1.0              1.0  ...            0.0   7.0        1\n",
              "4  67.0  0.0              3.0  ...            1.0   3.0        1\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "on normalise les data"
      ],
      "metadata": {
        "id": "mdO2odn5agWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X=data.iloc[:,range(13)]\n",
        "mean = X.mean(axis=0)\n",
        "X -= mean\n",
        "std = X.std(axis=0)\n",
        "X /= std\n",
        "y=data.iloc[:,[13]]\n",
        "trainX=X.values\n",
        "trainy=y.values"
      ],
      "metadata": {
        "id": "081vp1R-DHEr"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VEsrXAGGEFr",
        "outputId": "13ccaf88-a3d7-4d92-d524-b115adce1a1b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.78543536, -1.41093612, -0.17643348, ..., -0.9672943 ,\n",
              "         0.32805277, -0.85461145],\n",
              "       [ 0.05943127, -1.41093612,  0.85355657, ...,  0.65236127,\n",
              "        -0.7295502 , -0.85461145],\n",
              "       [-1.34294705,  0.70546806,  0.85355657, ...,  0.65236127,\n",
              "        -0.7295502 ,  0.69228302],\n",
              "       ...,\n",
              "       [ 1.03030857, -1.41093612,  0.85355657, ...,  0.65236127,\n",
              "         1.38565575, -0.85461145],\n",
              "       [-1.01932128, -1.41093612,  0.85355657, ...,  0.65236127,\n",
              "        -0.7295502 , -0.85461145],\n",
              "       [-0.58782026, -1.41093612,  0.85355657, ..., -0.9672943 ,\n",
              "        -0.7295502 , -0.85461145]])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "F1FVDxdMEiHG",
        "outputId": "242abb89-8a9a-4cfc-d34c-b03fea575680"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-291c4107-4a04-4800-a770-069a9123d8d3\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-291c4107-4a04-4800-a770-069a9123d8d3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving heart_test.txt to heart_test.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val = pd.read_csv('heart_test.txt', sep=\" \", header=None)"
      ],
      "metadata": {
        "id": "YPgY7e0BE6BZ"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val.columns = [\"age\", \"sex\", \"chest_pain_type\", \"resting_blood_pressure\",\"serum_cholesteral\",\n",
        "                \"blood_sugar\",\"electro_results\",\"max_heart_rate\",\"angina\",\"oldpeak\",\"slope_peak\",\n",
        "                \"major_vessels\",\"thal\",\"disease\"]\n",
        "val[\"disease\"]=val[\"disease\"].replace(2,0)"
      ],
      "metadata": {
        "id": "JPDR43t0FDXE"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "on remplace les 2 par des 0 pour pouvoir utiliser la sigmoid"
      ],
      "metadata": {
        "id": "paXhRBGiaX-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "Y0YNCgfRFI6b",
        "outputId": "505a19e4-c918-487a-fa0a-3724d8ae6df4"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-772ebfae-3083-4526-a961-26098565212b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>chest_pain_type</th>\n",
              "      <th>resting_blood_pressure</th>\n",
              "      <th>serum_cholesteral</th>\n",
              "      <th>blood_sugar</th>\n",
              "      <th>electro_results</th>\n",
              "      <th>max_heart_rate</th>\n",
              "      <th>angina</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope_peak</th>\n",
              "      <th>major_vessels</th>\n",
              "      <th>thal</th>\n",
              "      <th>disease</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>63.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>252.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>172.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>51.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>227.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>154.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>54.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>258.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>147.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>44.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>220.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>54.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>239.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>126.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.8</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-772ebfae-3083-4526-a961-26098565212b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-772ebfae-3083-4526-a961-26098565212b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-772ebfae-3083-4526-a961-26098565212b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    age  sex  chest_pain_type  ...  major_vessels  thal  disease\n",
              "0  63.0  0.0              3.0  ...            0.0   3.0        1\n",
              "1  51.0  1.0              3.0  ...            1.0   7.0        1\n",
              "2  54.0  1.0              3.0  ...            0.0   7.0        1\n",
              "3  44.0  1.0              2.0  ...            0.0   3.0        1\n",
              "4  54.0  1.0              4.0  ...            1.0   7.0        0\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "on normalise les data"
      ],
      "metadata": {
        "id": "V0Ah9i3jakzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valX=val.iloc[:,range(13)]\n",
        "mean = valX.mean(axis=0)\n",
        "valX -= mean\n",
        "std = valX.std(axis=0)\n",
        "valX /= std\n",
        "valy=val.iloc[:,[13]]\n",
        "testX=valX.values\n",
        "testy=valy.values"
      ],
      "metadata": {
        "id": "luoipXDvFbrY"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testX[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5i0GTjwUGvGw",
        "outputId": "8d4da276-4ea1-46d0-880b-48f46aa7114b"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.01322274, -1.59745167, -0.2127705 ,  0.08298288, -0.00864762,\n",
              "       -0.38233244,  0.99069747,  0.98817852, -0.58610453, -0.95014154,\n",
              "       -0.88705655, -0.62903408, -0.94666068])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "simple model with adam"
      ],
      "metadata": {
        "id": "WmMcLUBeap7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "my_model=keras.Sequential([\n",
        "    keras.layers.Dense(32,activation='relu'),\n",
        "    #keras.layers.Dropout(0.4),\n",
        "    #keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(32, activation='relu'),\n",
        "    #keras.layers.BatchNormalization(),\n",
        "    #keras.layers.Dropout(0.4),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "my_model.compile(optimizer='adam',\n",
        "                       loss= 'binary_crossentropy',\n",
        "                       metrics=['acc'])"
      ],
      "metadata": {
        "id": "n6YDMjAgG0uQ"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_hist = my_model.fit(trainX, trainy, epochs=50,validation_data=(testX, testy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PfN4s4THfPb",
        "outputId": "b86bd007-1dfc-432c-a45d-5521de825885"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "7/7 [==============================] - 0s 25ms/step - loss: 0.2986 - acc: 0.8843 - val_loss: 0.3839 - val_acc: 0.8333\n",
            "Epoch 2/50\n",
            "7/7 [==============================] - 0s 17ms/step - loss: 0.2937 - acc: 0.8889 - val_loss: 0.3856 - val_acc: 0.8704\n",
            "Epoch 3/50\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.2899 - acc: 0.8889 - val_loss: 0.3868 - val_acc: 0.8704\n",
            "Epoch 4/50\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.2849 - acc: 0.8935 - val_loss: 0.3887 - val_acc: 0.8704\n",
            "Epoch 5/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.2808 - acc: 0.8935 - val_loss: 0.3906 - val_acc: 0.8704\n",
            "Epoch 6/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.2765 - acc: 0.9028 - val_loss: 0.3925 - val_acc: 0.8704\n",
            "Epoch 7/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.2731 - acc: 0.9028 - val_loss: 0.3962 - val_acc: 0.8704\n",
            "Epoch 8/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.2692 - acc: 0.9028 - val_loss: 0.3986 - val_acc: 0.8704\n",
            "Epoch 9/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.2653 - acc: 0.9028 - val_loss: 0.4008 - val_acc: 0.8704\n",
            "Epoch 10/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.2621 - acc: 0.9028 - val_loss: 0.4033 - val_acc: 0.8704\n",
            "Epoch 11/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.2581 - acc: 0.9028 - val_loss: 0.4051 - val_acc: 0.8704\n",
            "Epoch 12/50\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.2544 - acc: 0.9028 - val_loss: 0.4071 - val_acc: 0.8704\n",
            "Epoch 13/50\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.2511 - acc: 0.9028 - val_loss: 0.4087 - val_acc: 0.8704\n",
            "Epoch 14/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.2480 - acc: 0.9167 - val_loss: 0.4109 - val_acc: 0.8704\n",
            "Epoch 15/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.2443 - acc: 0.9213 - val_loss: 0.4133 - val_acc: 0.8704\n",
            "Epoch 16/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.2413 - acc: 0.9167 - val_loss: 0.4166 - val_acc: 0.8704\n",
            "Epoch 17/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.2374 - acc: 0.9213 - val_loss: 0.4187 - val_acc: 0.8704\n",
            "Epoch 18/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.2343 - acc: 0.9213 - val_loss: 0.4203 - val_acc: 0.8704\n",
            "Epoch 19/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.2321 - acc: 0.9213 - val_loss: 0.4229 - val_acc: 0.8704\n",
            "Epoch 20/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.2280 - acc: 0.9213 - val_loss: 0.4243 - val_acc: 0.8704\n",
            "Epoch 21/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.2248 - acc: 0.9259 - val_loss: 0.4263 - val_acc: 0.8704\n",
            "Epoch 22/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.2219 - acc: 0.9259 - val_loss: 0.4296 - val_acc: 0.8704\n",
            "Epoch 23/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.2188 - acc: 0.9259 - val_loss: 0.4324 - val_acc: 0.8704\n",
            "Epoch 24/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.2164 - acc: 0.9259 - val_loss: 0.4340 - val_acc: 0.8704\n",
            "Epoch 25/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.2132 - acc: 0.9259 - val_loss: 0.4372 - val_acc: 0.8704\n",
            "Epoch 26/50\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.2102 - acc: 0.9306 - val_loss: 0.4393 - val_acc: 0.8704\n",
            "Epoch 27/50\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.2076 - acc: 0.9306 - val_loss: 0.4418 - val_acc: 0.8704\n",
            "Epoch 28/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.2042 - acc: 0.9306 - val_loss: 0.4451 - val_acc: 0.8704\n",
            "Epoch 29/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.2013 - acc: 0.9352 - val_loss: 0.4470 - val_acc: 0.8704\n",
            "Epoch 30/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.1987 - acc: 0.9352 - val_loss: 0.4511 - val_acc: 0.8889\n",
            "Epoch 31/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.1953 - acc: 0.9398 - val_loss: 0.4543 - val_acc: 0.8704\n",
            "Epoch 32/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.1925 - acc: 0.9398 - val_loss: 0.4560 - val_acc: 0.8704\n",
            "Epoch 33/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.1897 - acc: 0.9398 - val_loss: 0.4571 - val_acc: 0.8704\n",
            "Epoch 34/50\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.1860 - acc: 0.9398 - val_loss: 0.4620 - val_acc: 0.8704\n",
            "Epoch 35/50\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.1831 - acc: 0.9444 - val_loss: 0.4648 - val_acc: 0.8704\n",
            "Epoch 36/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.1802 - acc: 0.9444 - val_loss: 0.4700 - val_acc: 0.8519\n",
            "Epoch 37/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.1773 - acc: 0.9444 - val_loss: 0.4714 - val_acc: 0.8519\n",
            "Epoch 38/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.1741 - acc: 0.9491 - val_loss: 0.4747 - val_acc: 0.8519\n",
            "Epoch 39/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.1716 - acc: 0.9491 - val_loss: 0.4773 - val_acc: 0.8519\n",
            "Epoch 40/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.1688 - acc: 0.9537 - val_loss: 0.4805 - val_acc: 0.8519\n",
            "Epoch 41/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.1658 - acc: 0.9537 - val_loss: 0.4829 - val_acc: 0.8519\n",
            "Epoch 42/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.1629 - acc: 0.9630 - val_loss: 0.4866 - val_acc: 0.8519\n",
            "Epoch 43/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.1599 - acc: 0.9630 - val_loss: 0.4895 - val_acc: 0.8519\n",
            "Epoch 44/50\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.1579 - acc: 0.9676 - val_loss: 0.4915 - val_acc: 0.8519\n",
            "Epoch 45/50\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.1549 - acc: 0.9630 - val_loss: 0.4943 - val_acc: 0.8519\n",
            "Epoch 46/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.1527 - acc: 0.9676 - val_loss: 0.4975 - val_acc: 0.8519\n",
            "Epoch 47/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.1497 - acc: 0.9676 - val_loss: 0.5001 - val_acc: 0.8519\n",
            "Epoch 48/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.1470 - acc: 0.9676 - val_loss: 0.5057 - val_acc: 0.8519\n",
            "Epoch 49/50\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.1443 - acc: 0.9676 - val_loss: 0.5099 - val_acc: 0.8519\n",
            "Epoch 50/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.1420 - acc: 0.9676 - val_loss: 0.5114 - val_acc: 0.8519\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "simple model with rmsprop"
      ],
      "metadata": {
        "id": "SQJBCj7HauXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "my_model2=keras.Sequential([\n",
        "    keras.layers.Dense(32,activation='relu'),\n",
        "    #keras.layers.Dropout(0.4),\n",
        "    #keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(32, activation='relu'),\n",
        "    #keras.layers.BatchNormalization(),\n",
        "    #keras.layers.Dropout(0.4),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "my_model2.compile(optimizer='rmsprop',\n",
        "                       loss= 'binary_crossentropy',\n",
        "                       metrics=['acc'])\n",
        "original_hist2 = my_model2.fit(trainX, trainy, epochs=50,validation_data=(testX, testy))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hJ0Z4CbIFz8",
        "outputId": "f0ba6808-ecf7-42a3-d267-8b0fda8a00ca"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "7/7 [==============================] - 2s 86ms/step - loss: 0.6550 - acc: 0.6296 - val_loss: 0.5876 - val_acc: 0.6667\n",
            "Epoch 2/50\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.5892 - acc: 0.7315 - val_loss: 0.5422 - val_acc: 0.7037\n",
            "Epoch 3/50\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.5476 - acc: 0.7731 - val_loss: 0.5081 - val_acc: 0.7778\n",
            "Epoch 4/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.5146 - acc: 0.8009 - val_loss: 0.4789 - val_acc: 0.8148\n",
            "Epoch 5/50\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.4858 - acc: 0.8102 - val_loss: 0.4551 - val_acc: 0.8333\n",
            "Epoch 6/50\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.4614 - acc: 0.8287 - val_loss: 0.4357 - val_acc: 0.8333\n",
            "Epoch 7/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4392 - acc: 0.8333 - val_loss: 0.4215 - val_acc: 0.8333\n",
            "Epoch 8/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.4226 - acc: 0.8472 - val_loss: 0.4104 - val_acc: 0.8333\n",
            "Epoch 9/50\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.4071 - acc: 0.8472 - val_loss: 0.4014 - val_acc: 0.8333\n",
            "Epoch 10/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.3917 - acc: 0.8565 - val_loss: 0.3939 - val_acc: 0.8333\n",
            "Epoch 11/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3804 - acc: 0.8611 - val_loss: 0.3891 - val_acc: 0.8333\n",
            "Epoch 12/50\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.3695 - acc: 0.8704 - val_loss: 0.3864 - val_acc: 0.8333\n",
            "Epoch 13/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.3585 - acc: 0.8796 - val_loss: 0.3841 - val_acc: 0.8333\n",
            "Epoch 14/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.3499 - acc: 0.8796 - val_loss: 0.3822 - val_acc: 0.8333\n",
            "Epoch 15/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3418 - acc: 0.8796 - val_loss: 0.3812 - val_acc: 0.8333\n",
            "Epoch 16/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.3342 - acc: 0.8843 - val_loss: 0.3803 - val_acc: 0.8333\n",
            "Epoch 17/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.3284 - acc: 0.8843 - val_loss: 0.3803 - val_acc: 0.8148\n",
            "Epoch 18/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.3215 - acc: 0.8889 - val_loss: 0.3805 - val_acc: 0.8333\n",
            "Epoch 19/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3166 - acc: 0.8935 - val_loss: 0.3814 - val_acc: 0.8148\n",
            "Epoch 20/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.3102 - acc: 0.8981 - val_loss: 0.3830 - val_acc: 0.8333\n",
            "Epoch 21/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.3056 - acc: 0.8935 - val_loss: 0.3837 - val_acc: 0.8333\n",
            "Epoch 22/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.3009 - acc: 0.8935 - val_loss: 0.3848 - val_acc: 0.8333\n",
            "Epoch 23/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.2969 - acc: 0.8935 - val_loss: 0.3858 - val_acc: 0.8333\n",
            "Epoch 24/50\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.2926 - acc: 0.8935 - val_loss: 0.3868 - val_acc: 0.8333\n",
            "Epoch 25/50\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.2884 - acc: 0.8935 - val_loss: 0.3881 - val_acc: 0.8333\n",
            "Epoch 26/50\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.2836 - acc: 0.9028 - val_loss: 0.3895 - val_acc: 0.8333\n",
            "Epoch 27/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.2799 - acc: 0.8981 - val_loss: 0.3911 - val_acc: 0.8333\n",
            "Epoch 28/50\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.2750 - acc: 0.8981 - val_loss: 0.3923 - val_acc: 0.8333\n",
            "Epoch 29/50\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.2724 - acc: 0.8981 - val_loss: 0.3945 - val_acc: 0.8333\n",
            "Epoch 30/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.2703 - acc: 0.9028 - val_loss: 0.3964 - val_acc: 0.8333\n",
            "Epoch 31/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.2654 - acc: 0.9028 - val_loss: 0.3979 - val_acc: 0.8333\n",
            "Epoch 32/50\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.2619 - acc: 0.9028 - val_loss: 0.3984 - val_acc: 0.8333\n",
            "Epoch 33/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.2583 - acc: 0.9028 - val_loss: 0.4006 - val_acc: 0.8333\n",
            "Epoch 34/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.2550 - acc: 0.9028 - val_loss: 0.4023 - val_acc: 0.8333\n",
            "Epoch 35/50\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.2517 - acc: 0.9074 - val_loss: 0.4049 - val_acc: 0.8333\n",
            "Epoch 36/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.2495 - acc: 0.9028 - val_loss: 0.4064 - val_acc: 0.8333\n",
            "Epoch 37/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.2459 - acc: 0.9074 - val_loss: 0.4088 - val_acc: 0.8333\n",
            "Epoch 38/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.2429 - acc: 0.9028 - val_loss: 0.4105 - val_acc: 0.8333\n",
            "Epoch 39/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.2405 - acc: 0.9120 - val_loss: 0.4129 - val_acc: 0.8333\n",
            "Epoch 40/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.2370 - acc: 0.9167 - val_loss: 0.4145 - val_acc: 0.8333\n",
            "Epoch 41/50\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.2351 - acc: 0.9120 - val_loss: 0.4155 - val_acc: 0.8333\n",
            "Epoch 42/50\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.2318 - acc: 0.9167 - val_loss: 0.4183 - val_acc: 0.8148\n",
            "Epoch 43/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.2287 - acc: 0.9167 - val_loss: 0.4194 - val_acc: 0.8148\n",
            "Epoch 44/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.2248 - acc: 0.9213 - val_loss: 0.4219 - val_acc: 0.8148\n",
            "Epoch 45/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.2229 - acc: 0.9259 - val_loss: 0.4236 - val_acc: 0.8148\n",
            "Epoch 46/50\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.2196 - acc: 0.9213 - val_loss: 0.4249 - val_acc: 0.8148\n",
            "Epoch 47/50\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.2166 - acc: 0.9213 - val_loss: 0.4255 - val_acc: 0.8148\n",
            "Epoch 48/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.2137 - acc: 0.9259 - val_loss: 0.4275 - val_acc: 0.8148\n",
            "Epoch 49/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.2115 - acc: 0.9259 - val_loss: 0.4291 - val_acc: 0.8148\n",
            "Epoch 50/50\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.2090 - acc: 0.9259 - val_loss: 0.4299 - val_acc: 0.8148\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "simple model with adagrad"
      ],
      "metadata": {
        "id": "bWU-rLoSazOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "my_model3=keras.Sequential([\n",
        "    keras.layers.Dense(32,activation='relu'),\n",
        "    #keras.layers.Dropout(0.4),\n",
        "    #keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(32, activation='relu'),\n",
        "    #keras.layers.BatchNormalization(),\n",
        "    #keras.layers.Dropout(0.4),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "my_model3.compile(optimizer='adagrad',\n",
        "                       loss= 'binary_crossentropy',\n",
        "                       metrics=['acc'])\n",
        "original_hist3 = my_model3.fit(trainX, trainy, epochs=50,validation_data=(testX, testy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEBTzaa9IU7W",
        "outputId": "204128c0-724e-44aa-d242-031b45280763"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "7/7 [==============================] - 1s 31ms/step - loss: 0.7891 - acc: 0.3935 - val_loss: 0.7738 - val_acc: 0.4630\n",
            "Epoch 2/50\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.7820 - acc: 0.3935 - val_loss: 0.7672 - val_acc: 0.4815\n",
            "Epoch 3/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.7756 - acc: 0.4028 - val_loss: 0.7612 - val_acc: 0.5000\n",
            "Epoch 4/50\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.7693 - acc: 0.4120 - val_loss: 0.7554 - val_acc: 0.5185\n",
            "Epoch 5/50\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.7635 - acc: 0.4213 - val_loss: 0.7499 - val_acc: 0.5185\n",
            "Epoch 6/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.7579 - acc: 0.4306 - val_loss: 0.7446 - val_acc: 0.5185\n",
            "Epoch 7/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.7527 - acc: 0.4352 - val_loss: 0.7397 - val_acc: 0.5185\n",
            "Epoch 8/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.7477 - acc: 0.4444 - val_loss: 0.7349 - val_acc: 0.5185\n",
            "Epoch 9/50\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.7429 - acc: 0.4583 - val_loss: 0.7304 - val_acc: 0.5185\n",
            "Epoch 10/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.7385 - acc: 0.4583 - val_loss: 0.7262 - val_acc: 0.5185\n",
            "Epoch 11/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.7341 - acc: 0.4583 - val_loss: 0.7220 - val_acc: 0.5185\n",
            "Epoch 12/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.7298 - acc: 0.4630 - val_loss: 0.7180 - val_acc: 0.5185\n",
            "Epoch 13/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.7257 - acc: 0.4676 - val_loss: 0.7141 - val_acc: 0.5370\n",
            "Epoch 14/50\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.7219 - acc: 0.4769 - val_loss: 0.7103 - val_acc: 0.5370\n",
            "Epoch 15/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.7179 - acc: 0.4815 - val_loss: 0.7067 - val_acc: 0.5370\n",
            "Epoch 16/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.7142 - acc: 0.4861 - val_loss: 0.7031 - val_acc: 0.5741\n",
            "Epoch 17/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.7106 - acc: 0.4954 - val_loss: 0.6996 - val_acc: 0.5741\n",
            "Epoch 18/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.7070 - acc: 0.5046 - val_loss: 0.6963 - val_acc: 0.5741\n",
            "Epoch 19/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.7036 - acc: 0.5093 - val_loss: 0.6930 - val_acc: 0.5741\n",
            "Epoch 20/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.7002 - acc: 0.5139 - val_loss: 0.6897 - val_acc: 0.5926\n",
            "Epoch 21/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.6969 - acc: 0.5278 - val_loss: 0.6865 - val_acc: 0.5926\n",
            "Epoch 22/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.6936 - acc: 0.5463 - val_loss: 0.6833 - val_acc: 0.5926\n",
            "Epoch 23/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.6904 - acc: 0.5556 - val_loss: 0.6803 - val_acc: 0.5926\n",
            "Epoch 24/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.6873 - acc: 0.5556 - val_loss: 0.6773 - val_acc: 0.5926\n",
            "Epoch 25/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.6842 - acc: 0.5648 - val_loss: 0.6744 - val_acc: 0.5926\n",
            "Epoch 26/50\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.6813 - acc: 0.5694 - val_loss: 0.6716 - val_acc: 0.5926\n",
            "Epoch 27/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.6784 - acc: 0.5787 - val_loss: 0.6688 - val_acc: 0.6111\n",
            "Epoch 28/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.6755 - acc: 0.5926 - val_loss: 0.6661 - val_acc: 0.6296\n",
            "Epoch 29/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.6728 - acc: 0.5972 - val_loss: 0.6633 - val_acc: 0.6296\n",
            "Epoch 30/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.6701 - acc: 0.6065 - val_loss: 0.6607 - val_acc: 0.6296\n",
            "Epoch 31/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.6674 - acc: 0.6111 - val_loss: 0.6581 - val_acc: 0.6296\n",
            "Epoch 32/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.6648 - acc: 0.6157 - val_loss: 0.6556 - val_acc: 0.6296\n",
            "Epoch 33/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.6623 - acc: 0.6157 - val_loss: 0.6531 - val_acc: 0.6296\n",
            "Epoch 34/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.6597 - acc: 0.6250 - val_loss: 0.6506 - val_acc: 0.6296\n",
            "Epoch 35/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.6572 - acc: 0.6250 - val_loss: 0.6482 - val_acc: 0.6481\n",
            "Epoch 36/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.6548 - acc: 0.6343 - val_loss: 0.6458 - val_acc: 0.6296\n",
            "Epoch 37/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.6524 - acc: 0.6435 - val_loss: 0.6434 - val_acc: 0.6481\n",
            "Epoch 38/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.6500 - acc: 0.6481 - val_loss: 0.6411 - val_acc: 0.6481\n",
            "Epoch 39/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.6476 - acc: 0.6481 - val_loss: 0.6388 - val_acc: 0.6481\n",
            "Epoch 40/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.6453 - acc: 0.6528 - val_loss: 0.6365 - val_acc: 0.6481\n",
            "Epoch 41/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.6431 - acc: 0.6528 - val_loss: 0.6343 - val_acc: 0.6481\n",
            "Epoch 42/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.6409 - acc: 0.6528 - val_loss: 0.6321 - val_acc: 0.6481\n",
            "Epoch 43/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.6387 - acc: 0.6528 - val_loss: 0.6299 - val_acc: 0.6481\n",
            "Epoch 44/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.6365 - acc: 0.6574 - val_loss: 0.6278 - val_acc: 0.6481\n",
            "Epoch 45/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.6344 - acc: 0.6574 - val_loss: 0.6257 - val_acc: 0.6481\n",
            "Epoch 46/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.6323 - acc: 0.6574 - val_loss: 0.6235 - val_acc: 0.6481\n",
            "Epoch 47/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.6301 - acc: 0.6620 - val_loss: 0.6214 - val_acc: 0.6667\n",
            "Epoch 48/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.6281 - acc: 0.6713 - val_loss: 0.6194 - val_acc: 0.6667\n",
            "Epoch 49/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.6260 - acc: 0.6759 - val_loss: 0.6173 - val_acc: 0.6667\n",
            "Epoch 50/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.6240 - acc: 0.6759 - val_loss: 0.6153 - val_acc: 0.6667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "adagrad is not very good but rmsprop and adam are okay"
      ],
      "metadata": {
        "id": "ML9YWMqha38w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we add dropout\n"
      ],
      "metadata": {
        "id": "1Lr9PA-Ha7BG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "dropout+adam"
      ],
      "metadata": {
        "id": "aBCuLtJcbExh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "my_model4=keras.Sequential([\n",
        "    keras.layers.Dense(32,activation='relu'),\n",
        "    keras.layers.Dropout(0.4),\n",
        "    #keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(32, activation='relu'),\n",
        "    #keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dropout(0.4),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "my_model4.compile(optimizer='adam',\n",
        "                       loss= 'binary_crossentropy',\n",
        "                       metrics=['acc'])\n",
        "original_hist4 = my_model4.fit(trainX, trainy, epochs=50,validation_data=(testX, testy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_sMxKWqI4g0",
        "outputId": "a96236a9-43e9-4905-ffc2-f856ddeb609f"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "7/7 [==============================] - 1s 29ms/step - loss: 0.7180 - acc: 0.5509 - val_loss: 0.6765 - val_acc: 0.6111\n",
            "Epoch 2/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.7250 - acc: 0.5185 - val_loss: 0.6495 - val_acc: 0.6667\n",
            "Epoch 3/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.6779 - acc: 0.5509 - val_loss: 0.6261 - val_acc: 0.6667\n",
            "Epoch 4/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.6643 - acc: 0.6019 - val_loss: 0.6057 - val_acc: 0.7037\n",
            "Epoch 5/50\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.6395 - acc: 0.6435 - val_loss: 0.5859 - val_acc: 0.7407\n",
            "Epoch 6/50\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.6305 - acc: 0.6667 - val_loss: 0.5666 - val_acc: 0.7407\n",
            "Epoch 7/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.5760 - acc: 0.7176 - val_loss: 0.5485 - val_acc: 0.7593\n",
            "Epoch 8/50\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.5918 - acc: 0.7176 - val_loss: 0.5321 - val_acc: 0.7778\n",
            "Epoch 9/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.5610 - acc: 0.7269 - val_loss: 0.5156 - val_acc: 0.7963\n",
            "Epoch 10/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.5688 - acc: 0.7361 - val_loss: 0.5010 - val_acc: 0.7963\n",
            "Epoch 11/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.5494 - acc: 0.7269 - val_loss: 0.4874 - val_acc: 0.7963\n",
            "Epoch 12/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.5274 - acc: 0.7639 - val_loss: 0.4749 - val_acc: 0.7963\n",
            "Epoch 13/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.4714 - acc: 0.7917 - val_loss: 0.4610 - val_acc: 0.7963\n",
            "Epoch 14/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.5157 - acc: 0.7315 - val_loss: 0.4469 - val_acc: 0.7778\n",
            "Epoch 15/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.4669 - acc: 0.7824 - val_loss: 0.4353 - val_acc: 0.7963\n",
            "Epoch 16/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.4798 - acc: 0.7824 - val_loss: 0.4246 - val_acc: 0.8148\n",
            "Epoch 17/50\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.4386 - acc: 0.8472 - val_loss: 0.4149 - val_acc: 0.8148\n",
            "Epoch 18/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.4466 - acc: 0.7963 - val_loss: 0.4058 - val_acc: 0.8148\n",
            "Epoch 19/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.4373 - acc: 0.8148 - val_loss: 0.3983 - val_acc: 0.8148\n",
            "Epoch 20/50\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.4496 - acc: 0.8102 - val_loss: 0.3917 - val_acc: 0.8333\n",
            "Epoch 21/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.4005 - acc: 0.8380 - val_loss: 0.3860 - val_acc: 0.8333\n",
            "Epoch 22/50\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.4548 - acc: 0.8009 - val_loss: 0.3819 - val_acc: 0.8333\n",
            "Epoch 23/50\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.4033 - acc: 0.8333 - val_loss: 0.3768 - val_acc: 0.8333\n",
            "Epoch 24/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.4467 - acc: 0.8241 - val_loss: 0.3724 - val_acc: 0.8333\n",
            "Epoch 25/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.3988 - acc: 0.8287 - val_loss: 0.3700 - val_acc: 0.8333\n",
            "Epoch 26/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4109 - acc: 0.8009 - val_loss: 0.3673 - val_acc: 0.8333\n",
            "Epoch 27/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.4377 - acc: 0.8241 - val_loss: 0.3654 - val_acc: 0.8333\n",
            "Epoch 28/50\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.3820 - acc: 0.8519 - val_loss: 0.3643 - val_acc: 0.8333\n",
            "Epoch 29/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.3989 - acc: 0.8241 - val_loss: 0.3628 - val_acc: 0.8333\n",
            "Epoch 30/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.4020 - acc: 0.8009 - val_loss: 0.3612 - val_acc: 0.8333\n",
            "Epoch 31/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.3963 - acc: 0.8194 - val_loss: 0.3602 - val_acc: 0.8333\n",
            "Epoch 32/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.3564 - acc: 0.8611 - val_loss: 0.3589 - val_acc: 0.8333\n",
            "Epoch 33/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.4122 - acc: 0.8009 - val_loss: 0.3569 - val_acc: 0.8333\n",
            "Epoch 34/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3785 - acc: 0.8380 - val_loss: 0.3562 - val_acc: 0.8333\n",
            "Epoch 35/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4107 - acc: 0.8148 - val_loss: 0.3552 - val_acc: 0.8148\n",
            "Epoch 36/50\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.3623 - acc: 0.8380 - val_loss: 0.3549 - val_acc: 0.8148\n",
            "Epoch 37/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3724 - acc: 0.8333 - val_loss: 0.3543 - val_acc: 0.8148\n",
            "Epoch 38/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3626 - acc: 0.8704 - val_loss: 0.3531 - val_acc: 0.8148\n",
            "Epoch 39/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.3560 - acc: 0.8611 - val_loss: 0.3522 - val_acc: 0.8148\n",
            "Epoch 40/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3707 - acc: 0.8519 - val_loss: 0.3511 - val_acc: 0.8148\n",
            "Epoch 41/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.3605 - acc: 0.8426 - val_loss: 0.3504 - val_acc: 0.8148\n",
            "Epoch 42/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.4053 - acc: 0.8194 - val_loss: 0.3501 - val_acc: 0.8148\n",
            "Epoch 43/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3689 - acc: 0.8426 - val_loss: 0.3503 - val_acc: 0.8148\n",
            "Epoch 44/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3816 - acc: 0.8519 - val_loss: 0.3505 - val_acc: 0.8333\n",
            "Epoch 45/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.3067 - acc: 0.8611 - val_loss: 0.3497 - val_acc: 0.8333\n",
            "Epoch 46/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.3509 - acc: 0.8565 - val_loss: 0.3498 - val_acc: 0.8333\n",
            "Epoch 47/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3832 - acc: 0.8333 - val_loss: 0.3487 - val_acc: 0.8333\n",
            "Epoch 48/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.3373 - acc: 0.8565 - val_loss: 0.3484 - val_acc: 0.8148\n",
            "Epoch 49/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.3606 - acc: 0.8333 - val_loss: 0.3486 - val_acc: 0.8333\n",
            "Epoch 50/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.3520 - acc: 0.8657 - val_loss: 0.3491 - val_acc: 0.8333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "maintenant on peut voir qu'il y a bien moins d'écart entre le training et la validation, on a réduit l'over-fitting"
      ],
      "metadata": {
        "id": "VbUUcr0abKnC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "dropout+adam+batchnormalization"
      ],
      "metadata": {
        "id": "5zv3GHWZbOkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "my_model5=keras.Sequential([\n",
        "    keras.layers.Dense(32,activation='relu'),\n",
        "    keras.layers.Dropout(0.4),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(32, activation='relu'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dropout(0.4),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "my_model5.compile(optimizer='adam',\n",
        "                       loss= 'binary_crossentropy',\n",
        "                       metrics=['acc'])\n",
        "original_hist5 = my_model5.fit(trainX, trainy, epochs=50,validation_data=(testX, testy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHrz5-5XJep_",
        "outputId": "287ae328-7574-439f-a0c2-68848e4307a7"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "7/7 [==============================] - 3s 37ms/step - loss: 1.0680 - acc: 0.4861 - val_loss: 0.7545 - val_acc: 0.4074\n",
            "Epoch 2/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.0448 - acc: 0.4815 - val_loss: 0.7120 - val_acc: 0.4630\n",
            "Epoch 3/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.8882 - acc: 0.5694 - val_loss: 0.6764 - val_acc: 0.5556\n",
            "Epoch 4/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.8688 - acc: 0.5787 - val_loss: 0.6465 - val_acc: 0.6296\n",
            "Epoch 5/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.7680 - acc: 0.5926 - val_loss: 0.6186 - val_acc: 0.6667\n",
            "Epoch 6/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.7078 - acc: 0.6343 - val_loss: 0.5931 - val_acc: 0.7037\n",
            "Epoch 7/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.6638 - acc: 0.6898 - val_loss: 0.5697 - val_acc: 0.7407\n",
            "Epoch 8/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.6878 - acc: 0.6806 - val_loss: 0.5482 - val_acc: 0.7593\n",
            "Epoch 9/50\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.6751 - acc: 0.6991 - val_loss: 0.5292 - val_acc: 0.7593\n",
            "Epoch 10/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.5822 - acc: 0.7407 - val_loss: 0.5121 - val_acc: 0.7593\n",
            "Epoch 11/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.6491 - acc: 0.6852 - val_loss: 0.5003 - val_acc: 0.7593\n",
            "Epoch 12/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.5948 - acc: 0.6852 - val_loss: 0.4886 - val_acc: 0.7593\n",
            "Epoch 13/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.6137 - acc: 0.7407 - val_loss: 0.4789 - val_acc: 0.7778\n",
            "Epoch 14/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.5946 - acc: 0.7315 - val_loss: 0.4688 - val_acc: 0.7778\n",
            "Epoch 15/50\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.6594 - acc: 0.7083 - val_loss: 0.4599 - val_acc: 0.7778\n",
            "Epoch 16/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.5782 - acc: 0.7269 - val_loss: 0.4515 - val_acc: 0.7963\n",
            "Epoch 17/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.5394 - acc: 0.7269 - val_loss: 0.4458 - val_acc: 0.8148\n",
            "Epoch 18/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4732 - acc: 0.8102 - val_loss: 0.4386 - val_acc: 0.8148\n",
            "Epoch 19/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.5212 - acc: 0.7778 - val_loss: 0.4330 - val_acc: 0.8148\n",
            "Epoch 20/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.5714 - acc: 0.7639 - val_loss: 0.4266 - val_acc: 0.8148\n",
            "Epoch 21/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4845 - acc: 0.7824 - val_loss: 0.4215 - val_acc: 0.8333\n",
            "Epoch 22/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.5506 - acc: 0.7500 - val_loss: 0.4174 - val_acc: 0.8333\n",
            "Epoch 23/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.5576 - acc: 0.7315 - val_loss: 0.4128 - val_acc: 0.8148\n",
            "Epoch 24/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.5112 - acc: 0.7546 - val_loss: 0.4099 - val_acc: 0.8148\n",
            "Epoch 25/50\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.5488 - acc: 0.7546 - val_loss: 0.4054 - val_acc: 0.8333\n",
            "Epoch 26/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4865 - acc: 0.7870 - val_loss: 0.4026 - val_acc: 0.8333\n",
            "Epoch 27/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.4543 - acc: 0.7963 - val_loss: 0.4014 - val_acc: 0.8333\n",
            "Epoch 28/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4520 - acc: 0.7778 - val_loss: 0.3999 - val_acc: 0.8148\n",
            "Epoch 29/50\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.4538 - acc: 0.8102 - val_loss: 0.3971 - val_acc: 0.8148\n",
            "Epoch 30/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4609 - acc: 0.8194 - val_loss: 0.3940 - val_acc: 0.8333\n",
            "Epoch 31/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.5112 - acc: 0.7546 - val_loss: 0.3916 - val_acc: 0.8519\n",
            "Epoch 32/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.5262 - acc: 0.7685 - val_loss: 0.3903 - val_acc: 0.8333\n",
            "Epoch 33/50\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.4545 - acc: 0.7870 - val_loss: 0.3893 - val_acc: 0.8333\n",
            "Epoch 34/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.4450 - acc: 0.8241 - val_loss: 0.3876 - val_acc: 0.8333\n",
            "Epoch 35/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4874 - acc: 0.7963 - val_loss: 0.3862 - val_acc: 0.8333\n",
            "Epoch 36/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.4332 - acc: 0.8148 - val_loss: 0.3856 - val_acc: 0.8333\n",
            "Epoch 37/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4061 - acc: 0.8102 - val_loss: 0.3850 - val_acc: 0.8333\n",
            "Epoch 38/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4622 - acc: 0.7917 - val_loss: 0.3841 - val_acc: 0.8333\n",
            "Epoch 39/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4783 - acc: 0.7917 - val_loss: 0.3832 - val_acc: 0.8333\n",
            "Epoch 40/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4673 - acc: 0.8148 - val_loss: 0.3814 - val_acc: 0.8333\n",
            "Epoch 41/50\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.4399 - acc: 0.7778 - val_loss: 0.3808 - val_acc: 0.8333\n",
            "Epoch 42/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4850 - acc: 0.7685 - val_loss: 0.3807 - val_acc: 0.8519\n",
            "Epoch 43/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4286 - acc: 0.8426 - val_loss: 0.3810 - val_acc: 0.8519\n",
            "Epoch 44/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3722 - acc: 0.8333 - val_loss: 0.3803 - val_acc: 0.8519\n",
            "Epoch 45/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3886 - acc: 0.8426 - val_loss: 0.3793 - val_acc: 0.8519\n",
            "Epoch 46/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3877 - acc: 0.8380 - val_loss: 0.3793 - val_acc: 0.8519\n",
            "Epoch 47/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4034 - acc: 0.8426 - val_loss: 0.3786 - val_acc: 0.8519\n",
            "Epoch 48/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3499 - acc: 0.8380 - val_loss: 0.3782 - val_acc: 0.8519\n",
            "Epoch 49/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3793 - acc: 0.8194 - val_loss: 0.3787 - val_acc: 0.8519\n",
            "Epoch 50/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.3725 - acc: 0.8472 - val_loss: 0.3803 - val_acc: 0.8519\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "on peut voir qu'avec batchNormalization, la val_acc et et la val_loss sont plus stables, elles varient moins"
      ],
      "metadata": {
        "id": "yphOGE24bV5t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "dropout+adam+batchNormalization+l2_regularizer"
      ],
      "metadata": {
        "id": "1o38JNGTbZhV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "my_model6=keras.Sequential([\n",
        "    keras.layers.Dense(32,activation='relu',kernel_regularizer= keras.regularizers.l2(0.001)),\n",
        "    keras.layers.Dropout(0.4),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(32, activation='relu',kernel_regularizer= keras.regularizers.l2(0.1)),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dropout(0.4),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "my_model6.compile(optimizer='adam',\n",
        "                       loss= 'binary_crossentropy',\n",
        "                       metrics=['acc'])\n",
        "original_hist6 = my_model6.fit(trainX, trainy, epochs=50,validation_data=(testX, testy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NicLUOXBKGs5",
        "outputId": "6031a075-b4fb-4e0b-aff0-6fd0f9fc5c98"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "7/7 [==============================] - 1s 40ms/step - loss: 3.9912 - acc: 0.5185 - val_loss: 3.6949 - val_acc: 0.5741\n",
            "Epoch 2/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 3.8167 - acc: 0.5000 - val_loss: 3.5197 - val_acc: 0.5741\n",
            "Epoch 3/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 3.6296 - acc: 0.5463 - val_loss: 3.3521 - val_acc: 0.6481\n",
            "Epoch 4/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 3.4283 - acc: 0.5833 - val_loss: 3.1923 - val_acc: 0.7407\n",
            "Epoch 5/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 3.3109 - acc: 0.5694 - val_loss: 3.0411 - val_acc: 0.7593\n",
            "Epoch 6/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 3.0844 - acc: 0.6528 - val_loss: 2.8977 - val_acc: 0.7778\n",
            "Epoch 7/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 2.9414 - acc: 0.6620 - val_loss: 2.7597 - val_acc: 0.7778\n",
            "Epoch 8/50\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 2.8021 - acc: 0.6667 - val_loss: 2.6274 - val_acc: 0.7778\n",
            "Epoch 9/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 2.6433 - acc: 0.6574 - val_loss: 2.4998 - val_acc: 0.7778\n",
            "Epoch 10/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 2.5220 - acc: 0.6898 - val_loss: 2.3786 - val_acc: 0.7963\n",
            "Epoch 11/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 2.4065 - acc: 0.7037 - val_loss: 2.2644 - val_acc: 0.8519\n",
            "Epoch 12/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 2.2936 - acc: 0.6713 - val_loss: 2.1567 - val_acc: 0.8704\n",
            "Epoch 13/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 2.1276 - acc: 0.7315 - val_loss: 2.0540 - val_acc: 0.8704\n",
            "Epoch 14/50\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 2.0671 - acc: 0.7315 - val_loss: 1.9572 - val_acc: 0.8704\n",
            "Epoch 15/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.9533 - acc: 0.7407 - val_loss: 1.8674 - val_acc: 0.8704\n",
            "Epoch 16/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 1.8593 - acc: 0.7361 - val_loss: 1.7818 - val_acc: 0.8519\n",
            "Epoch 17/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.7940 - acc: 0.7639 - val_loss: 1.7014 - val_acc: 0.8519\n",
            "Epoch 18/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 1.7073 - acc: 0.7500 - val_loss: 1.6251 - val_acc: 0.8519\n",
            "Epoch 19/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 1.6013 - acc: 0.7685 - val_loss: 1.5521 - val_acc: 0.8519\n",
            "Epoch 20/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 1.5620 - acc: 0.7454 - val_loss: 1.4842 - val_acc: 0.8519\n",
            "Epoch 21/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 1.5160 - acc: 0.7315 - val_loss: 1.4201 - val_acc: 0.8519\n",
            "Epoch 22/50\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 1.3581 - acc: 0.8194 - val_loss: 1.3601 - val_acc: 0.8519\n",
            "Epoch 23/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 1.3391 - acc: 0.7685 - val_loss: 1.3023 - val_acc: 0.8519\n",
            "Epoch 24/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 1.3100 - acc: 0.7963 - val_loss: 1.2482 - val_acc: 0.8704\n",
            "Epoch 25/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 1.2739 - acc: 0.7824 - val_loss: 1.1985 - val_acc: 0.8704\n",
            "Epoch 26/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 1.2177 - acc: 0.7731 - val_loss: 1.1517 - val_acc: 0.8704\n",
            "Epoch 27/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.1491 - acc: 0.7824 - val_loss: 1.1077 - val_acc: 0.8704\n",
            "Epoch 28/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 1.1038 - acc: 0.7685 - val_loss: 1.0654 - val_acc: 0.8704\n",
            "Epoch 29/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 1.0752 - acc: 0.7731 - val_loss: 1.0261 - val_acc: 0.8704\n",
            "Epoch 30/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 1.0022 - acc: 0.7917 - val_loss: 0.9899 - val_acc: 0.8704\n",
            "Epoch 31/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.9873 - acc: 0.8148 - val_loss: 0.9569 - val_acc: 0.8704\n",
            "Epoch 32/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.9337 - acc: 0.8056 - val_loss: 0.9252 - val_acc: 0.8704\n",
            "Epoch 33/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.9163 - acc: 0.8102 - val_loss: 0.8967 - val_acc: 0.8704\n",
            "Epoch 34/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.8925 - acc: 0.7963 - val_loss: 0.8703 - val_acc: 0.8704\n",
            "Epoch 35/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.8689 - acc: 0.8102 - val_loss: 0.8453 - val_acc: 0.8704\n",
            "Epoch 36/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.8010 - acc: 0.8472 - val_loss: 0.8227 - val_acc: 0.8704\n",
            "Epoch 37/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.8854 - acc: 0.7917 - val_loss: 0.8011 - val_acc: 0.8704\n",
            "Epoch 38/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.7941 - acc: 0.8241 - val_loss: 0.7811 - val_acc: 0.8704\n",
            "Epoch 39/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.7416 - acc: 0.8333 - val_loss: 0.7615 - val_acc: 0.8704\n",
            "Epoch 40/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.7250 - acc: 0.8287 - val_loss: 0.7410 - val_acc: 0.8704\n",
            "Epoch 41/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.7028 - acc: 0.7917 - val_loss: 0.7222 - val_acc: 0.8704\n",
            "Epoch 42/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.6691 - acc: 0.8194 - val_loss: 0.7038 - val_acc: 0.8704\n",
            "Epoch 43/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.6996 - acc: 0.8148 - val_loss: 0.6899 - val_acc: 0.8704\n",
            "Epoch 44/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.6775 - acc: 0.8194 - val_loss: 0.6759 - val_acc: 0.8704\n",
            "Epoch 45/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.6279 - acc: 0.8380 - val_loss: 0.6606 - val_acc: 0.8519\n",
            "Epoch 46/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.6542 - acc: 0.8287 - val_loss: 0.6453 - val_acc: 0.8519\n",
            "Epoch 47/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.6175 - acc: 0.8611 - val_loss: 0.6324 - val_acc: 0.8519\n",
            "Epoch 48/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.5910 - acc: 0.8380 - val_loss: 0.6209 - val_acc: 0.8519\n",
            "Epoch 49/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.5932 - acc: 0.8241 - val_loss: 0.6104 - val_acc: 0.8519\n",
            "Epoch 50/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.5738 - acc: 0.8380 - val_loss: 0.6030 - val_acc: 0.8704\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "on peut voir qu'avec le l2 on peut augmenter la val_acc, et bcp moins d'over fitting puisque le train a des moins bons scores que le test"
      ],
      "metadata": {
        "id": "Y8QcjKxabh5i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "dropout+adam+batchNormalization+l1_regularizer"
      ],
      "metadata": {
        "id": "eJlcSfKablJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "my_model7=keras.Sequential([\n",
        "    keras.layers.Dense(32,activation='relu',kernel_regularizer= keras.regularizers.l1(0.001)),\n",
        "    keras.layers.Dropout(0.4),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(32, activation='relu',kernel_regularizer= keras.regularizers.l1(0.1)),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dropout(0.4),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "my_model7.compile(optimizer='adam',\n",
        "                       loss= 'binary_crossentropy',\n",
        "                       metrics=['acc'])\n",
        "original_hist7 = my_model7.fit(trainX, trainy, epochs=50,validation_data=(testX, testy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-KLSB-hLDe6",
        "outputId": "2a30a524-b169-47a2-c61e-a4066b6fe11d"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "7/7 [==============================] - 1s 42ms/step - loss: 16.6844 - acc: 0.5139 - val_loss: 16.1476 - val_acc: 0.4630\n",
            "Epoch 2/50\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 15.9207 - acc: 0.6389 - val_loss: 15.4709 - val_acc: 0.5741\n",
            "Epoch 3/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 15.3580 - acc: 0.5741 - val_loss: 14.8124 - val_acc: 0.7037\n",
            "Epoch 4/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 14.6123 - acc: 0.6435 - val_loss: 14.1713 - val_acc: 0.6852\n",
            "Epoch 5/50\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 14.0600 - acc: 0.5648 - val_loss: 13.5472 - val_acc: 0.7037\n",
            "Epoch 6/50\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 13.3814 - acc: 0.6713 - val_loss: 12.9365 - val_acc: 0.7593\n",
            "Epoch 7/50\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 12.7947 - acc: 0.6204 - val_loss: 12.3392 - val_acc: 0.7593\n",
            "Epoch 8/50\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 12.2403 - acc: 0.6250 - val_loss: 11.7652 - val_acc: 0.7593\n",
            "Epoch 9/50\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 11.5822 - acc: 0.7037 - val_loss: 11.1993 - val_acc: 0.7963\n",
            "Epoch 10/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 11.0187 - acc: 0.6852 - val_loss: 10.6524 - val_acc: 0.7963\n",
            "Epoch 11/50\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 10.4357 - acc: 0.7269 - val_loss: 10.1191 - val_acc: 0.7963\n",
            "Epoch 12/50\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 9.9828 - acc: 0.6204 - val_loss: 9.6019 - val_acc: 0.7963\n",
            "Epoch 13/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 9.4050 - acc: 0.7269 - val_loss: 9.1046 - val_acc: 0.7963\n",
            "Epoch 14/50\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 8.8726 - acc: 0.7222 - val_loss: 8.6234 - val_acc: 0.7963\n",
            "Epoch 15/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 8.5042 - acc: 0.6528 - val_loss: 8.1519 - val_acc: 0.7963\n",
            "Epoch 16/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 7.9591 - acc: 0.7222 - val_loss: 7.6952 - val_acc: 0.7963\n",
            "Epoch 17/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 7.4790 - acc: 0.7500 - val_loss: 7.2532 - val_acc: 0.7963\n",
            "Epoch 18/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 7.1913 - acc: 0.6806 - val_loss: 6.8257 - val_acc: 0.7963\n",
            "Epoch 19/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 6.6172 - acc: 0.7639 - val_loss: 6.4107 - val_acc: 0.7963\n",
            "Epoch 20/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 6.2753 - acc: 0.7639 - val_loss: 6.0144 - val_acc: 0.7963\n",
            "Epoch 21/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 5.8929 - acc: 0.7176 - val_loss: 5.6290 - val_acc: 0.7963\n",
            "Epoch 22/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 5.4714 - acc: 0.7454 - val_loss: 5.2579 - val_acc: 0.7963\n",
            "Epoch 23/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 5.1145 - acc: 0.7176 - val_loss: 4.9014 - val_acc: 0.8148\n",
            "Epoch 24/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 4.7936 - acc: 0.7639 - val_loss: 4.5627 - val_acc: 0.7963\n",
            "Epoch 25/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 4.3774 - acc: 0.7917 - val_loss: 4.2410 - val_acc: 0.7778\n",
            "Epoch 26/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 4.1224 - acc: 0.7500 - val_loss: 3.9291 - val_acc: 0.7593\n",
            "Epoch 27/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 3.7566 - acc: 0.7824 - val_loss: 3.6360 - val_acc: 0.7593\n",
            "Epoch 28/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 3.4896 - acc: 0.7778 - val_loss: 3.3537 - val_acc: 0.7593\n",
            "Epoch 29/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 3.1755 - acc: 0.7778 - val_loss: 3.0893 - val_acc: 0.7778\n",
            "Epoch 30/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 2.9272 - acc: 0.7917 - val_loss: 2.8348 - val_acc: 0.7778\n",
            "Epoch 31/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 2.6471 - acc: 0.8009 - val_loss: 2.5960 - val_acc: 0.7593\n",
            "Epoch 32/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 2.4062 - acc: 0.8056 - val_loss: 2.3722 - val_acc: 0.7407\n",
            "Epoch 33/50\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 2.1888 - acc: 0.8194 - val_loss: 2.1585 - val_acc: 0.7407\n",
            "Epoch 34/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 2.0038 - acc: 0.8056 - val_loss: 1.9685 - val_acc: 0.7407\n",
            "Epoch 35/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.7689 - acc: 0.8241 - val_loss: 1.7906 - val_acc: 0.7222\n",
            "Epoch 36/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 1.6216 - acc: 0.7963 - val_loss: 1.6308 - val_acc: 0.7407\n",
            "Epoch 37/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.4479 - acc: 0.8102 - val_loss: 1.4791 - val_acc: 0.7222\n",
            "Epoch 38/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 1.2760 - acc: 0.8241 - val_loss: 1.3513 - val_acc: 0.7222\n",
            "Epoch 39/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.2263 - acc: 0.7870 - val_loss: 1.2499 - val_acc: 0.7222\n",
            "Epoch 40/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.0714 - acc: 0.8102 - val_loss: 1.1618 - val_acc: 0.7037\n",
            "Epoch 41/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.9523 - acc: 0.8194 - val_loss: 1.0767 - val_acc: 0.7037\n",
            "Epoch 42/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.8986 - acc: 0.8611 - val_loss: 1.0131 - val_acc: 0.7037\n",
            "Epoch 43/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.8323 - acc: 0.8241 - val_loss: 0.9743 - val_acc: 0.7037\n",
            "Epoch 44/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.7795 - acc: 0.8241 - val_loss: 0.9568 - val_acc: 0.6852\n",
            "Epoch 45/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.7590 - acc: 0.8194 - val_loss: 0.9297 - val_acc: 0.6667\n",
            "Epoch 46/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.7202 - acc: 0.8611 - val_loss: 0.8909 - val_acc: 0.7037\n",
            "Epoch 47/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.7300 - acc: 0.8194 - val_loss: 0.8750 - val_acc: 0.7037\n",
            "Epoch 48/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.7050 - acc: 0.8472 - val_loss: 0.8773 - val_acc: 0.6667\n",
            "Epoch 49/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.6825 - acc: 0.8194 - val_loss: 0.8605 - val_acc: 0.6667\n",
            "Epoch 50/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.6430 - acc: 0.8380 - val_loss: 0.8451 - val_acc: 0.6852\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "l1 ne produit pas de très bons résultats, on gardera l2"
      ],
      "metadata": {
        "id": "6W2E9fUAboyh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "on ajoute du batch_size, pas beaucoup car notre dataset n'est pas très grand"
      ],
      "metadata": {
        "id": "0dwjpFvfbsL0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "dropout+adam+batchNormalization+l2_regularizer+batch_size"
      ],
      "metadata": {
        "id": "HCIMPt32bzlr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "my_model8=keras.Sequential([\n",
        "    keras.layers.Dense(32,activation='relu',kernel_regularizer= keras.regularizers.l2(0.001)),\n",
        "    keras.layers.Dropout(0.4),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(32, activation='relu',kernel_regularizer= keras.regularizers.l2(0.1)),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dropout(0.4),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "my_model8.compile(optimizer='adam',\n",
        "                       loss= 'binary_crossentropy',\n",
        "                       metrics=['acc'])\n",
        "original_hist8 = my_model8.fit(trainX, trainy,batch_size=5, epochs=50,validation_data=(testX, testy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUNDwLEeLV-K",
        "outputId": "00e63c82-2297-4102-9641-a44f0b6f033f"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "44/44 [==============================] - 2s 9ms/step - loss: 3.4575 - acc: 0.6204 - val_loss: 2.9992 - val_acc: 0.7963\n",
            "Epoch 2/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 2.9070 - acc: 0.5926 - val_loss: 2.4598 - val_acc: 0.8519\n",
            "Epoch 3/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 2.4409 - acc: 0.6343 - val_loss: 2.0468 - val_acc: 0.8704\n",
            "Epoch 4/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 2.0381 - acc: 0.6528 - val_loss: 1.7384 - val_acc: 0.8333\n",
            "Epoch 5/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 1.7982 - acc: 0.7130 - val_loss: 1.5135 - val_acc: 0.8148\n",
            "Epoch 6/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 1.5960 - acc: 0.6944 - val_loss: 1.3325 - val_acc: 0.8148\n",
            "Epoch 7/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 1.4047 - acc: 0.7037 - val_loss: 1.1857 - val_acc: 0.8148\n",
            "Epoch 8/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 1.3048 - acc: 0.7407 - val_loss: 1.0694 - val_acc: 0.8148\n",
            "Epoch 9/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 1.1707 - acc: 0.7546 - val_loss: 0.9798 - val_acc: 0.8333\n",
            "Epoch 10/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 1.1361 - acc: 0.7037 - val_loss: 0.8974 - val_acc: 0.8333\n",
            "Epoch 11/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 1.0421 - acc: 0.7083 - val_loss: 0.8409 - val_acc: 0.8148\n",
            "Epoch 12/50\n",
            "44/44 [==============================] - 0s 5ms/step - loss: 0.8304 - acc: 0.8148 - val_loss: 0.7684 - val_acc: 0.7963\n",
            "Epoch 13/50\n",
            "44/44 [==============================] - 0s 5ms/step - loss: 0.9447 - acc: 0.7222 - val_loss: 0.7176 - val_acc: 0.8148\n",
            "Epoch 14/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.9386 - acc: 0.6944 - val_loss: 0.7103 - val_acc: 0.8148\n",
            "Epoch 15/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.7555 - acc: 0.7685 - val_loss: 0.6657 - val_acc: 0.8148\n",
            "Epoch 16/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.7827 - acc: 0.7870 - val_loss: 0.6454 - val_acc: 0.8148\n",
            "Epoch 17/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.7616 - acc: 0.7546 - val_loss: 0.6138 - val_acc: 0.8333\n",
            "Epoch 18/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.7017 - acc: 0.7639 - val_loss: 0.5904 - val_acc: 0.8333\n",
            "Epoch 19/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.7502 - acc: 0.7315 - val_loss: 0.5689 - val_acc: 0.8333\n",
            "Epoch 20/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.6887 - acc: 0.7824 - val_loss: 0.5783 - val_acc: 0.8333\n",
            "Epoch 21/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.6259 - acc: 0.7731 - val_loss: 0.5661 - val_acc: 0.8519\n",
            "Epoch 22/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.6414 - acc: 0.7731 - val_loss: 0.5553 - val_acc: 0.8519\n",
            "Epoch 23/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.6391 - acc: 0.8194 - val_loss: 0.5300 - val_acc: 0.8519\n",
            "Epoch 24/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.5515 - acc: 0.8009 - val_loss: 0.5238 - val_acc: 0.8333\n",
            "Epoch 25/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.5506 - acc: 0.8056 - val_loss: 0.5055 - val_acc: 0.8333\n",
            "Epoch 26/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.6292 - acc: 0.7824 - val_loss: 0.5016 - val_acc: 0.8889\n",
            "Epoch 27/50\n",
            "44/44 [==============================] - 0s 3ms/step - loss: 0.6596 - acc: 0.7500 - val_loss: 0.5028 - val_acc: 0.8889\n",
            "Epoch 28/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.5811 - acc: 0.7917 - val_loss: 0.5026 - val_acc: 0.8889\n",
            "Epoch 29/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.5904 - acc: 0.7685 - val_loss: 0.4955 - val_acc: 0.8704\n",
            "Epoch 30/50\n",
            "44/44 [==============================] - 0s 5ms/step - loss: 0.5563 - acc: 0.7917 - val_loss: 0.4800 - val_acc: 0.8704\n",
            "Epoch 31/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.5553 - acc: 0.8194 - val_loss: 0.4752 - val_acc: 0.8704\n",
            "Epoch 32/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.6132 - acc: 0.7731 - val_loss: 0.4973 - val_acc: 0.8333\n",
            "Epoch 33/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.5765 - acc: 0.7546 - val_loss: 0.4837 - val_acc: 0.8333\n",
            "Epoch 34/50\n",
            "44/44 [==============================] - 0s 3ms/step - loss: 0.5964 - acc: 0.7870 - val_loss: 0.4708 - val_acc: 0.8148\n",
            "Epoch 35/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.5874 - acc: 0.7685 - val_loss: 0.4635 - val_acc: 0.8333\n",
            "Epoch 36/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.5747 - acc: 0.7454 - val_loss: 0.4673 - val_acc: 0.8519\n",
            "Epoch 37/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.5135 - acc: 0.8056 - val_loss: 0.4645 - val_acc: 0.8519\n",
            "Epoch 38/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.4866 - acc: 0.8287 - val_loss: 0.4835 - val_acc: 0.8333\n",
            "Epoch 39/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.5344 - acc: 0.7778 - val_loss: 0.4875 - val_acc: 0.8519\n",
            "Epoch 40/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.5390 - acc: 0.8102 - val_loss: 0.4900 - val_acc: 0.8519\n",
            "Epoch 41/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.5473 - acc: 0.7870 - val_loss: 0.4911 - val_acc: 0.8519\n",
            "Epoch 42/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.5287 - acc: 0.7593 - val_loss: 0.4874 - val_acc: 0.8704\n",
            "Epoch 43/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.5923 - acc: 0.7593 - val_loss: 0.4836 - val_acc: 0.8333\n",
            "Epoch 44/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.5005 - acc: 0.8148 - val_loss: 0.4720 - val_acc: 0.8519\n",
            "Epoch 45/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.5638 - acc: 0.7361 - val_loss: 0.4716 - val_acc: 0.8519\n",
            "Epoch 46/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.5123 - acc: 0.8102 - val_loss: 0.4664 - val_acc: 0.8519\n",
            "Epoch 47/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.5906 - acc: 0.7685 - val_loss: 0.4495 - val_acc: 0.8889\n",
            "Epoch 48/50\n",
            "44/44 [==============================] - 0s 3ms/step - loss: 0.4915 - acc: 0.8102 - val_loss: 0.4546 - val_acc: 0.8519\n",
            "Epoch 49/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.5027 - acc: 0.8009 - val_loss: 0.4419 - val_acc: 0.8704\n",
            "Epoch 50/50\n",
            "44/44 [==============================] - 0s 4ms/step - loss: 0.4731 - acc: 0.8009 - val_loss: 0.4319 - val_acc: 0.8704\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "on obtient des assez bons résultats, mais le temps de calcul est plus gros"
      ],
      "metadata": {
        "id": "wcFwBOFHeWPr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "on va essayer de trouver la meilleur valeur de l2 possible"
      ],
      "metadata": {
        "id": "0eIJ096SeNgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_model(trainX, trainy, testX, testy, l2_rate):\n",
        "  # define model\n",
        "  model=keras.Sequential([\n",
        "    keras.layers.Dense(32,activation='relu',kernel_regularizer= keras.regularizers.l2(l2_rate)),\n",
        "    keras.layers.Dropout(0.4),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(32, activation='relu',kernel_regularizer= keras.regularizers.l2(l2_rate)),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dropout(0.4),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  # fit model\n",
        "  history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=50)"
      ],
      "metadata": {
        "id": "VI9X45U8McjN"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l=[0.0001,0.001,0.1]\n",
        "for i in l:\n",
        "  fit_model(trainX,trainy,testX,testy,i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dQxov3lNBIa",
        "outputId": "f78f06c1-f1d6-4fc2-8d6e-67e47a85b095"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "7/7 [==============================] - 1s 41ms/step - loss: 0.9570 - accuracy: 0.5000 - val_loss: 0.7153 - val_accuracy: 0.4630\n",
            "Epoch 2/50\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.9582 - accuracy: 0.5185 - val_loss: 0.6891 - val_accuracy: 0.4815\n",
            "Epoch 3/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.7595 - accuracy: 0.6019 - val_loss: 0.6668 - val_accuracy: 0.4815\n",
            "Epoch 4/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.7942 - accuracy: 0.5648 - val_loss: 0.6448 - val_accuracy: 0.6111\n",
            "Epoch 5/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.7263 - accuracy: 0.6389 - val_loss: 0.6253 - val_accuracy: 0.6481\n",
            "Epoch 6/50\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.6653 - accuracy: 0.6759 - val_loss: 0.6064 - val_accuracy: 0.6667\n",
            "Epoch 7/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.6280 - accuracy: 0.6667 - val_loss: 0.5911 - val_accuracy: 0.6667\n",
            "Epoch 8/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.6775 - accuracy: 0.6944 - val_loss: 0.5775 - val_accuracy: 0.7407\n",
            "Epoch 9/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.5981 - accuracy: 0.7083 - val_loss: 0.5638 - val_accuracy: 0.7593\n",
            "Epoch 10/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.6093 - accuracy: 0.7037 - val_loss: 0.5519 - val_accuracy: 0.7963\n",
            "Epoch 11/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.6840 - accuracy: 0.6713 - val_loss: 0.5427 - val_accuracy: 0.7963\n",
            "Epoch 12/50\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.5569 - accuracy: 0.7083 - val_loss: 0.5353 - val_accuracy: 0.7963\n",
            "Epoch 13/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.5502 - accuracy: 0.7037 - val_loss: 0.5263 - val_accuracy: 0.7963\n",
            "Epoch 14/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.4986 - accuracy: 0.7685 - val_loss: 0.5176 - val_accuracy: 0.7963\n",
            "Epoch 15/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4932 - accuracy: 0.7917 - val_loss: 0.5072 - val_accuracy: 0.7963\n",
            "Epoch 16/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.5702 - accuracy: 0.7083 - val_loss: 0.4985 - val_accuracy: 0.7963\n",
            "Epoch 17/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.5506 - accuracy: 0.7731 - val_loss: 0.4911 - val_accuracy: 0.8148\n",
            "Epoch 18/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.5215 - accuracy: 0.7593 - val_loss: 0.4837 - val_accuracy: 0.8148\n",
            "Epoch 19/50\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.5899 - accuracy: 0.7407 - val_loss: 0.4764 - val_accuracy: 0.8148\n",
            "Epoch 20/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4732 - accuracy: 0.8102 - val_loss: 0.4690 - val_accuracy: 0.8148\n",
            "Epoch 21/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.5068 - accuracy: 0.7685 - val_loss: 0.4624 - val_accuracy: 0.8148\n",
            "Epoch 22/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4975 - accuracy: 0.7824 - val_loss: 0.4557 - val_accuracy: 0.8148\n",
            "Epoch 23/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4428 - accuracy: 0.8148 - val_loss: 0.4505 - val_accuracy: 0.8148\n",
            "Epoch 24/50\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.4884 - accuracy: 0.7824 - val_loss: 0.4470 - val_accuracy: 0.8148\n",
            "Epoch 25/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4800 - accuracy: 0.7824 - val_loss: 0.4451 - val_accuracy: 0.8148\n",
            "Epoch 26/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.5361 - accuracy: 0.7454 - val_loss: 0.4422 - val_accuracy: 0.8148\n",
            "Epoch 27/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4793 - accuracy: 0.8102 - val_loss: 0.4400 - val_accuracy: 0.8148\n",
            "Epoch 28/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.4827 - accuracy: 0.7731 - val_loss: 0.4372 - val_accuracy: 0.8148\n",
            "Epoch 29/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4660 - accuracy: 0.7963 - val_loss: 0.4342 - val_accuracy: 0.8148\n",
            "Epoch 30/50\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.4428 - accuracy: 0.8009 - val_loss: 0.4323 - val_accuracy: 0.8333\n",
            "Epoch 31/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4407 - accuracy: 0.8102 - val_loss: 0.4288 - val_accuracy: 0.8333\n",
            "Epoch 32/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4477 - accuracy: 0.7917 - val_loss: 0.4275 - val_accuracy: 0.8333\n",
            "Epoch 33/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.4901 - accuracy: 0.7824 - val_loss: 0.4260 - val_accuracy: 0.8148\n",
            "Epoch 34/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3828 - accuracy: 0.8333 - val_loss: 0.4251 - val_accuracy: 0.8148\n",
            "Epoch 35/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.4366 - accuracy: 0.8102 - val_loss: 0.4237 - val_accuracy: 0.8148\n",
            "Epoch 36/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.4310 - accuracy: 0.8102 - val_loss: 0.4222 - val_accuracy: 0.8148\n",
            "Epoch 37/50\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.4299 - accuracy: 0.8333 - val_loss: 0.4204 - val_accuracy: 0.8148\n",
            "Epoch 38/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4546 - accuracy: 0.8056 - val_loss: 0.4187 - val_accuracy: 0.8148\n",
            "Epoch 39/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4208 - accuracy: 0.8194 - val_loss: 0.4173 - val_accuracy: 0.8148\n",
            "Epoch 40/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.4184 - accuracy: 0.8426 - val_loss: 0.4162 - val_accuracy: 0.8148\n",
            "Epoch 41/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4325 - accuracy: 0.8148 - val_loss: 0.4150 - val_accuracy: 0.8148\n",
            "Epoch 42/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4684 - accuracy: 0.8009 - val_loss: 0.4146 - val_accuracy: 0.8148\n",
            "Epoch 43/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.3887 - accuracy: 0.8472 - val_loss: 0.4155 - val_accuracy: 0.8148\n",
            "Epoch 44/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4270 - accuracy: 0.7963 - val_loss: 0.4171 - val_accuracy: 0.8148\n",
            "Epoch 45/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4133 - accuracy: 0.8194 - val_loss: 0.4182 - val_accuracy: 0.8148\n",
            "Epoch 46/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.3813 - accuracy: 0.8194 - val_loss: 0.4187 - val_accuracy: 0.8148\n",
            "Epoch 47/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4296 - accuracy: 0.8148 - val_loss: 0.4190 - val_accuracy: 0.8148\n",
            "Epoch 48/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.4213 - accuracy: 0.8333 - val_loss: 0.4188 - val_accuracy: 0.8148\n",
            "Epoch 49/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4238 - accuracy: 0.7917 - val_loss: 0.4181 - val_accuracy: 0.8148\n",
            "Epoch 50/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.4169 - accuracy: 0.8426 - val_loss: 0.4181 - val_accuracy: 0.8148\n",
            "Epoch 1/50\n",
            "7/7 [==============================] - 1s 38ms/step - loss: 1.2112 - accuracy: 0.4815 - val_loss: 0.8676 - val_accuracy: 0.4074\n",
            "Epoch 2/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.9074 - accuracy: 0.5787 - val_loss: 0.7983 - val_accuracy: 0.4074\n",
            "Epoch 3/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.8595 - accuracy: 0.5972 - val_loss: 0.7398 - val_accuracy: 0.4630\n",
            "Epoch 4/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.7813 - accuracy: 0.6713 - val_loss: 0.6906 - val_accuracy: 0.6481\n",
            "Epoch 5/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.7094 - accuracy: 0.6667 - val_loss: 0.6488 - val_accuracy: 0.7407\n",
            "Epoch 6/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.6833 - accuracy: 0.6852 - val_loss: 0.6134 - val_accuracy: 0.7593\n",
            "Epoch 7/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.6659 - accuracy: 0.7130 - val_loss: 0.5851 - val_accuracy: 0.8148\n",
            "Epoch 8/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.6489 - accuracy: 0.7176 - val_loss: 0.5599 - val_accuracy: 0.8519\n",
            "Epoch 9/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.7034 - accuracy: 0.6713 - val_loss: 0.5420 - val_accuracy: 0.8333\n",
            "Epoch 10/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.6181 - accuracy: 0.7407 - val_loss: 0.5272 - val_accuracy: 0.8333\n",
            "Epoch 11/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.6636 - accuracy: 0.7222 - val_loss: 0.5140 - val_accuracy: 0.8333\n",
            "Epoch 12/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.6106 - accuracy: 0.7176 - val_loss: 0.5040 - val_accuracy: 0.8148\n",
            "Epoch 13/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.5981 - accuracy: 0.7454 - val_loss: 0.4966 - val_accuracy: 0.8148\n",
            "Epoch 14/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.5665 - accuracy: 0.7870 - val_loss: 0.4900 - val_accuracy: 0.8148\n",
            "Epoch 15/50\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.5792 - accuracy: 0.7500 - val_loss: 0.4841 - val_accuracy: 0.8148\n",
            "Epoch 16/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.5671 - accuracy: 0.7546 - val_loss: 0.4790 - val_accuracy: 0.8148\n",
            "Epoch 17/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.4834 - accuracy: 0.7963 - val_loss: 0.4733 - val_accuracy: 0.8148\n",
            "Epoch 18/50\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.5195 - accuracy: 0.7963 - val_loss: 0.4681 - val_accuracy: 0.8148\n",
            "Epoch 19/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.5983 - accuracy: 0.7500 - val_loss: 0.4631 - val_accuracy: 0.8148\n",
            "Epoch 20/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4416 - accuracy: 0.8380 - val_loss: 0.4601 - val_accuracy: 0.8333\n",
            "Epoch 21/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.4887 - accuracy: 0.8102 - val_loss: 0.4595 - val_accuracy: 0.8333\n",
            "Epoch 22/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.5305 - accuracy: 0.7963 - val_loss: 0.4581 - val_accuracy: 0.8333\n",
            "Epoch 23/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4858 - accuracy: 0.7731 - val_loss: 0.4558 - val_accuracy: 0.8148\n",
            "Epoch 24/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.5247 - accuracy: 0.8194 - val_loss: 0.4539 - val_accuracy: 0.8333\n",
            "Epoch 25/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.5581 - accuracy: 0.7361 - val_loss: 0.4550 - val_accuracy: 0.8333\n",
            "Epoch 26/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4366 - accuracy: 0.8241 - val_loss: 0.4556 - val_accuracy: 0.8333\n",
            "Epoch 27/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.5203 - accuracy: 0.8148 - val_loss: 0.4539 - val_accuracy: 0.8333\n",
            "Epoch 28/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.5134 - accuracy: 0.7870 - val_loss: 0.4522 - val_accuracy: 0.8519\n",
            "Epoch 29/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.5053 - accuracy: 0.8009 - val_loss: 0.4500 - val_accuracy: 0.8519\n",
            "Epoch 30/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4783 - accuracy: 0.8194 - val_loss: 0.4499 - val_accuracy: 0.8519\n",
            "Epoch 31/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.5345 - accuracy: 0.7685 - val_loss: 0.4490 - val_accuracy: 0.8519\n",
            "Epoch 32/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.4938 - accuracy: 0.7824 - val_loss: 0.4485 - val_accuracy: 0.8519\n",
            "Epoch 33/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.4719 - accuracy: 0.7917 - val_loss: 0.4468 - val_accuracy: 0.8519\n",
            "Epoch 34/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4596 - accuracy: 0.8102 - val_loss: 0.4460 - val_accuracy: 0.8519\n",
            "Epoch 35/50\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.4861 - accuracy: 0.7870 - val_loss: 0.4472 - val_accuracy: 0.8519\n",
            "Epoch 36/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4604 - accuracy: 0.8009 - val_loss: 0.4497 - val_accuracy: 0.8519\n",
            "Epoch 37/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4711 - accuracy: 0.8241 - val_loss: 0.4513 - val_accuracy: 0.8519\n",
            "Epoch 38/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4710 - accuracy: 0.8148 - val_loss: 0.4525 - val_accuracy: 0.8704\n",
            "Epoch 39/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.5374 - accuracy: 0.7870 - val_loss: 0.4537 - val_accuracy: 0.8704\n",
            "Epoch 40/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.4698 - accuracy: 0.8009 - val_loss: 0.4547 - val_accuracy: 0.8704\n",
            "Epoch 41/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.4668 - accuracy: 0.8102 - val_loss: 0.4569 - val_accuracy: 0.8333\n",
            "Epoch 42/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4061 - accuracy: 0.8472 - val_loss: 0.4564 - val_accuracy: 0.8519\n",
            "Epoch 43/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.4174 - accuracy: 0.8287 - val_loss: 0.4539 - val_accuracy: 0.8704\n",
            "Epoch 44/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4424 - accuracy: 0.8241 - val_loss: 0.4518 - val_accuracy: 0.8519\n",
            "Epoch 45/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4519 - accuracy: 0.8333 - val_loss: 0.4507 - val_accuracy: 0.8519\n",
            "Epoch 46/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4039 - accuracy: 0.8287 - val_loss: 0.4494 - val_accuracy: 0.8519\n",
            "Epoch 47/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.4427 - accuracy: 0.8194 - val_loss: 0.4496 - val_accuracy: 0.8519\n",
            "Epoch 48/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.4388 - accuracy: 0.8148 - val_loss: 0.4498 - val_accuracy: 0.8519\n",
            "Epoch 49/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.4414 - accuracy: 0.8194 - val_loss: 0.4500 - val_accuracy: 0.8519\n",
            "Epoch 50/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.3743 - accuracy: 0.8704 - val_loss: 0.4518 - val_accuracy: 0.8704\n",
            "Epoch 1/50\n",
            "7/7 [==============================] - 1s 38ms/step - loss: 5.7080 - accuracy: 0.5417 - val_loss: 5.3894 - val_accuracy: 0.6667\n",
            "Epoch 2/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 5.4540 - accuracy: 0.5509 - val_loss: 5.1567 - val_accuracy: 0.6852\n",
            "Epoch 3/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 5.1813 - accuracy: 0.6250 - val_loss: 4.9337 - val_accuracy: 0.7222\n",
            "Epoch 4/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 5.0001 - accuracy: 0.5880 - val_loss: 4.7189 - val_accuracy: 0.7593\n",
            "Epoch 5/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 4.6894 - accuracy: 0.6528 - val_loss: 4.5138 - val_accuracy: 0.7407\n",
            "Epoch 6/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 4.5800 - accuracy: 0.6250 - val_loss: 4.3177 - val_accuracy: 0.7593\n",
            "Epoch 7/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 4.3609 - accuracy: 0.6435 - val_loss: 4.1287 - val_accuracy: 0.7963\n",
            "Epoch 8/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 4.0123 - accuracy: 0.7315 - val_loss: 3.9482 - val_accuracy: 0.7963\n",
            "Epoch 9/50\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 3.9950 - accuracy: 0.7037 - val_loss: 3.7753 - val_accuracy: 0.7963\n",
            "Epoch 10/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 3.7471 - accuracy: 0.6944 - val_loss: 3.6120 - val_accuracy: 0.7963\n",
            "Epoch 11/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 3.6305 - accuracy: 0.6806 - val_loss: 3.4556 - val_accuracy: 0.7963\n",
            "Epoch 12/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 3.5442 - accuracy: 0.6852 - val_loss: 3.3061 - val_accuracy: 0.8333\n",
            "Epoch 13/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 3.2777 - accuracy: 0.7269 - val_loss: 3.1642 - val_accuracy: 0.8148\n",
            "Epoch 14/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 3.1309 - accuracy: 0.7454 - val_loss: 3.0297 - val_accuracy: 0.8148\n",
            "Epoch 15/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 3.0102 - accuracy: 0.7176 - val_loss: 2.9007 - val_accuracy: 0.8148\n",
            "Epoch 16/50\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 2.8458 - accuracy: 0.7870 - val_loss: 2.7761 - val_accuracy: 0.8333\n",
            "Epoch 17/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 2.7274 - accuracy: 0.7731 - val_loss: 2.6583 - val_accuracy: 0.8519\n",
            "Epoch 18/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 2.6345 - accuracy: 0.7639 - val_loss: 2.5478 - val_accuracy: 0.8519\n",
            "Epoch 19/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 2.4899 - accuracy: 0.7731 - val_loss: 2.4412 - val_accuracy: 0.8519\n",
            "Epoch 20/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 2.4120 - accuracy: 0.7870 - val_loss: 2.3385 - val_accuracy: 0.8519\n",
            "Epoch 21/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 2.2884 - accuracy: 0.8102 - val_loss: 2.2427 - val_accuracy: 0.8519\n",
            "Epoch 22/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 2.1833 - accuracy: 0.7963 - val_loss: 2.1527 - val_accuracy: 0.8519\n",
            "Epoch 23/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 2.1452 - accuracy: 0.7824 - val_loss: 2.0654 - val_accuracy: 0.8519\n",
            "Epoch 24/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 2.0300 - accuracy: 0.7870 - val_loss: 1.9817 - val_accuracy: 0.8519\n",
            "Epoch 25/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.9876 - accuracy: 0.7407 - val_loss: 1.9047 - val_accuracy: 0.8519\n",
            "Epoch 26/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 1.8930 - accuracy: 0.8056 - val_loss: 1.8320 - val_accuracy: 0.8519\n",
            "Epoch 27/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 1.8035 - accuracy: 0.7963 - val_loss: 1.7639 - val_accuracy: 0.8333\n",
            "Epoch 28/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.7400 - accuracy: 0.7870 - val_loss: 1.6972 - val_accuracy: 0.8333\n",
            "Epoch 29/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 1.6399 - accuracy: 0.8287 - val_loss: 1.6305 - val_accuracy: 0.8333\n",
            "Epoch 30/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 1.5320 - accuracy: 0.8333 - val_loss: 1.5675 - val_accuracy: 0.8333\n",
            "Epoch 31/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 1.5387 - accuracy: 0.8102 - val_loss: 1.5101 - val_accuracy: 0.8333\n",
            "Epoch 32/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 1.4649 - accuracy: 0.8380 - val_loss: 1.4553 - val_accuracy: 0.8519\n",
            "Epoch 33/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 1.3935 - accuracy: 0.8056 - val_loss: 1.4023 - val_accuracy: 0.8704\n",
            "Epoch 34/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 1.3659 - accuracy: 0.8148 - val_loss: 1.3528 - val_accuracy: 0.8704\n",
            "Epoch 35/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 1.2911 - accuracy: 0.8380 - val_loss: 1.3067 - val_accuracy: 0.8889\n",
            "Epoch 36/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.2638 - accuracy: 0.8148 - val_loss: 1.2623 - val_accuracy: 0.8889\n",
            "Epoch 37/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 1.2075 - accuracy: 0.8380 - val_loss: 1.2207 - val_accuracy: 0.8889\n",
            "Epoch 38/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 1.1565 - accuracy: 0.8380 - val_loss: 1.1814 - val_accuracy: 0.8889\n",
            "Epoch 39/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.1172 - accuracy: 0.8287 - val_loss: 1.1434 - val_accuracy: 0.8889\n",
            "Epoch 40/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.0209 - accuracy: 0.8750 - val_loss: 1.1049 - val_accuracy: 0.8889\n",
            "Epoch 41/50\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 1.0109 - accuracy: 0.8565 - val_loss: 1.0691 - val_accuracy: 0.8889\n",
            "Epoch 42/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.9644 - accuracy: 0.8426 - val_loss: 1.0364 - val_accuracy: 0.8889\n",
            "Epoch 43/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.9389 - accuracy: 0.8611 - val_loss: 1.0055 - val_accuracy: 0.8889\n",
            "Epoch 44/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.9393 - accuracy: 0.8472 - val_loss: 0.9759 - val_accuracy: 0.8889\n",
            "Epoch 45/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.8822 - accuracy: 0.8565 - val_loss: 0.9489 - val_accuracy: 0.8889\n",
            "Epoch 46/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.8556 - accuracy: 0.8843 - val_loss: 0.9247 - val_accuracy: 0.8889\n",
            "Epoch 47/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.8480 - accuracy: 0.8704 - val_loss: 0.9022 - val_accuracy: 0.8889\n",
            "Epoch 48/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.8345 - accuracy: 0.8611 - val_loss: 0.8786 - val_accuracy: 0.8889\n",
            "Epoch 49/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.7840 - accuracy: 0.8750 - val_loss: 0.8570 - val_accuracy: 0.8889\n",
            "Epoch 50/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.7876 - accuracy: 0.8657 - val_loss: 0.8351 - val_accuracy: 0.8889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "la plus grosse accuracy est obtenue pour l2=0.1"
      ],
      "metadata": {
        "id": "c07n3nXWd_vd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "on va essayer de trouver la meilleure valeur de dropout possible"
      ],
      "metadata": {
        "id": "F6tlXD_hd8vQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_dpt(trainX, trainy, testX, testy, dpt):\n",
        "  # define model\n",
        "  model=keras.Sequential([\n",
        "    keras.layers.Dense(32,activation='relu',kernel_regularizer= keras.regularizers.l2(0.1)),\n",
        "    keras.layers.Dropout(dpt),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(32, activation='relu',kernel_regularizer= keras.regularizers.l2(0.1)),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dropout(dpt),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  # fit model\n",
        "  history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=50)"
      ],
      "metadata": {
        "id": "YcpvncJEN249"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l=[0.4,0.5,0.6]\n",
        "for i in l:\n",
        "  fit_dpt(trainX, trainy, testX, testy,i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8JHsOBGOFmD",
        "outputId": "325aa998-7852-4905-e3eb-9411c6dc20bf"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "7/7 [==============================] - 1s 40ms/step - loss: 6.4168 - accuracy: 0.4213 - val_loss: 5.8449 - val_accuracy: 0.4444\n",
            "Epoch 2/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 6.2247 - accuracy: 0.4398 - val_loss: 5.6214 - val_accuracy: 0.5185\n",
            "Epoch 3/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 5.9496 - accuracy: 0.4167 - val_loss: 5.4092 - val_accuracy: 0.5556\n",
            "Epoch 4/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 5.6826 - accuracy: 0.4583 - val_loss: 5.2058 - val_accuracy: 0.5926\n",
            "Epoch 5/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 5.4228 - accuracy: 0.5417 - val_loss: 5.0094 - val_accuracy: 0.6481\n",
            "Epoch 6/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 5.1888 - accuracy: 0.5046 - val_loss: 4.8199 - val_accuracy: 0.7037\n",
            "Epoch 7/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 5.0160 - accuracy: 0.5370 - val_loss: 4.6370 - val_accuracy: 0.7037\n",
            "Epoch 8/50\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 4.7060 - accuracy: 0.5694 - val_loss: 4.4613 - val_accuracy: 0.7222\n",
            "Epoch 9/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 4.5241 - accuracy: 0.6204 - val_loss: 4.2913 - val_accuracy: 0.7222\n",
            "Epoch 10/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 4.2797 - accuracy: 0.6343 - val_loss: 4.1266 - val_accuracy: 0.7407\n",
            "Epoch 11/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 4.2337 - accuracy: 0.6852 - val_loss: 3.9684 - val_accuracy: 0.7778\n",
            "Epoch 12/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 4.0445 - accuracy: 0.6296 - val_loss: 3.8177 - val_accuracy: 0.7593\n",
            "Epoch 13/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 3.8152 - accuracy: 0.6620 - val_loss: 3.6710 - val_accuracy: 0.7593\n",
            "Epoch 14/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 3.6590 - accuracy: 0.7037 - val_loss: 3.5292 - val_accuracy: 0.7593\n",
            "Epoch 15/50\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 3.5337 - accuracy: 0.7130 - val_loss: 3.3925 - val_accuracy: 0.7593\n",
            "Epoch 16/50\n",
            "7/7 [==============================] - 0s 19ms/step - loss: 3.5325 - accuracy: 0.6481 - val_loss: 3.2620 - val_accuracy: 0.7593\n",
            "Epoch 17/50\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 3.3496 - accuracy: 0.6713 - val_loss: 3.1372 - val_accuracy: 0.7593\n",
            "Epoch 18/50\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 3.1643 - accuracy: 0.7222 - val_loss: 3.0169 - val_accuracy: 0.7593\n",
            "Epoch 19/50\n",
            "7/7 [==============================] - 0s 17ms/step - loss: 3.1080 - accuracy: 0.7176 - val_loss: 2.9021 - val_accuracy: 0.7963\n",
            "Epoch 20/50\n",
            "7/7 [==============================] - 0s 21ms/step - loss: 2.9668 - accuracy: 0.6944 - val_loss: 2.7919 - val_accuracy: 0.7963\n",
            "Epoch 21/50\n",
            "7/7 [==============================] - 0s 20ms/step - loss: 2.7108 - accuracy: 0.7593 - val_loss: 2.6866 - val_accuracy: 0.7963\n",
            "Epoch 22/50\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 2.6572 - accuracy: 0.7870 - val_loss: 2.5841 - val_accuracy: 0.8148\n",
            "Epoch 23/50\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 2.5905 - accuracy: 0.7546 - val_loss: 2.4868 - val_accuracy: 0.8148\n",
            "Epoch 24/50\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 2.4578 - accuracy: 0.7824 - val_loss: 2.3932 - val_accuracy: 0.8148\n",
            "Epoch 25/50\n",
            "7/7 [==============================] - 0s 20ms/step - loss: 2.4289 - accuracy: 0.7593 - val_loss: 2.3049 - val_accuracy: 0.8148\n",
            "Epoch 26/50\n",
            "7/7 [==============================] - 0s 25ms/step - loss: 2.3099 - accuracy: 0.7593 - val_loss: 2.2183 - val_accuracy: 0.8148\n",
            "Epoch 27/50\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 2.1779 - accuracy: 0.8102 - val_loss: 2.1357 - val_accuracy: 0.8148\n",
            "Epoch 28/50\n",
            "7/7 [==============================] - 0s 22ms/step - loss: 2.1200 - accuracy: 0.7778 - val_loss: 2.0572 - val_accuracy: 0.8148\n",
            "Epoch 29/50\n",
            "7/7 [==============================] - 0s 17ms/step - loss: 2.0494 - accuracy: 0.7917 - val_loss: 1.9826 - val_accuracy: 0.8148\n",
            "Epoch 30/50\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 1.9220 - accuracy: 0.8009 - val_loss: 1.9105 - val_accuracy: 0.8148\n",
            "Epoch 31/50\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 1.8375 - accuracy: 0.8056 - val_loss: 1.8414 - val_accuracy: 0.8333\n",
            "Epoch 32/50\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 1.7830 - accuracy: 0.8102 - val_loss: 1.7747 - val_accuracy: 0.8148\n",
            "Epoch 33/50\n",
            "7/7 [==============================] - 0s 17ms/step - loss: 1.7062 - accuracy: 0.8056 - val_loss: 1.7110 - val_accuracy: 0.8519\n",
            "Epoch 34/50\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 1.7316 - accuracy: 0.7963 - val_loss: 1.6495 - val_accuracy: 0.8519\n",
            "Epoch 35/50\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 1.6458 - accuracy: 0.8102 - val_loss: 1.5924 - val_accuracy: 0.8519\n",
            "Epoch 36/50\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 1.6153 - accuracy: 0.7731 - val_loss: 1.5397 - val_accuracy: 0.8704\n",
            "Epoch 37/50\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 1.5150 - accuracy: 0.7963 - val_loss: 1.4892 - val_accuracy: 0.8889\n",
            "Epoch 38/50\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 1.4685 - accuracy: 0.8148 - val_loss: 1.4397 - val_accuracy: 0.8704\n",
            "Epoch 39/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 1.3853 - accuracy: 0.8472 - val_loss: 1.3932 - val_accuracy: 0.8704\n",
            "Epoch 40/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 1.3849 - accuracy: 0.8056 - val_loss: 1.3490 - val_accuracy: 0.8704\n",
            "Epoch 41/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.3827 - accuracy: 0.8194 - val_loss: 1.3047 - val_accuracy: 0.8889\n",
            "Epoch 42/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.2121 - accuracy: 0.8519 - val_loss: 1.2668 - val_accuracy: 0.8704\n",
            "Epoch 43/50\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 1.2447 - accuracy: 0.8056 - val_loss: 1.2311 - val_accuracy: 0.8704\n",
            "Epoch 44/50\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 1.2695 - accuracy: 0.7963 - val_loss: 1.1953 - val_accuracy: 0.8704\n",
            "Epoch 45/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 1.1488 - accuracy: 0.8056 - val_loss: 1.1587 - val_accuracy: 0.8519\n",
            "Epoch 46/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 1.1320 - accuracy: 0.8102 - val_loss: 1.1255 - val_accuracy: 0.8519\n",
            "Epoch 47/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.0661 - accuracy: 0.8333 - val_loss: 1.0957 - val_accuracy: 0.8519\n",
            "Epoch 48/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.0551 - accuracy: 0.8380 - val_loss: 1.0645 - val_accuracy: 0.8519\n",
            "Epoch 49/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.9852 - accuracy: 0.8796 - val_loss: 1.0345 - val_accuracy: 0.8519\n",
            "Epoch 50/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.9353 - accuracy: 0.8565 - val_loss: 1.0075 - val_accuracy: 0.8519\n",
            "Epoch 1/50\n",
            "7/7 [==============================] - 1s 40ms/step - loss: 6.0943 - accuracy: 0.4583 - val_loss: 5.6516 - val_accuracy: 0.3889\n",
            "Epoch 2/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 5.8818 - accuracy: 0.4954 - val_loss: 5.4168 - val_accuracy: 0.4630\n",
            "Epoch 3/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 5.6367 - accuracy: 0.4861 - val_loss: 5.1976 - val_accuracy: 0.5741\n",
            "Epoch 4/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 5.2528 - accuracy: 0.5602 - val_loss: 4.9855 - val_accuracy: 0.6111\n",
            "Epoch 5/50\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 5.1483 - accuracy: 0.5463 - val_loss: 4.7820 - val_accuracy: 0.7037\n",
            "Epoch 6/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 4.7890 - accuracy: 0.6343 - val_loss: 4.5879 - val_accuracy: 0.7778\n",
            "Epoch 7/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 4.6554 - accuracy: 0.6435 - val_loss: 4.3998 - val_accuracy: 0.7778\n",
            "Epoch 8/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 4.4913 - accuracy: 0.6435 - val_loss: 4.2234 - val_accuracy: 0.8148\n",
            "Epoch 9/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 4.1764 - accuracy: 0.6944 - val_loss: 4.0536 - val_accuracy: 0.8148\n",
            "Epoch 10/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 4.1845 - accuracy: 0.6019 - val_loss: 3.8918 - val_accuracy: 0.8148\n",
            "Epoch 11/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 3.9782 - accuracy: 0.6389 - val_loss: 3.7356 - val_accuracy: 0.8148\n",
            "Epoch 12/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 3.7771 - accuracy: 0.6898 - val_loss: 3.5856 - val_accuracy: 0.8148\n",
            "Epoch 13/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 3.6960 - accuracy: 0.6111 - val_loss: 3.4428 - val_accuracy: 0.8333\n",
            "Epoch 14/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 3.4804 - accuracy: 0.7222 - val_loss: 3.3076 - val_accuracy: 0.8148\n",
            "Epoch 15/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 3.3307 - accuracy: 0.7176 - val_loss: 3.1768 - val_accuracy: 0.8148\n",
            "Epoch 16/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 3.2066 - accuracy: 0.7454 - val_loss: 3.0517 - val_accuracy: 0.8333\n",
            "Epoch 17/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 2.9775 - accuracy: 0.7824 - val_loss: 2.9310 - val_accuracy: 0.8333\n",
            "Epoch 18/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 2.9325 - accuracy: 0.7639 - val_loss: 2.8164 - val_accuracy: 0.8333\n",
            "Epoch 19/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 2.8791 - accuracy: 0.7037 - val_loss: 2.7086 - val_accuracy: 0.8333\n",
            "Epoch 20/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 2.7757 - accuracy: 0.7176 - val_loss: 2.6047 - val_accuracy: 0.8333\n",
            "Epoch 21/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 2.7215 - accuracy: 0.7130 - val_loss: 2.5046 - val_accuracy: 0.8333\n",
            "Epoch 22/50\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 2.5363 - accuracy: 0.7176 - val_loss: 2.4105 - val_accuracy: 0.8519\n",
            "Epoch 23/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 2.4783 - accuracy: 0.7361 - val_loss: 2.3214 - val_accuracy: 0.8519\n",
            "Epoch 24/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 2.3951 - accuracy: 0.7454 - val_loss: 2.2351 - val_accuracy: 0.8519\n",
            "Epoch 25/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 2.2606 - accuracy: 0.7454 - val_loss: 2.1535 - val_accuracy: 0.8519\n",
            "Epoch 26/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 2.2111 - accuracy: 0.7963 - val_loss: 2.0739 - val_accuracy: 0.8704\n",
            "Epoch 27/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 2.1113 - accuracy: 0.7454 - val_loss: 1.9974 - val_accuracy: 0.8704\n",
            "Epoch 28/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 2.0729 - accuracy: 0.7454 - val_loss: 1.9270 - val_accuracy: 0.8519\n",
            "Epoch 29/50\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 1.9414 - accuracy: 0.7963 - val_loss: 1.8588 - val_accuracy: 0.8519\n",
            "Epoch 30/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 1.8928 - accuracy: 0.7731 - val_loss: 1.7940 - val_accuracy: 0.8519\n",
            "Epoch 31/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.7484 - accuracy: 0.8009 - val_loss: 1.7328 - val_accuracy: 0.8519\n",
            "Epoch 32/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.7661 - accuracy: 0.7870 - val_loss: 1.6763 - val_accuracy: 0.8519\n",
            "Epoch 33/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.7434 - accuracy: 0.7685 - val_loss: 1.6219 - val_accuracy: 0.8519\n",
            "Epoch 34/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 1.6151 - accuracy: 0.7870 - val_loss: 1.5681 - val_accuracy: 0.8519\n",
            "Epoch 35/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.6055 - accuracy: 0.7500 - val_loss: 1.5169 - val_accuracy: 0.8704\n",
            "Epoch 36/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.5028 - accuracy: 0.8102 - val_loss: 1.4682 - val_accuracy: 0.8519\n",
            "Epoch 37/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.4965 - accuracy: 0.8009 - val_loss: 1.4225 - val_accuracy: 0.8519\n",
            "Epoch 38/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.4347 - accuracy: 0.7963 - val_loss: 1.3763 - val_accuracy: 0.8519\n",
            "Epoch 39/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 1.4405 - accuracy: 0.7639 - val_loss: 1.3347 - val_accuracy: 0.8519\n",
            "Epoch 40/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.3057 - accuracy: 0.8287 - val_loss: 1.2964 - val_accuracy: 0.8519\n",
            "Epoch 41/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.2625 - accuracy: 0.8102 - val_loss: 1.2582 - val_accuracy: 0.8519\n",
            "Epoch 42/50\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 1.2647 - accuracy: 0.7963 - val_loss: 1.2211 - val_accuracy: 0.8519\n",
            "Epoch 43/50\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 1.1968 - accuracy: 0.8009 - val_loss: 1.1866 - val_accuracy: 0.8519\n",
            "Epoch 44/50\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 1.2151 - accuracy: 0.7731 - val_loss: 1.1535 - val_accuracy: 0.8519\n",
            "Epoch 45/50\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 1.0749 - accuracy: 0.8194 - val_loss: 1.1202 - val_accuracy: 0.8519\n",
            "Epoch 46/50\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 1.1521 - accuracy: 0.8194 - val_loss: 1.0880 - val_accuracy: 0.8519\n",
            "Epoch 47/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.0377 - accuracy: 0.8380 - val_loss: 1.0576 - val_accuracy: 0.8519\n",
            "Epoch 48/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.9772 - accuracy: 0.8519 - val_loss: 1.0276 - val_accuracy: 0.8519\n",
            "Epoch 49/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.9947 - accuracy: 0.8056 - val_loss: 1.0004 - val_accuracy: 0.8519\n",
            "Epoch 50/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.9886 - accuracy: 0.8426 - val_loss: 0.9748 - val_accuracy: 0.8519\n",
            "Epoch 1/50\n",
            "7/7 [==============================] - 1s 42ms/step - loss: 6.1280 - accuracy: 0.5231 - val_loss: 5.5881 - val_accuracy: 0.7593\n",
            "Epoch 2/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 5.7215 - accuracy: 0.5926 - val_loss: 5.3641 - val_accuracy: 0.7407\n",
            "Epoch 3/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 5.6007 - accuracy: 0.5556 - val_loss: 5.1501 - val_accuracy: 0.7593\n",
            "Epoch 4/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 5.2788 - accuracy: 0.6019 - val_loss: 4.9425 - val_accuracy: 0.7593\n",
            "Epoch 5/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 5.0964 - accuracy: 0.5741 - val_loss: 4.7445 - val_accuracy: 0.7593\n",
            "Epoch 6/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 4.9172 - accuracy: 0.5972 - val_loss: 4.5545 - val_accuracy: 0.7778\n",
            "Epoch 7/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 4.7483 - accuracy: 0.6065 - val_loss: 4.3734 - val_accuracy: 0.7963\n",
            "Epoch 8/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 4.4830 - accuracy: 0.6250 - val_loss: 4.2008 - val_accuracy: 0.7963\n",
            "Epoch 9/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 4.3671 - accuracy: 0.6065 - val_loss: 4.0348 - val_accuracy: 0.7963\n",
            "Epoch 10/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 4.1457 - accuracy: 0.6204 - val_loss: 3.8766 - val_accuracy: 0.8148\n",
            "Epoch 11/50\n",
            "7/7 [==============================] - 0s 22ms/step - loss: 4.0778 - accuracy: 0.5972 - val_loss: 3.7267 - val_accuracy: 0.7963\n",
            "Epoch 12/50\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 3.8839 - accuracy: 0.6065 - val_loss: 3.5832 - val_accuracy: 0.7963\n",
            "Epoch 13/50\n",
            "7/7 [==============================] - 0s 21ms/step - loss: 3.7149 - accuracy: 0.6852 - val_loss: 3.4458 - val_accuracy: 0.7963\n",
            "Epoch 14/50\n",
            "7/7 [==============================] - 0s 17ms/step - loss: 3.5292 - accuracy: 0.6435 - val_loss: 3.3147 - val_accuracy: 0.8148\n",
            "Epoch 15/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 3.4985 - accuracy: 0.6065 - val_loss: 3.1896 - val_accuracy: 0.7963\n",
            "Epoch 16/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 3.3840 - accuracy: 0.5833 - val_loss: 3.0715 - val_accuracy: 0.7963\n",
            "Epoch 17/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 3.1345 - accuracy: 0.6620 - val_loss: 2.9581 - val_accuracy: 0.7963\n",
            "Epoch 18/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 3.0653 - accuracy: 0.6759 - val_loss: 2.8498 - val_accuracy: 0.8148\n",
            "Epoch 19/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 2.9079 - accuracy: 0.6528 - val_loss: 2.7460 - val_accuracy: 0.8148\n",
            "Epoch 20/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 2.7579 - accuracy: 0.6806 - val_loss: 2.6460 - val_accuracy: 0.7963\n",
            "Epoch 21/50\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 2.8222 - accuracy: 0.6157 - val_loss: 2.5499 - val_accuracy: 0.8148\n",
            "Epoch 22/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 2.6550 - accuracy: 0.6620 - val_loss: 2.4584 - val_accuracy: 0.8148\n",
            "Epoch 23/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 2.5476 - accuracy: 0.6852 - val_loss: 2.3720 - val_accuracy: 0.8148\n",
            "Epoch 24/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 2.5278 - accuracy: 0.6806 - val_loss: 2.2893 - val_accuracy: 0.8148\n",
            "Epoch 25/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 2.3020 - accuracy: 0.7269 - val_loss: 2.2076 - val_accuracy: 0.8148\n",
            "Epoch 26/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 2.2266 - accuracy: 0.7222 - val_loss: 2.1298 - val_accuracy: 0.8148\n",
            "Epoch 27/50\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 2.1873 - accuracy: 0.7130 - val_loss: 2.0562 - val_accuracy: 0.8148\n",
            "Epoch 28/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 2.1186 - accuracy: 0.6898 - val_loss: 1.9848 - val_accuracy: 0.8148\n",
            "Epoch 29/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 2.0522 - accuracy: 0.7361 - val_loss: 1.9179 - val_accuracy: 0.8148\n",
            "Epoch 30/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.9641 - accuracy: 0.7454 - val_loss: 1.8542 - val_accuracy: 0.7963\n",
            "Epoch 31/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.8725 - accuracy: 0.7269 - val_loss: 1.7930 - val_accuracy: 0.7963\n",
            "Epoch 32/50\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 1.8309 - accuracy: 0.6991 - val_loss: 1.7336 - val_accuracy: 0.7963\n",
            "Epoch 33/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.8033 - accuracy: 0.7454 - val_loss: 1.6774 - val_accuracy: 0.7963\n",
            "Epoch 34/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.7049 - accuracy: 0.7639 - val_loss: 1.6215 - val_accuracy: 0.7963\n",
            "Epoch 35/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 1.7146 - accuracy: 0.7407 - val_loss: 1.5707 - val_accuracy: 0.7963\n",
            "Epoch 36/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.6122 - accuracy: 0.7500 - val_loss: 1.5236 - val_accuracy: 0.7963\n",
            "Epoch 37/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 1.4771 - accuracy: 0.7778 - val_loss: 1.4779 - val_accuracy: 0.7963\n",
            "Epoch 38/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.4753 - accuracy: 0.7824 - val_loss: 1.4321 - val_accuracy: 0.7963\n",
            "Epoch 39/50\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 1.4110 - accuracy: 0.7824 - val_loss: 1.3891 - val_accuracy: 0.7963\n",
            "Epoch 40/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.4168 - accuracy: 0.7639 - val_loss: 1.3468 - val_accuracy: 0.7963\n",
            "Epoch 41/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 1.3625 - accuracy: 0.7315 - val_loss: 1.3061 - val_accuracy: 0.8148\n",
            "Epoch 42/50\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 1.3860 - accuracy: 0.7269 - val_loss: 1.2679 - val_accuracy: 0.8148\n",
            "Epoch 43/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.2853 - accuracy: 0.7778 - val_loss: 1.2328 - val_accuracy: 0.8148\n",
            "Epoch 44/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.2171 - accuracy: 0.8148 - val_loss: 1.1992 - val_accuracy: 0.7963\n",
            "Epoch 45/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.2023 - accuracy: 0.7870 - val_loss: 1.1665 - val_accuracy: 0.7963\n",
            "Epoch 46/50\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 1.1120 - accuracy: 0.8009 - val_loss: 1.1335 - val_accuracy: 0.7963\n",
            "Epoch 47/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.1570 - accuracy: 0.7685 - val_loss: 1.1015 - val_accuracy: 0.7963\n",
            "Epoch 48/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 1.1082 - accuracy: 0.7917 - val_loss: 1.0712 - val_accuracy: 0.8148\n",
            "Epoch 49/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.0819 - accuracy: 0.7778 - val_loss: 1.0429 - val_accuracy: 0.8148\n",
            "Epoch 50/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 1.0219 - accuracy: 0.7824 - val_loss: 1.0146 - val_accuracy: 0.8148\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "on gardera 0.4 car c'est celui qui est capable d'avoir la meilleure acc"
      ],
      "metadata": {
        "id": "Quk9H1nKd3HW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we add layers, we have to increase the number of epochs"
      ],
      "metadata": {
        "id": "B2aPJRzAdzBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model9=keras.Sequential([\n",
        "    keras.layers.Dense(32,activation='relu',kernel_regularizer= keras.regularizers.l2(0.1)),\n",
        "    keras.layers.Dropout(0.4),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(32, activation='relu',kernel_regularizer= keras.regularizers.l2(0.1)),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dropout(0.4),\n",
        "    keras.layers.Dense(32, activation='relu',kernel_regularizer= keras.regularizers.l2(0.1)),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dropout(0.4),\n",
        "    keras.layers.Dense(32, activation='relu',kernel_regularizer= keras.regularizers.l2(0.1)),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dropout(0.4),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "model9.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit model\n",
        "original_hist9 = model9.fit(trainX, trainy, validation_data=(testX, testy), epochs=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYAuiIiWO_bW",
        "outputId": "3641d44d-1fc8-446d-903b-5df2bb620fcf"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "7/7 [==============================] - 3s 56ms/step - loss: 12.2243 - accuracy: 0.5185 - val_loss: 11.6987 - val_accuracy: 0.3704\n",
            "Epoch 2/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 11.7118 - accuracy: 0.4954 - val_loss: 11.2188 - val_accuracy: 0.3889\n",
            "Epoch 3/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 11.1014 - accuracy: 0.5509 - val_loss: 10.7543 - val_accuracy: 0.4259\n",
            "Epoch 4/200\n",
            "7/7 [==============================] - 0s 18ms/step - loss: 10.7297 - accuracy: 0.5093 - val_loss: 10.3072 - val_accuracy: 0.4815\n",
            "Epoch 5/200\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 10.2486 - accuracy: 0.5648 - val_loss: 9.8818 - val_accuracy: 0.5185\n",
            "Epoch 6/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 9.9459 - accuracy: 0.4815 - val_loss: 9.4765 - val_accuracy: 0.5370\n",
            "Epoch 7/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 9.4507 - accuracy: 0.5463 - val_loss: 9.0886 - val_accuracy: 0.5926\n",
            "Epoch 8/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 9.0422 - accuracy: 0.5463 - val_loss: 8.7170 - val_accuracy: 0.6111\n",
            "Epoch 9/200\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 8.6392 - accuracy: 0.5694 - val_loss: 8.3608 - val_accuracy: 0.6296\n",
            "Epoch 10/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 8.2802 - accuracy: 0.5648 - val_loss: 8.0204 - val_accuracy: 0.6296\n",
            "Epoch 11/200\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 7.9701 - accuracy: 0.6111 - val_loss: 7.6956 - val_accuracy: 0.6296\n",
            "Epoch 12/200\n",
            "7/7 [==============================] - 0s 30ms/step - loss: 7.6331 - accuracy: 0.5833 - val_loss: 7.3849 - val_accuracy: 0.6481\n",
            "Epoch 13/200\n",
            "7/7 [==============================] - 0s 40ms/step - loss: 7.3492 - accuracy: 0.5880 - val_loss: 7.0888 - val_accuracy: 0.6667\n",
            "Epoch 14/200\n",
            "7/7 [==============================] - 0s 22ms/step - loss: 6.9934 - accuracy: 0.6343 - val_loss: 6.8050 - val_accuracy: 0.6852\n",
            "Epoch 15/200\n",
            "7/7 [==============================] - 0s 23ms/step - loss: 6.7392 - accuracy: 0.6250 - val_loss: 6.5325 - val_accuracy: 0.6852\n",
            "Epoch 16/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 6.4858 - accuracy: 0.6157 - val_loss: 6.2729 - val_accuracy: 0.6852\n",
            "Epoch 17/200\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 6.1401 - accuracy: 0.6713 - val_loss: 6.0235 - val_accuracy: 0.6852\n",
            "Epoch 18/200\n",
            "7/7 [==============================] - 0s 18ms/step - loss: 5.9363 - accuracy: 0.6065 - val_loss: 5.7850 - val_accuracy: 0.7037\n",
            "Epoch 19/200\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 5.6888 - accuracy: 0.6343 - val_loss: 5.5557 - val_accuracy: 0.7037\n",
            "Epoch 20/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 5.4046 - accuracy: 0.6713 - val_loss: 5.3361 - val_accuracy: 0.7037\n",
            "Epoch 21/200\n",
            "7/7 [==============================] - 0s 18ms/step - loss: 5.2265 - accuracy: 0.6481 - val_loss: 5.1258 - val_accuracy: 0.7037\n",
            "Epoch 22/200\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 5.0550 - accuracy: 0.6759 - val_loss: 4.9254 - val_accuracy: 0.7037\n",
            "Epoch 23/200\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 4.8361 - accuracy: 0.6157 - val_loss: 4.7337 - val_accuracy: 0.7037\n",
            "Epoch 24/200\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 4.5697 - accuracy: 0.7083 - val_loss: 4.5491 - val_accuracy: 0.7222\n",
            "Epoch 25/200\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 4.3989 - accuracy: 0.7222 - val_loss: 4.3708 - val_accuracy: 0.7222\n",
            "Epoch 26/200\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 4.2738 - accuracy: 0.6667 - val_loss: 4.2018 - val_accuracy: 0.7222\n",
            "Epoch 27/200\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 4.0296 - accuracy: 0.7083 - val_loss: 4.0399 - val_accuracy: 0.7222\n",
            "Epoch 28/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 3.9293 - accuracy: 0.7361 - val_loss: 3.8850 - val_accuracy: 0.7222\n",
            "Epoch 29/200\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 3.7559 - accuracy: 0.7130 - val_loss: 3.7389 - val_accuracy: 0.7593\n",
            "Epoch 30/200\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 3.6213 - accuracy: 0.7407 - val_loss: 3.5993 - val_accuracy: 0.7593\n",
            "Epoch 31/200\n",
            "7/7 [==============================] - 0s 17ms/step - loss: 3.5340 - accuracy: 0.7130 - val_loss: 3.4663 - val_accuracy: 0.7593\n",
            "Epoch 32/200\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 3.2874 - accuracy: 0.7546 - val_loss: 3.3385 - val_accuracy: 0.7778\n",
            "Epoch 33/200\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 3.2267 - accuracy: 0.7824 - val_loss: 3.2162 - val_accuracy: 0.7778\n",
            "Epoch 34/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 3.1497 - accuracy: 0.7269 - val_loss: 3.0976 - val_accuracy: 0.7593\n",
            "Epoch 35/200\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 2.9013 - accuracy: 0.7778 - val_loss: 2.9819 - val_accuracy: 0.7593\n",
            "Epoch 36/200\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 2.8494 - accuracy: 0.7824 - val_loss: 2.8720 - val_accuracy: 0.7593\n",
            "Epoch 37/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 2.7630 - accuracy: 0.7222 - val_loss: 2.7679 - val_accuracy: 0.7593\n",
            "Epoch 38/200\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 2.6720 - accuracy: 0.7593 - val_loss: 2.6691 - val_accuracy: 0.7593\n",
            "Epoch 39/200\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 2.5139 - accuracy: 0.7778 - val_loss: 2.5730 - val_accuracy: 0.7593\n",
            "Epoch 40/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 2.4244 - accuracy: 0.7685 - val_loss: 2.4782 - val_accuracy: 0.7407\n",
            "Epoch 41/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 2.3393 - accuracy: 0.8241 - val_loss: 2.3900 - val_accuracy: 0.7778\n",
            "Epoch 42/200\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 2.2382 - accuracy: 0.8009 - val_loss: 2.3074 - val_accuracy: 0.7778\n",
            "Epoch 43/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 2.1930 - accuracy: 0.7963 - val_loss: 2.2258 - val_accuracy: 0.7963\n",
            "Epoch 44/200\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 2.0758 - accuracy: 0.7870 - val_loss: 2.1498 - val_accuracy: 0.7963\n",
            "Epoch 45/200\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 2.0025 - accuracy: 0.8287 - val_loss: 2.0775 - val_accuracy: 0.7963\n",
            "Epoch 46/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 1.9311 - accuracy: 0.7778 - val_loss: 2.0048 - val_accuracy: 0.7778\n",
            "Epoch 47/200\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 1.9330 - accuracy: 0.7685 - val_loss: 1.9380 - val_accuracy: 0.7778\n",
            "Epoch 48/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 1.7715 - accuracy: 0.8241 - val_loss: 1.8756 - val_accuracy: 0.7778\n",
            "Epoch 49/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 1.7421 - accuracy: 0.8009 - val_loss: 1.8156 - val_accuracy: 0.7778\n",
            "Epoch 50/200\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 1.6796 - accuracy: 0.8148 - val_loss: 1.7513 - val_accuracy: 0.7593\n",
            "Epoch 51/200\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 1.6335 - accuracy: 0.8194 - val_loss: 1.6909 - val_accuracy: 0.7778\n",
            "Epoch 52/200\n",
            "7/7 [==============================] - 0s 18ms/step - loss: 1.6257 - accuracy: 0.7870 - val_loss: 1.6370 - val_accuracy: 0.7963\n",
            "Epoch 53/200\n",
            "7/7 [==============================] - 0s 17ms/step - loss: 1.5298 - accuracy: 0.8148 - val_loss: 1.5861 - val_accuracy: 0.8148\n",
            "Epoch 54/200\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 1.4763 - accuracy: 0.8287 - val_loss: 1.5383 - val_accuracy: 0.8148\n",
            "Epoch 55/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 1.5069 - accuracy: 0.8287 - val_loss: 1.4924 - val_accuracy: 0.7963\n",
            "Epoch 56/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 1.3572 - accuracy: 0.8380 - val_loss: 1.4519 - val_accuracy: 0.8333\n",
            "Epoch 57/200\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 1.3402 - accuracy: 0.8148 - val_loss: 1.4160 - val_accuracy: 0.8148\n",
            "Epoch 58/200\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 1.3043 - accuracy: 0.8519 - val_loss: 1.3757 - val_accuracy: 0.8519\n",
            "Epoch 59/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 1.2785 - accuracy: 0.8241 - val_loss: 1.3418 - val_accuracy: 0.8148\n",
            "Epoch 60/200\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 1.2377 - accuracy: 0.8380 - val_loss: 1.3085 - val_accuracy: 0.7963\n",
            "Epoch 61/200\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 1.1496 - accuracy: 0.8380 - val_loss: 1.2646 - val_accuracy: 0.7963\n",
            "Epoch 62/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 1.1045 - accuracy: 0.8472 - val_loss: 1.2214 - val_accuracy: 0.8148\n",
            "Epoch 63/200\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 1.0939 - accuracy: 0.8565 - val_loss: 1.1789 - val_accuracy: 0.8519\n",
            "Epoch 64/200\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 1.1270 - accuracy: 0.8102 - val_loss: 1.1372 - val_accuracy: 0.8704\n",
            "Epoch 65/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 1.0727 - accuracy: 0.8194 - val_loss: 1.1144 - val_accuracy: 0.8704\n",
            "Epoch 66/200\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.9798 - accuracy: 0.8565 - val_loss: 1.0878 - val_accuracy: 0.8704\n",
            "Epoch 67/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 1.0195 - accuracy: 0.8241 - val_loss: 1.0626 - val_accuracy: 0.8704\n",
            "Epoch 68/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.9918 - accuracy: 0.8380 - val_loss: 1.0449 - val_accuracy: 0.8704\n",
            "Epoch 69/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.9016 - accuracy: 0.8843 - val_loss: 1.0210 - val_accuracy: 0.8889\n",
            "Epoch 70/200\n",
            "7/7 [==============================] - 0s 18ms/step - loss: 0.8936 - accuracy: 0.8657 - val_loss: 0.9851 - val_accuracy: 0.8889\n",
            "Epoch 71/200\n",
            "7/7 [==============================] - 0s 18ms/step - loss: 0.8691 - accuracy: 0.8657 - val_loss: 0.9560 - val_accuracy: 0.8889\n",
            "Epoch 72/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.8085 - accuracy: 0.8889 - val_loss: 0.9318 - val_accuracy: 0.8889\n",
            "Epoch 73/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.8303 - accuracy: 0.8565 - val_loss: 0.9130 - val_accuracy: 0.8704\n",
            "Epoch 74/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.8265 - accuracy: 0.8565 - val_loss: 0.8950 - val_accuracy: 0.8889\n",
            "Epoch 75/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.8155 - accuracy: 0.8611 - val_loss: 0.8796 - val_accuracy: 0.8704\n",
            "Epoch 76/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.7767 - accuracy: 0.8611 - val_loss: 0.8624 - val_accuracy: 0.8704\n",
            "Epoch 77/200\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.7767 - accuracy: 0.8426 - val_loss: 0.8518 - val_accuracy: 0.8519\n",
            "Epoch 78/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.7284 - accuracy: 0.8750 - val_loss: 0.8395 - val_accuracy: 0.8519\n",
            "Epoch 79/200\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.7377 - accuracy: 0.8611 - val_loss: 0.8239 - val_accuracy: 0.8704\n",
            "Epoch 80/200\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.7506 - accuracy: 0.8519 - val_loss: 0.8131 - val_accuracy: 0.8704\n",
            "Epoch 81/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.6872 - accuracy: 0.8519 - val_loss: 0.8025 - val_accuracy: 0.8704\n",
            "Epoch 82/200\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.6972 - accuracy: 0.8519 - val_loss: 0.7882 - val_accuracy: 0.8704\n",
            "Epoch 83/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.6604 - accuracy: 0.8657 - val_loss: 0.7616 - val_accuracy: 0.8704\n",
            "Epoch 84/200\n",
            "7/7 [==============================] - 0s 40ms/step - loss: 0.6403 - accuracy: 0.8565 - val_loss: 0.7382 - val_accuracy: 0.8889\n",
            "Epoch 85/200\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.6584 - accuracy: 0.8565 - val_loss: 0.7252 - val_accuracy: 0.8519\n",
            "Epoch 86/200\n",
            "7/7 [==============================] - 0s 25ms/step - loss: 0.6043 - accuracy: 0.8657 - val_loss: 0.7215 - val_accuracy: 0.8704\n",
            "Epoch 87/200\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.6345 - accuracy: 0.8472 - val_loss: 0.7160 - val_accuracy: 0.8519\n",
            "Epoch 88/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.6306 - accuracy: 0.8796 - val_loss: 0.7093 - val_accuracy: 0.8519\n",
            "Epoch 89/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.6107 - accuracy: 0.8611 - val_loss: 0.6904 - val_accuracy: 0.8519\n",
            "Epoch 90/200\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.5893 - accuracy: 0.8796 - val_loss: 0.6785 - val_accuracy: 0.8519\n",
            "Epoch 91/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.6172 - accuracy: 0.8426 - val_loss: 0.6724 - val_accuracy: 0.8704\n",
            "Epoch 92/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.5977 - accuracy: 0.8611 - val_loss: 0.6605 - val_accuracy: 0.8519\n",
            "Epoch 93/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.5206 - accuracy: 0.8750 - val_loss: 0.6471 - val_accuracy: 0.8519\n",
            "Epoch 94/200\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.5208 - accuracy: 0.8889 - val_loss: 0.6362 - val_accuracy: 0.8519\n",
            "Epoch 95/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.5259 - accuracy: 0.8750 - val_loss: 0.6312 - val_accuracy: 0.8519\n",
            "Epoch 96/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.5410 - accuracy: 0.8750 - val_loss: 0.6245 - val_accuracy: 0.8519\n",
            "Epoch 97/200\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.4676 - accuracy: 0.8796 - val_loss: 0.6125 - val_accuracy: 0.8519\n",
            "Epoch 98/200\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 0.4920 - accuracy: 0.8796 - val_loss: 0.5986 - val_accuracy: 0.8519\n",
            "Epoch 99/200\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.5227 - accuracy: 0.8704 - val_loss: 0.5960 - val_accuracy: 0.8519\n",
            "Epoch 100/200\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.5382 - accuracy: 0.8472 - val_loss: 0.5899 - val_accuracy: 0.8519\n",
            "Epoch 101/200\n",
            "7/7 [==============================] - 0s 19ms/step - loss: 0.4808 - accuracy: 0.8843 - val_loss: 0.5893 - val_accuracy: 0.8519\n",
            "Epoch 102/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4348 - accuracy: 0.9074 - val_loss: 0.5776 - val_accuracy: 0.8519\n",
            "Epoch 103/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.4643 - accuracy: 0.8750 - val_loss: 0.5739 - val_accuracy: 0.8519\n",
            "Epoch 104/200\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.4963 - accuracy: 0.8611 - val_loss: 0.5723 - val_accuracy: 0.8519\n",
            "Epoch 105/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4708 - accuracy: 0.8750 - val_loss: 0.5738 - val_accuracy: 0.8519\n",
            "Epoch 106/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4426 - accuracy: 0.8796 - val_loss: 0.5692 - val_accuracy: 0.8519\n",
            "Epoch 107/200\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.4832 - accuracy: 0.8796 - val_loss: 0.5568 - val_accuracy: 0.8519\n",
            "Epoch 108/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.5092 - accuracy: 0.8519 - val_loss: 0.5572 - val_accuracy: 0.8519\n",
            "Epoch 109/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.4428 - accuracy: 0.8981 - val_loss: 0.5482 - val_accuracy: 0.8519\n",
            "Epoch 110/200\n",
            "7/7 [==============================] - 0s 21ms/step - loss: 0.3978 - accuracy: 0.8981 - val_loss: 0.5466 - val_accuracy: 0.8519\n",
            "Epoch 111/200\n",
            "7/7 [==============================] - 0s 18ms/step - loss: 0.4223 - accuracy: 0.8889 - val_loss: 0.5416 - val_accuracy: 0.8704\n",
            "Epoch 112/200\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 0.4142 - accuracy: 0.8981 - val_loss: 0.5330 - val_accuracy: 0.8704\n",
            "Epoch 113/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4290 - accuracy: 0.8750 - val_loss: 0.5300 - val_accuracy: 0.8704\n",
            "Epoch 114/200\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.4288 - accuracy: 0.8843 - val_loss: 0.5273 - val_accuracy: 0.8704\n",
            "Epoch 115/200\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.3950 - accuracy: 0.9120 - val_loss: 0.5397 - val_accuracy: 0.8704\n",
            "Epoch 116/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4054 - accuracy: 0.8796 - val_loss: 0.5491 - val_accuracy: 0.8704\n",
            "Epoch 117/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4489 - accuracy: 0.8750 - val_loss: 0.5342 - val_accuracy: 0.8704\n",
            "Epoch 118/200\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 0.3903 - accuracy: 0.8796 - val_loss: 0.5145 - val_accuracy: 0.8704\n",
            "Epoch 119/200\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.3469 - accuracy: 0.9213 - val_loss: 0.5163 - val_accuracy: 0.8704\n",
            "Epoch 120/200\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 0.3375 - accuracy: 0.9120 - val_loss: 0.5235 - val_accuracy: 0.8704\n",
            "Epoch 121/200\n",
            "7/7 [==============================] - 0s 30ms/step - loss: 0.3837 - accuracy: 0.8981 - val_loss: 0.5255 - val_accuracy: 0.8704\n",
            "Epoch 122/200\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.4395 - accuracy: 0.8704 - val_loss: 0.5146 - val_accuracy: 0.8704\n",
            "Epoch 123/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.3819 - accuracy: 0.8889 - val_loss: 0.5010 - val_accuracy: 0.8704\n",
            "Epoch 124/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3953 - accuracy: 0.8889 - val_loss: 0.4859 - val_accuracy: 0.8704\n",
            "Epoch 125/200\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.3817 - accuracy: 0.8843 - val_loss: 0.4963 - val_accuracy: 0.8704\n",
            "Epoch 126/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.3626 - accuracy: 0.8981 - val_loss: 0.5119 - val_accuracy: 0.8704\n",
            "Epoch 127/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.3776 - accuracy: 0.8889 - val_loss: 0.5344 - val_accuracy: 0.8704\n",
            "Epoch 128/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4185 - accuracy: 0.9028 - val_loss: 0.5389 - val_accuracy: 0.8704\n",
            "Epoch 129/200\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.3844 - accuracy: 0.9074 - val_loss: 0.5242 - val_accuracy: 0.8704\n",
            "Epoch 130/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.3796 - accuracy: 0.8750 - val_loss: 0.5263 - val_accuracy: 0.8704\n",
            "Epoch 131/200\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.4344 - accuracy: 0.8657 - val_loss: 0.5289 - val_accuracy: 0.8704\n",
            "Epoch 132/200\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.3856 - accuracy: 0.8796 - val_loss: 0.5390 - val_accuracy: 0.8704\n",
            "Epoch 133/200\n",
            "7/7 [==============================] - 0s 18ms/step - loss: 0.3597 - accuracy: 0.9074 - val_loss: 0.5253 - val_accuracy: 0.8704\n",
            "Epoch 134/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.3327 - accuracy: 0.9167 - val_loss: 0.5075 - val_accuracy: 0.8704\n",
            "Epoch 135/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.3754 - accuracy: 0.8704 - val_loss: 0.5064 - val_accuracy: 0.8889\n",
            "Epoch 136/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.3721 - accuracy: 0.8935 - val_loss: 0.5200 - val_accuracy: 0.8704\n",
            "Epoch 137/200\n",
            "7/7 [==============================] - 0s 17ms/step - loss: 0.3176 - accuracy: 0.9213 - val_loss: 0.5179 - val_accuracy: 0.8889\n",
            "Epoch 138/200\n",
            "7/7 [==============================] - 0s 17ms/step - loss: 0.3636 - accuracy: 0.8796 - val_loss: 0.5155 - val_accuracy: 0.8889\n",
            "Epoch 139/200\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.3033 - accuracy: 0.9120 - val_loss: 0.5246 - val_accuracy: 0.8889\n",
            "Epoch 140/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.3538 - accuracy: 0.9074 - val_loss: 0.5252 - val_accuracy: 0.8889\n",
            "Epoch 141/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.3512 - accuracy: 0.8843 - val_loss: 0.5286 - val_accuracy: 0.8704\n",
            "Epoch 142/200\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.3943 - accuracy: 0.9028 - val_loss: 0.5228 - val_accuracy: 0.8889\n",
            "Epoch 143/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.3164 - accuracy: 0.8981 - val_loss: 0.5216 - val_accuracy: 0.8889\n",
            "Epoch 144/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.3201 - accuracy: 0.9028 - val_loss: 0.5282 - val_accuracy: 0.8889\n",
            "Epoch 145/200\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.3057 - accuracy: 0.9074 - val_loss: 0.5318 - val_accuracy: 0.8889\n",
            "Epoch 146/200\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 0.3662 - accuracy: 0.9028 - val_loss: 0.5257 - val_accuracy: 0.8889\n",
            "Epoch 147/200\n",
            "7/7 [==============================] - 0s 17ms/step - loss: 0.3582 - accuracy: 0.9028 - val_loss: 0.5114 - val_accuracy: 0.8704\n",
            "Epoch 148/200\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 0.3291 - accuracy: 0.8935 - val_loss: 0.5175 - val_accuracy: 0.8704\n",
            "Epoch 149/200\n",
            "7/7 [==============================] - 0s 17ms/step - loss: 0.3658 - accuracy: 0.9028 - val_loss: 0.5356 - val_accuracy: 0.8519\n",
            "Epoch 150/200\n",
            "7/7 [==============================] - 0s 22ms/step - loss: 0.3838 - accuracy: 0.8565 - val_loss: 0.5492 - val_accuracy: 0.8333\n",
            "Epoch 151/200\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 0.3143 - accuracy: 0.9028 - val_loss: 0.5492 - val_accuracy: 0.8333\n",
            "Epoch 152/200\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 0.3429 - accuracy: 0.8981 - val_loss: 0.5406 - val_accuracy: 0.8333\n",
            "Epoch 153/200\n",
            "7/7 [==============================] - 0s 24ms/step - loss: 0.3319 - accuracy: 0.9167 - val_loss: 0.5447 - val_accuracy: 0.8704\n",
            "Epoch 154/200\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 0.2871 - accuracy: 0.9213 - val_loss: 0.5611 - val_accuracy: 0.8519\n",
            "Epoch 155/200\n",
            "7/7 [==============================] - 0s 22ms/step - loss: 0.3132 - accuracy: 0.9074 - val_loss: 0.5831 - val_accuracy: 0.8519\n",
            "Epoch 156/200\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 0.3122 - accuracy: 0.9213 - val_loss: 0.6089 - val_accuracy: 0.8333\n",
            "Epoch 157/200\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.3331 - accuracy: 0.9074 - val_loss: 0.6089 - val_accuracy: 0.8333\n",
            "Epoch 158/200\n",
            "7/7 [==============================] - 0s 24ms/step - loss: 0.2519 - accuracy: 0.9491 - val_loss: 0.5943 - val_accuracy: 0.8704\n",
            "Epoch 159/200\n",
            "7/7 [==============================] - 0s 20ms/step - loss: 0.2904 - accuracy: 0.9074 - val_loss: 0.5952 - val_accuracy: 0.8519\n",
            "Epoch 160/200\n",
            "7/7 [==============================] - 0s 18ms/step - loss: 0.2664 - accuracy: 0.9398 - val_loss: 0.5915 - val_accuracy: 0.8519\n",
            "Epoch 161/200\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 0.2939 - accuracy: 0.9120 - val_loss: 0.5868 - val_accuracy: 0.8519\n",
            "Epoch 162/200\n",
            "7/7 [==============================] - 0s 18ms/step - loss: 0.3103 - accuracy: 0.9213 - val_loss: 0.5582 - val_accuracy: 0.8889\n",
            "Epoch 163/200\n",
            "7/7 [==============================] - 0s 18ms/step - loss: 0.2659 - accuracy: 0.9259 - val_loss: 0.5769 - val_accuracy: 0.8519\n",
            "Epoch 164/200\n",
            "7/7 [==============================] - 0s 18ms/step - loss: 0.3270 - accuracy: 0.9167 - val_loss: 0.6008 - val_accuracy: 0.8519\n",
            "Epoch 165/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.3132 - accuracy: 0.9120 - val_loss: 0.5803 - val_accuracy: 0.8519\n",
            "Epoch 166/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.2581 - accuracy: 0.9259 - val_loss: 0.5754 - val_accuracy: 0.8519\n",
            "Epoch 167/200\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.2798 - accuracy: 0.9259 - val_loss: 0.5761 - val_accuracy: 0.8889\n",
            "Epoch 168/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.3128 - accuracy: 0.9074 - val_loss: 0.5815 - val_accuracy: 0.8889\n",
            "Epoch 169/200\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.2511 - accuracy: 0.9398 - val_loss: 0.5891 - val_accuracy: 0.8704\n",
            "Epoch 170/200\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.3132 - accuracy: 0.9167 - val_loss: 0.5815 - val_accuracy: 0.8704\n",
            "Epoch 171/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.3404 - accuracy: 0.9028 - val_loss: 0.5746 - val_accuracy: 0.8704\n",
            "Epoch 172/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.2920 - accuracy: 0.9167 - val_loss: 0.5627 - val_accuracy: 0.8889\n",
            "Epoch 173/200\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.3168 - accuracy: 0.9213 - val_loss: 0.5577 - val_accuracy: 0.8889\n",
            "Epoch 174/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.2797 - accuracy: 0.9074 - val_loss: 0.5571 - val_accuracy: 0.8889\n",
            "Epoch 175/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.2921 - accuracy: 0.9213 - val_loss: 0.5491 - val_accuracy: 0.8519\n",
            "Epoch 176/200\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.3014 - accuracy: 0.9259 - val_loss: 0.5453 - val_accuracy: 0.8519\n",
            "Epoch 177/200\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.4115 - accuracy: 0.8565 - val_loss: 0.5629 - val_accuracy: 0.8519\n",
            "Epoch 178/200\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.3615 - accuracy: 0.8796 - val_loss: 0.5710 - val_accuracy: 0.8704\n",
            "Epoch 179/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.2855 - accuracy: 0.9167 - val_loss: 0.6003 - val_accuracy: 0.8704\n",
            "Epoch 180/200\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.2944 - accuracy: 0.9259 - val_loss: 0.6001 - val_accuracy: 0.8704\n",
            "Epoch 181/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.2918 - accuracy: 0.9074 - val_loss: 0.5944 - val_accuracy: 0.8889\n",
            "Epoch 182/200\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.2715 - accuracy: 0.9213 - val_loss: 0.6000 - val_accuracy: 0.8889\n",
            "Epoch 183/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.3065 - accuracy: 0.9259 - val_loss: 0.5993 - val_accuracy: 0.8704\n",
            "Epoch 184/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.2575 - accuracy: 0.9444 - val_loss: 0.6212 - val_accuracy: 0.8519\n",
            "Epoch 185/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.2694 - accuracy: 0.9259 - val_loss: 0.6259 - val_accuracy: 0.8519\n",
            "Epoch 186/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.3072 - accuracy: 0.9028 - val_loss: 0.6222 - val_accuracy: 0.8519\n",
            "Epoch 187/200\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.2536 - accuracy: 0.9213 - val_loss: 0.6015 - val_accuracy: 0.8519\n",
            "Epoch 188/200\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.3290 - accuracy: 0.8843 - val_loss: 0.5902 - val_accuracy: 0.8704\n",
            "Epoch 189/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.3187 - accuracy: 0.8889 - val_loss: 0.6107 - val_accuracy: 0.8704\n",
            "Epoch 190/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.2943 - accuracy: 0.9028 - val_loss: 0.6388 - val_accuracy: 0.8519\n",
            "Epoch 191/200\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.2674 - accuracy: 0.9259 - val_loss: 0.6661 - val_accuracy: 0.8519\n",
            "Epoch 192/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.2501 - accuracy: 0.9028 - val_loss: 0.6769 - val_accuracy: 0.8519\n",
            "Epoch 193/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.3348 - accuracy: 0.9074 - val_loss: 0.6558 - val_accuracy: 0.8519\n",
            "Epoch 194/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.2589 - accuracy: 0.9259 - val_loss: 0.6292 - val_accuracy: 0.8704\n",
            "Epoch 195/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.3351 - accuracy: 0.9074 - val_loss: 0.6287 - val_accuracy: 0.8889\n",
            "Epoch 196/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.3267 - accuracy: 0.8889 - val_loss: 0.6939 - val_accuracy: 0.8519\n",
            "Epoch 197/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.2900 - accuracy: 0.9120 - val_loss: 0.7247 - val_accuracy: 0.8519\n",
            "Epoch 198/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.2407 - accuracy: 0.9398 - val_loss: 0.7047 - val_accuracy: 0.8704\n",
            "Epoch 199/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.2570 - accuracy: 0.9120 - val_loss: 0.6887 - val_accuracy: 0.8704\n",
            "Epoch 200/200\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.2784 - accuracy: 0.9352 - val_loss: 0.7532 - val_accuracy: 0.8333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "on voit que le modèle n'est pas forcément plus performant"
      ],
      "metadata": {
        "id": "X7ZQHmDudhCi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "selu activation"
      ],
      "metadata": {
        "id": "E8b3UyLodj1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model10=keras.Sequential([\n",
        "    keras.layers.Dense(32,activation='selu',kernel_regularizer= keras.regularizers.l2(0.1)),\n",
        "    keras.layers.Dropout(0.4),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(32, activation='selu',kernel_regularizer= keras.regularizers.l2(0.1)),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dropout(0.4),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "model10.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit model\n",
        "original_hist10 = model10.fit(trainX, trainy, validation_data=(testX, testy), epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kihd8WpRP8Eg",
        "outputId": "7bcc2751-4b3a-497f-fdf1-7a891441dd9a"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "7/7 [==============================] - 3s 77ms/step - loss: 6.0518 - accuracy: 0.5324 - val_loss: 5.5753 - val_accuracy: 0.6481\n",
            "Epoch 2/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 5.6222 - accuracy: 0.6296 - val_loss: 5.2581 - val_accuracy: 0.6667\n",
            "Epoch 3/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 5.3772 - accuracy: 0.6019 - val_loss: 4.9774 - val_accuracy: 0.7037\n",
            "Epoch 4/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 5.1389 - accuracy: 0.6574 - val_loss: 4.7306 - val_accuracy: 0.7778\n",
            "Epoch 5/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 4.9452 - accuracy: 0.6713 - val_loss: 4.5009 - val_accuracy: 0.7778\n",
            "Epoch 6/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 4.6689 - accuracy: 0.7037 - val_loss: 4.2883 - val_accuracy: 0.8519\n",
            "Epoch 7/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 4.3867 - accuracy: 0.7130 - val_loss: 4.0899 - val_accuracy: 0.8519\n",
            "Epoch 8/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 4.2613 - accuracy: 0.6991 - val_loss: 3.9034 - val_accuracy: 0.8519\n",
            "Epoch 9/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 4.0154 - accuracy: 0.7546 - val_loss: 3.7260 - val_accuracy: 0.8704\n",
            "Epoch 10/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 3.8433 - accuracy: 0.7222 - val_loss: 3.5567 - val_accuracy: 0.8704\n",
            "Epoch 11/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 3.6240 - accuracy: 0.7639 - val_loss: 3.3945 - val_accuracy: 0.8704\n",
            "Epoch 12/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 3.4949 - accuracy: 0.7454 - val_loss: 3.2385 - val_accuracy: 0.8704\n",
            "Epoch 13/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 3.2688 - accuracy: 0.7870 - val_loss: 3.0902 - val_accuracy: 0.8704\n",
            "Epoch 14/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 3.2035 - accuracy: 0.7639 - val_loss: 2.9487 - val_accuracy: 0.8333\n",
            "Epoch 15/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 3.0675 - accuracy: 0.7824 - val_loss: 2.8142 - val_accuracy: 0.8519\n",
            "Epoch 16/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 2.8713 - accuracy: 0.7685 - val_loss: 2.6885 - val_accuracy: 0.8519\n",
            "Epoch 17/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 2.7731 - accuracy: 0.7963 - val_loss: 2.5692 - val_accuracy: 0.8519\n",
            "Epoch 18/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 2.6357 - accuracy: 0.7824 - val_loss: 2.4556 - val_accuracy: 0.8519\n",
            "Epoch 19/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 2.5412 - accuracy: 0.7778 - val_loss: 2.3507 - val_accuracy: 0.8704\n",
            "Epoch 20/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 2.4354 - accuracy: 0.7870 - val_loss: 2.2507 - val_accuracy: 0.8704\n",
            "Epoch 21/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 2.2815 - accuracy: 0.7500 - val_loss: 2.1546 - val_accuracy: 0.8704\n",
            "Epoch 22/50\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 2.1401 - accuracy: 0.8009 - val_loss: 2.0632 - val_accuracy: 0.8704\n",
            "Epoch 23/50\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 2.0923 - accuracy: 0.8102 - val_loss: 1.9766 - val_accuracy: 0.8704\n",
            "Epoch 24/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 2.0239 - accuracy: 0.8241 - val_loss: 1.8931 - val_accuracy: 0.8704\n",
            "Epoch 25/50\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 1.9188 - accuracy: 0.8102 - val_loss: 1.8142 - val_accuracy: 0.8889\n",
            "Epoch 26/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.8631 - accuracy: 0.8056 - val_loss: 1.7392 - val_accuracy: 0.8889\n",
            "Epoch 27/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.7385 - accuracy: 0.8148 - val_loss: 1.6693 - val_accuracy: 0.8889\n",
            "Epoch 28/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 1.6914 - accuracy: 0.8380 - val_loss: 1.6016 - val_accuracy: 0.8889\n",
            "Epoch 29/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.6850 - accuracy: 0.7778 - val_loss: 1.5365 - val_accuracy: 0.8889\n",
            "Epoch 30/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.5451 - accuracy: 0.8194 - val_loss: 1.4761 - val_accuracy: 0.8704\n",
            "Epoch 31/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 1.4835 - accuracy: 0.8287 - val_loss: 1.4207 - val_accuracy: 0.8704\n",
            "Epoch 32/50\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 1.4305 - accuracy: 0.8333 - val_loss: 1.3674 - val_accuracy: 0.8704\n",
            "Epoch 33/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.4502 - accuracy: 0.7731 - val_loss: 1.3207 - val_accuracy: 0.8704\n",
            "Epoch 34/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 1.3356 - accuracy: 0.8194 - val_loss: 1.2764 - val_accuracy: 0.8704\n",
            "Epoch 35/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 1.2561 - accuracy: 0.8287 - val_loss: 1.2275 - val_accuracy: 0.8704\n",
            "Epoch 36/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 1.2515 - accuracy: 0.8472 - val_loss: 1.1871 - val_accuracy: 0.8889\n",
            "Epoch 37/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.1528 - accuracy: 0.8426 - val_loss: 1.1498 - val_accuracy: 0.8889\n",
            "Epoch 38/50\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 1.2029 - accuracy: 0.8056 - val_loss: 1.1119 - val_accuracy: 0.8889\n",
            "Epoch 39/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 1.1263 - accuracy: 0.8194 - val_loss: 1.0789 - val_accuracy: 0.8889\n",
            "Epoch 40/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.0784 - accuracy: 0.8194 - val_loss: 1.0425 - val_accuracy: 0.8889\n",
            "Epoch 41/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.0640 - accuracy: 0.7963 - val_loss: 1.0125 - val_accuracy: 0.8889\n",
            "Epoch 42/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.9824 - accuracy: 0.8241 - val_loss: 0.9834 - val_accuracy: 0.8889\n",
            "Epoch 43/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.9675 - accuracy: 0.8194 - val_loss: 0.9496 - val_accuracy: 0.8889\n",
            "Epoch 44/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.9092 - accuracy: 0.8519 - val_loss: 0.9161 - val_accuracy: 0.8889\n",
            "Epoch 45/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.8990 - accuracy: 0.8565 - val_loss: 0.8901 - val_accuracy: 0.8889\n",
            "Epoch 46/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.9391 - accuracy: 0.8148 - val_loss: 0.8688 - val_accuracy: 0.8704\n",
            "Epoch 47/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.8837 - accuracy: 0.8519 - val_loss: 0.8553 - val_accuracy: 0.8704\n",
            "Epoch 48/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.8674 - accuracy: 0.8333 - val_loss: 0.8364 - val_accuracy: 0.8704\n",
            "Epoch 49/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.8287 - accuracy: 0.8148 - val_loss: 0.8111 - val_accuracy: 0.8704\n",
            "Epoch 50/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.7674 - accuracy: 0.8333 - val_loss: 0.7869 - val_accuracy: 0.8889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "meilleures prédictions que relu"
      ],
      "metadata": {
        "id": "jnyatKRmdZYs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "elu activation"
      ],
      "metadata": {
        "id": "GzE_VPZZdcd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model11=keras.Sequential([\n",
        "    keras.layers.Dense(32,activation='elu',kernel_regularizer= keras.regularizers.l2(0.1)),\n",
        "    keras.layers.Dropout(0.4),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(32, activation='elu',kernel_regularizer= keras.regularizers.l2(0.1)),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dropout(0.4),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "model11.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit model\n",
        "original_hist11 = model11.fit(trainX, trainy, validation_data=(testX, testy), epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nRGHzMaQS5K",
        "outputId": "9920bd4b-35ec-4f11-b79b-013e51ff72f5"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "7/7 [==============================] - 1s 40ms/step - loss: 6.1951 - accuracy: 0.4583 - val_loss: 5.7078 - val_accuracy: 0.6667\n",
            "Epoch 2/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 5.8325 - accuracy: 0.5463 - val_loss: 5.4267 - val_accuracy: 0.7407\n",
            "Epoch 3/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 5.5368 - accuracy: 0.5787 - val_loss: 5.1673 - val_accuracy: 0.7222\n",
            "Epoch 4/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 5.2962 - accuracy: 0.6204 - val_loss: 4.9260 - val_accuracy: 0.7593\n",
            "Epoch 5/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 5.0187 - accuracy: 0.7222 - val_loss: 4.7021 - val_accuracy: 0.8148\n",
            "Epoch 6/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 4.7627 - accuracy: 0.7176 - val_loss: 4.4881 - val_accuracy: 0.8519\n",
            "Epoch 7/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 4.5785 - accuracy: 0.6713 - val_loss: 4.2837 - val_accuracy: 0.8519\n",
            "Epoch 8/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 4.4579 - accuracy: 0.7037 - val_loss: 4.0882 - val_accuracy: 0.8333\n",
            "Epoch 9/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 4.1038 - accuracy: 0.7546 - val_loss: 3.9066 - val_accuracy: 0.8333\n",
            "Epoch 10/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 3.9572 - accuracy: 0.7407 - val_loss: 3.7337 - val_accuracy: 0.8333\n",
            "Epoch 11/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 3.8257 - accuracy: 0.7269 - val_loss: 3.5663 - val_accuracy: 0.8333\n",
            "Epoch 12/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 3.6086 - accuracy: 0.7731 - val_loss: 3.4034 - val_accuracy: 0.8148\n",
            "Epoch 13/50\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 3.4320 - accuracy: 0.7778 - val_loss: 3.2472 - val_accuracy: 0.8333\n",
            "Epoch 14/50\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 3.2975 - accuracy: 0.7500 - val_loss: 3.1028 - val_accuracy: 0.8333\n",
            "Epoch 15/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 3.1593 - accuracy: 0.7824 - val_loss: 2.9638 - val_accuracy: 0.8519\n",
            "Epoch 16/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 2.9757 - accuracy: 0.8148 - val_loss: 2.8317 - val_accuracy: 0.8704\n",
            "Epoch 17/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 2.8421 - accuracy: 0.7778 - val_loss: 2.7067 - val_accuracy: 0.8704\n",
            "Epoch 18/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 2.7122 - accuracy: 0.8056 - val_loss: 2.5867 - val_accuracy: 0.8704\n",
            "Epoch 19/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 2.6061 - accuracy: 0.7593 - val_loss: 2.4727 - val_accuracy: 0.8704\n",
            "Epoch 20/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 2.5071 - accuracy: 0.8009 - val_loss: 2.3660 - val_accuracy: 0.8704\n",
            "Epoch 21/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 2.3730 - accuracy: 0.8148 - val_loss: 2.2646 - val_accuracy: 0.8519\n",
            "Epoch 22/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 2.3201 - accuracy: 0.7685 - val_loss: 2.1678 - val_accuracy: 0.8704\n",
            "Epoch 23/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 2.2359 - accuracy: 0.7824 - val_loss: 2.0774 - val_accuracy: 0.8519\n",
            "Epoch 24/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 2.1437 - accuracy: 0.7685 - val_loss: 1.9931 - val_accuracy: 0.8704\n",
            "Epoch 25/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 1.9924 - accuracy: 0.8148 - val_loss: 1.9084 - val_accuracy: 0.8889\n",
            "Epoch 26/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 1.8963 - accuracy: 0.8194 - val_loss: 1.8301 - val_accuracy: 0.8889\n",
            "Epoch 27/50\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 1.8436 - accuracy: 0.8194 - val_loss: 1.7530 - val_accuracy: 0.8889\n",
            "Epoch 28/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 1.7701 - accuracy: 0.7917 - val_loss: 1.6840 - val_accuracy: 0.8889\n",
            "Epoch 29/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 1.6766 - accuracy: 0.8102 - val_loss: 1.6174 - val_accuracy: 0.8889\n",
            "Epoch 30/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 1.6906 - accuracy: 0.7917 - val_loss: 1.5552 - val_accuracy: 0.8889\n",
            "Epoch 31/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 1.5407 - accuracy: 0.8565 - val_loss: 1.4944 - val_accuracy: 0.8889\n",
            "Epoch 32/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.5496 - accuracy: 0.8241 - val_loss: 1.4368 - val_accuracy: 0.8889\n",
            "Epoch 33/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 1.4124 - accuracy: 0.8426 - val_loss: 1.3833 - val_accuracy: 0.8889\n",
            "Epoch 34/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 1.3779 - accuracy: 0.8148 - val_loss: 1.3384 - val_accuracy: 0.8889\n",
            "Epoch 35/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 1.3621 - accuracy: 0.8287 - val_loss: 1.2910 - val_accuracy: 0.8889\n",
            "Epoch 36/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 1.3117 - accuracy: 0.8102 - val_loss: 1.2461 - val_accuracy: 0.8889\n",
            "Epoch 37/50\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 1.2553 - accuracy: 0.8194 - val_loss: 1.1980 - val_accuracy: 0.8889\n",
            "Epoch 38/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 1.1910 - accuracy: 0.8472 - val_loss: 1.1551 - val_accuracy: 0.8704\n",
            "Epoch 39/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.2051 - accuracy: 0.7917 - val_loss: 1.1168 - val_accuracy: 0.8704\n",
            "Epoch 40/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 1.1050 - accuracy: 0.8102 - val_loss: 1.0779 - val_accuracy: 0.8704\n",
            "Epoch 41/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.0617 - accuracy: 0.8426 - val_loss: 1.0414 - val_accuracy: 0.8704\n",
            "Epoch 42/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 1.0430 - accuracy: 0.8333 - val_loss: 1.0100 - val_accuracy: 0.8889\n",
            "Epoch 43/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.0047 - accuracy: 0.8102 - val_loss: 0.9825 - val_accuracy: 0.8889\n",
            "Epoch 44/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.0022 - accuracy: 0.8148 - val_loss: 0.9529 - val_accuracy: 0.8889\n",
            "Epoch 45/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.9619 - accuracy: 0.8194 - val_loss: 0.9269 - val_accuracy: 0.8889\n",
            "Epoch 46/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.9494 - accuracy: 0.8426 - val_loss: 0.9005 - val_accuracy: 0.8889\n",
            "Epoch 47/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.8759 - accuracy: 0.8426 - val_loss: 0.8789 - val_accuracy: 0.8889\n",
            "Epoch 48/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.8630 - accuracy: 0.8380 - val_loss: 0.8583 - val_accuracy: 0.8889\n",
            "Epoch 49/50\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.8349 - accuracy: 0.8565 - val_loss: 0.8385 - val_accuracy: 0.8889\n",
            "Epoch 50/50\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.8378 - accuracy: 0.7963 - val_loss: 0.8162 - val_accuracy: 0.8889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "elu a l'air de converger encore plus rapidement vers une bonne accuracy"
      ],
      "metadata": {
        "id": "3h_xJtEOdTtr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model12=keras.Sequential([\n",
        "    keras.layers.Dense(50, activation='elu',kernel_regularizer= keras.regularizers.l2(0.1)),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dropout(0.4),\n",
        "    keras.layers.Dense(32, activation='elu',kernel_regularizer= keras.regularizers.l2(0.1)),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dropout(0.4),\n",
        "    keras.layers.Dense(32, activation='elu',kernel_regularizer= keras.regularizers.l2(0.1)),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dropout(0.4),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "model12.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "# fit model\n",
        "original_hist12 = model12.fit(trainX, trainy, validation_data=(testX, testy), epochs=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3C1Ea4vcQsMh",
        "outputId": "d6fc4c8c-04fb-4a67-e0c1-4e230dc0a513"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "7/7 [==============================] - 2s 43ms/step - loss: 9.5868 - accuracy: 0.5370 - val_loss: 8.8641 - val_accuracy: 0.7407\n",
            "Epoch 2/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 8.8704 - accuracy: 0.6343 - val_loss: 8.3143 - val_accuracy: 0.7778\n",
            "Epoch 3/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 8.2504 - accuracy: 0.6991 - val_loss: 7.8467 - val_accuracy: 0.7963\n",
            "Epoch 4/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 7.8142 - accuracy: 0.6944 - val_loss: 7.4218 - val_accuracy: 0.7963\n",
            "Epoch 5/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 7.4232 - accuracy: 0.6898 - val_loss: 7.0269 - val_accuracy: 0.8333\n",
            "Epoch 6/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 6.9387 - accuracy: 0.7639 - val_loss: 6.6537 - val_accuracy: 0.8519\n",
            "Epoch 7/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 6.6845 - accuracy: 0.7037 - val_loss: 6.3006 - val_accuracy: 0.8704\n",
            "Epoch 8/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 6.3316 - accuracy: 0.7222 - val_loss: 5.9608 - val_accuracy: 0.8704\n",
            "Epoch 9/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 5.8951 - accuracy: 0.7917 - val_loss: 5.6374 - val_accuracy: 0.8889\n",
            "Epoch 10/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 5.6353 - accuracy: 0.7546 - val_loss: 5.3351 - val_accuracy: 0.8704\n",
            "Epoch 11/200\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 5.2830 - accuracy: 0.7639 - val_loss: 5.0410 - val_accuracy: 0.8704\n",
            "Epoch 12/200\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 4.9959 - accuracy: 0.7731 - val_loss: 4.7600 - val_accuracy: 0.8704\n",
            "Epoch 13/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 4.7157 - accuracy: 0.8241 - val_loss: 4.4951 - val_accuracy: 0.8704\n",
            "Epoch 14/200\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 4.4888 - accuracy: 0.7778 - val_loss: 4.2519 - val_accuracy: 0.8704\n",
            "Epoch 15/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 4.1296 - accuracy: 0.8102 - val_loss: 4.0052 - val_accuracy: 0.8889\n",
            "Epoch 16/200\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 3.9735 - accuracy: 0.7685 - val_loss: 3.7896 - val_accuracy: 0.8889\n",
            "Epoch 17/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 3.7527 - accuracy: 0.7963 - val_loss: 3.5841 - val_accuracy: 0.8889\n",
            "Epoch 18/200\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 3.5463 - accuracy: 0.8056 - val_loss: 3.3813 - val_accuracy: 0.8889\n",
            "Epoch 19/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 3.3610 - accuracy: 0.8009 - val_loss: 3.1933 - val_accuracy: 0.8889\n",
            "Epoch 20/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 3.1183 - accuracy: 0.8241 - val_loss: 3.0197 - val_accuracy: 0.8889\n",
            "Epoch 21/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 2.9732 - accuracy: 0.8148 - val_loss: 2.8598 - val_accuracy: 0.8889\n",
            "Epoch 22/200\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 2.8149 - accuracy: 0.8009 - val_loss: 2.7099 - val_accuracy: 0.8889\n",
            "Epoch 23/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 2.7229 - accuracy: 0.7870 - val_loss: 2.5699 - val_accuracy: 0.8889\n",
            "Epoch 24/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 2.5148 - accuracy: 0.8194 - val_loss: 2.4385 - val_accuracy: 0.8889\n",
            "Epoch 25/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 2.4218 - accuracy: 0.8241 - val_loss: 2.3194 - val_accuracy: 0.8704\n",
            "Epoch 26/200\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 2.2779 - accuracy: 0.7963 - val_loss: 2.1986 - val_accuracy: 0.8704\n",
            "Epoch 27/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 2.1328 - accuracy: 0.8241 - val_loss: 2.0835 - val_accuracy: 0.8889\n",
            "Epoch 28/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 2.0533 - accuracy: 0.7870 - val_loss: 1.9767 - val_accuracy: 0.8889\n",
            "Epoch 29/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.9694 - accuracy: 0.8287 - val_loss: 1.8882 - val_accuracy: 0.8889\n",
            "Epoch 30/200\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 1.8618 - accuracy: 0.8194 - val_loss: 1.8019 - val_accuracy: 0.8704\n",
            "Epoch 31/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.7793 - accuracy: 0.8241 - val_loss: 1.7281 - val_accuracy: 0.8704\n",
            "Epoch 32/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.6553 - accuracy: 0.8565 - val_loss: 1.6549 - val_accuracy: 0.8704\n",
            "Epoch 33/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.5496 - accuracy: 0.8611 - val_loss: 1.5776 - val_accuracy: 0.8704\n",
            "Epoch 34/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.5558 - accuracy: 0.8472 - val_loss: 1.5182 - val_accuracy: 0.8704\n",
            "Epoch 35/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.5065 - accuracy: 0.8333 - val_loss: 1.4480 - val_accuracy: 0.8704\n",
            "Epoch 36/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 1.3809 - accuracy: 0.8194 - val_loss: 1.3890 - val_accuracy: 0.8889\n",
            "Epoch 37/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 1.3307 - accuracy: 0.8241 - val_loss: 1.3365 - val_accuracy: 0.8519\n",
            "Epoch 38/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 1.2654 - accuracy: 0.8472 - val_loss: 1.2839 - val_accuracy: 0.8519\n",
            "Epoch 39/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.2328 - accuracy: 0.8519 - val_loss: 1.2299 - val_accuracy: 0.8519\n",
            "Epoch 40/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 1.2048 - accuracy: 0.8333 - val_loss: 1.1928 - val_accuracy: 0.8519\n",
            "Epoch 41/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.1160 - accuracy: 0.8426 - val_loss: 1.1582 - val_accuracy: 0.8519\n",
            "Epoch 42/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 1.1488 - accuracy: 0.8102 - val_loss: 1.1310 - val_accuracy: 0.8519\n",
            "Epoch 43/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.1081 - accuracy: 0.8009 - val_loss: 1.0947 - val_accuracy: 0.8519\n",
            "Epoch 44/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 1.0410 - accuracy: 0.8241 - val_loss: 1.0556 - val_accuracy: 0.8704\n",
            "Epoch 45/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.9816 - accuracy: 0.8472 - val_loss: 1.0146 - val_accuracy: 0.8704\n",
            "Epoch 46/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.9511 - accuracy: 0.8380 - val_loss: 0.9825 - val_accuracy: 0.8889\n",
            "Epoch 47/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.9191 - accuracy: 0.8380 - val_loss: 0.9551 - val_accuracy: 0.8889\n",
            "Epoch 48/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.9283 - accuracy: 0.8333 - val_loss: 0.9245 - val_accuracy: 0.8889\n",
            "Epoch 49/200\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.8896 - accuracy: 0.8148 - val_loss: 0.8952 - val_accuracy: 0.8704\n",
            "Epoch 50/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.8309 - accuracy: 0.8333 - val_loss: 0.8851 - val_accuracy: 0.8889\n",
            "Epoch 51/200\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.8682 - accuracy: 0.8148 - val_loss: 0.8671 - val_accuracy: 0.8889\n",
            "Epoch 52/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.7894 - accuracy: 0.8426 - val_loss: 0.8417 - val_accuracy: 0.8889\n",
            "Epoch 53/200\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.7638 - accuracy: 0.8426 - val_loss: 0.8186 - val_accuracy: 0.8889\n",
            "Epoch 54/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.7458 - accuracy: 0.8565 - val_loss: 0.7974 - val_accuracy: 0.8889\n",
            "Epoch 55/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.7047 - accuracy: 0.8287 - val_loss: 0.7869 - val_accuracy: 0.8519\n",
            "Epoch 56/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.7139 - accuracy: 0.8519 - val_loss: 0.7746 - val_accuracy: 0.8704\n",
            "Epoch 57/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.7090 - accuracy: 0.8380 - val_loss: 0.7483 - val_accuracy: 0.8889\n",
            "Epoch 58/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.7068 - accuracy: 0.8519 - val_loss: 0.7416 - val_accuracy: 0.8519\n",
            "Epoch 59/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.6513 - accuracy: 0.8565 - val_loss: 0.7175 - val_accuracy: 0.8889\n",
            "Epoch 60/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.6411 - accuracy: 0.8426 - val_loss: 0.6922 - val_accuracy: 0.8889\n",
            "Epoch 61/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.6900 - accuracy: 0.8148 - val_loss: 0.6948 - val_accuracy: 0.8889\n",
            "Epoch 62/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.6553 - accuracy: 0.8287 - val_loss: 0.6859 - val_accuracy: 0.8704\n",
            "Epoch 63/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.5994 - accuracy: 0.8426 - val_loss: 0.6720 - val_accuracy: 0.8889\n",
            "Epoch 64/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.5961 - accuracy: 0.8380 - val_loss: 0.6601 - val_accuracy: 0.8889\n",
            "Epoch 65/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.5977 - accuracy: 0.8519 - val_loss: 0.6513 - val_accuracy: 0.8889\n",
            "Epoch 66/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.6041 - accuracy: 0.8565 - val_loss: 0.6340 - val_accuracy: 0.8889\n",
            "Epoch 67/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.5605 - accuracy: 0.8472 - val_loss: 0.6170 - val_accuracy: 0.8889\n",
            "Epoch 68/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.5636 - accuracy: 0.8611 - val_loss: 0.6191 - val_accuracy: 0.8889\n",
            "Epoch 69/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.6164 - accuracy: 0.8148 - val_loss: 0.6049 - val_accuracy: 0.8889\n",
            "Epoch 70/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.5259 - accuracy: 0.8565 - val_loss: 0.5991 - val_accuracy: 0.8889\n",
            "Epoch 71/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.5504 - accuracy: 0.8426 - val_loss: 0.5870 - val_accuracy: 0.8519\n",
            "Epoch 72/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.5535 - accuracy: 0.8519 - val_loss: 0.5734 - val_accuracy: 0.8704\n",
            "Epoch 73/200\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.5484 - accuracy: 0.8287 - val_loss: 0.5624 - val_accuracy: 0.8704\n",
            "Epoch 74/200\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.5242 - accuracy: 0.8426 - val_loss: 0.5472 - val_accuracy: 0.8519\n",
            "Epoch 75/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.5222 - accuracy: 0.8380 - val_loss: 0.5482 - val_accuracy: 0.8704\n",
            "Epoch 76/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4823 - accuracy: 0.8426 - val_loss: 0.5519 - val_accuracy: 0.8889\n",
            "Epoch 77/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.5119 - accuracy: 0.8472 - val_loss: 0.5746 - val_accuracy: 0.8889\n",
            "Epoch 78/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4726 - accuracy: 0.8472 - val_loss: 0.5488 - val_accuracy: 0.8889\n",
            "Epoch 79/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4802 - accuracy: 0.8611 - val_loss: 0.5375 - val_accuracy: 0.8889\n",
            "Epoch 80/200\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.4694 - accuracy: 0.8426 - val_loss: 0.5300 - val_accuracy: 0.8889\n",
            "Epoch 81/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.5092 - accuracy: 0.8287 - val_loss: 0.5129 - val_accuracy: 0.8704\n",
            "Epoch 82/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4720 - accuracy: 0.8657 - val_loss: 0.4971 - val_accuracy: 0.8704\n",
            "Epoch 83/200\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.4492 - accuracy: 0.8657 - val_loss: 0.5009 - val_accuracy: 0.8889\n",
            "Epoch 84/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.4428 - accuracy: 0.8611 - val_loss: 0.4921 - val_accuracy: 0.9074\n",
            "Epoch 85/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.5180 - accuracy: 0.8056 - val_loss: 0.4945 - val_accuracy: 0.8704\n",
            "Epoch 86/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4692 - accuracy: 0.8380 - val_loss: 0.4888 - val_accuracy: 0.8519\n",
            "Epoch 87/200\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.4267 - accuracy: 0.8657 - val_loss: 0.4727 - val_accuracy: 0.8519\n",
            "Epoch 88/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4614 - accuracy: 0.8519 - val_loss: 0.4691 - val_accuracy: 0.8889\n",
            "Epoch 89/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4168 - accuracy: 0.8657 - val_loss: 0.4752 - val_accuracy: 0.8889\n",
            "Epoch 90/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4731 - accuracy: 0.8611 - val_loss: 0.4667 - val_accuracy: 0.8519\n",
            "Epoch 91/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4731 - accuracy: 0.8565 - val_loss: 0.4618 - val_accuracy: 0.8519\n",
            "Epoch 92/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4243 - accuracy: 0.8565 - val_loss: 0.4532 - val_accuracy: 0.8519\n",
            "Epoch 93/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4284 - accuracy: 0.8380 - val_loss: 0.4539 - val_accuracy: 0.8704\n",
            "Epoch 94/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4759 - accuracy: 0.8333 - val_loss: 0.4566 - val_accuracy: 0.8704\n",
            "Epoch 95/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4376 - accuracy: 0.8472 - val_loss: 0.4577 - val_accuracy: 0.8889\n",
            "Epoch 96/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4568 - accuracy: 0.8519 - val_loss: 0.4438 - val_accuracy: 0.8889\n",
            "Epoch 97/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4610 - accuracy: 0.8426 - val_loss: 0.4328 - val_accuracy: 0.8704\n",
            "Epoch 98/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4377 - accuracy: 0.8287 - val_loss: 0.4360 - val_accuracy: 0.8519\n",
            "Epoch 99/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4422 - accuracy: 0.8750 - val_loss: 0.4366 - val_accuracy: 0.8889\n",
            "Epoch 100/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4368 - accuracy: 0.8704 - val_loss: 0.4357 - val_accuracy: 0.8519\n",
            "Epoch 101/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4242 - accuracy: 0.8472 - val_loss: 0.4431 - val_accuracy: 0.8889\n",
            "Epoch 102/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4247 - accuracy: 0.8472 - val_loss: 0.4380 - val_accuracy: 0.8889\n",
            "Epoch 103/200\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.4250 - accuracy: 0.8750 - val_loss: 0.4354 - val_accuracy: 0.8704\n",
            "Epoch 104/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3909 - accuracy: 0.8611 - val_loss: 0.4317 - val_accuracy: 0.8889\n",
            "Epoch 105/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4148 - accuracy: 0.8426 - val_loss: 0.4323 - val_accuracy: 0.8704\n",
            "Epoch 106/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4494 - accuracy: 0.8565 - val_loss: 0.4328 - val_accuracy: 0.8889\n",
            "Epoch 107/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3878 - accuracy: 0.8657 - val_loss: 0.4216 - val_accuracy: 0.8704\n",
            "Epoch 108/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3956 - accuracy: 0.8565 - val_loss: 0.4290 - val_accuracy: 0.8889\n",
            "Epoch 109/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4134 - accuracy: 0.8611 - val_loss: 0.4264 - val_accuracy: 0.8704\n",
            "Epoch 110/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3727 - accuracy: 0.8565 - val_loss: 0.4207 - val_accuracy: 0.8704\n",
            "Epoch 111/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4081 - accuracy: 0.8565 - val_loss: 0.4090 - val_accuracy: 0.8889\n",
            "Epoch 112/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4145 - accuracy: 0.8241 - val_loss: 0.4092 - val_accuracy: 0.9074\n",
            "Epoch 113/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4027 - accuracy: 0.8426 - val_loss: 0.4157 - val_accuracy: 0.8889\n",
            "Epoch 114/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4398 - accuracy: 0.8333 - val_loss: 0.4157 - val_accuracy: 0.8889\n",
            "Epoch 115/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4518 - accuracy: 0.8194 - val_loss: 0.4233 - val_accuracy: 0.8704\n",
            "Epoch 116/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.3752 - accuracy: 0.8519 - val_loss: 0.4102 - val_accuracy: 0.8704\n",
            "Epoch 117/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.4317 - accuracy: 0.8472 - val_loss: 0.4155 - val_accuracy: 0.8704\n",
            "Epoch 118/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.4232 - accuracy: 0.8472 - val_loss: 0.4178 - val_accuracy: 0.8704\n",
            "Epoch 119/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4358 - accuracy: 0.8241 - val_loss: 0.4066 - val_accuracy: 0.8889\n",
            "Epoch 120/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4115 - accuracy: 0.8519 - val_loss: 0.4097 - val_accuracy: 0.8704\n",
            "Epoch 121/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4174 - accuracy: 0.8333 - val_loss: 0.4250 - val_accuracy: 0.8889\n",
            "Epoch 122/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3858 - accuracy: 0.8519 - val_loss: 0.4189 - val_accuracy: 0.8704\n",
            "Epoch 123/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4282 - accuracy: 0.8380 - val_loss: 0.4177 - val_accuracy: 0.8704\n",
            "Epoch 124/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4071 - accuracy: 0.8611 - val_loss: 0.4117 - val_accuracy: 0.8704\n",
            "Epoch 125/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3978 - accuracy: 0.8611 - val_loss: 0.4157 - val_accuracy: 0.8704\n",
            "Epoch 126/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.3828 - accuracy: 0.8565 - val_loss: 0.4231 - val_accuracy: 0.8704\n",
            "Epoch 127/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.3913 - accuracy: 0.8519 - val_loss: 0.4115 - val_accuracy: 0.8889\n",
            "Epoch 128/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4054 - accuracy: 0.8657 - val_loss: 0.4066 - val_accuracy: 0.8889\n",
            "Epoch 129/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4048 - accuracy: 0.8565 - val_loss: 0.4084 - val_accuracy: 0.8704\n",
            "Epoch 130/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4054 - accuracy: 0.8565 - val_loss: 0.4091 - val_accuracy: 0.8704\n",
            "Epoch 131/200\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.4260 - accuracy: 0.8380 - val_loss: 0.4051 - val_accuracy: 0.8889\n",
            "Epoch 132/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4272 - accuracy: 0.8426 - val_loss: 0.4089 - val_accuracy: 0.9074\n",
            "Epoch 133/200\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.3525 - accuracy: 0.8611 - val_loss: 0.3940 - val_accuracy: 0.8889\n",
            "Epoch 134/200\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.4642 - accuracy: 0.8241 - val_loss: 0.4080 - val_accuracy: 0.9074\n",
            "Epoch 135/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.3620 - accuracy: 0.8611 - val_loss: 0.4067 - val_accuracy: 0.8704\n",
            "Epoch 136/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.3995 - accuracy: 0.8565 - val_loss: 0.4154 - val_accuracy: 0.8519\n",
            "Epoch 137/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.3759 - accuracy: 0.8519 - val_loss: 0.4043 - val_accuracy: 0.8704\n",
            "Epoch 138/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4024 - accuracy: 0.8519 - val_loss: 0.4020 - val_accuracy: 0.8704\n",
            "Epoch 139/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4041 - accuracy: 0.8287 - val_loss: 0.3920 - val_accuracy: 0.8889\n",
            "Epoch 140/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4130 - accuracy: 0.8565 - val_loss: 0.3914 - val_accuracy: 0.8519\n",
            "Epoch 141/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3987 - accuracy: 0.8426 - val_loss: 0.3876 - val_accuracy: 0.8704\n",
            "Epoch 142/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3998 - accuracy: 0.8519 - val_loss: 0.3844 - val_accuracy: 0.8889\n",
            "Epoch 143/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4048 - accuracy: 0.8380 - val_loss: 0.3933 - val_accuracy: 0.8704\n",
            "Epoch 144/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4108 - accuracy: 0.8426 - val_loss: 0.3922 - val_accuracy: 0.8704\n",
            "Epoch 145/200\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.4006 - accuracy: 0.8380 - val_loss: 0.3987 - val_accuracy: 0.8889\n",
            "Epoch 146/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.4056 - accuracy: 0.8472 - val_loss: 0.4028 - val_accuracy: 0.8889\n",
            "Epoch 147/200\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.3782 - accuracy: 0.8519 - val_loss: 0.4026 - val_accuracy: 0.9074\n",
            "Epoch 148/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4365 - accuracy: 0.8380 - val_loss: 0.3949 - val_accuracy: 0.9074\n",
            "Epoch 149/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4165 - accuracy: 0.8380 - val_loss: 0.4068 - val_accuracy: 0.8704\n",
            "Epoch 150/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3906 - accuracy: 0.8565 - val_loss: 0.4069 - val_accuracy: 0.8519\n",
            "Epoch 151/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3808 - accuracy: 0.8611 - val_loss: 0.3929 - val_accuracy: 0.8704\n",
            "Epoch 152/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.3817 - accuracy: 0.8611 - val_loss: 0.4022 - val_accuracy: 0.8519\n",
            "Epoch 153/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4132 - accuracy: 0.8472 - val_loss: 0.3964 - val_accuracy: 0.8519\n",
            "Epoch 154/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3969 - accuracy: 0.8472 - val_loss: 0.4072 - val_accuracy: 0.8519\n",
            "Epoch 155/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3901 - accuracy: 0.8611 - val_loss: 0.3907 - val_accuracy: 0.8704\n",
            "Epoch 156/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3917 - accuracy: 0.8519 - val_loss: 0.4013 - val_accuracy: 0.8519\n",
            "Epoch 157/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3917 - accuracy: 0.8380 - val_loss: 0.3951 - val_accuracy: 0.8519\n",
            "Epoch 158/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3979 - accuracy: 0.8102 - val_loss: 0.3970 - val_accuracy: 0.8519\n",
            "Epoch 159/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3911 - accuracy: 0.8657 - val_loss: 0.3959 - val_accuracy: 0.8704\n",
            "Epoch 160/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3866 - accuracy: 0.8472 - val_loss: 0.3937 - val_accuracy: 0.8519\n",
            "Epoch 161/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3832 - accuracy: 0.8565 - val_loss: 0.4051 - val_accuracy: 0.9074\n",
            "Epoch 162/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4098 - accuracy: 0.8472 - val_loss: 0.4025 - val_accuracy: 0.8889\n",
            "Epoch 163/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4040 - accuracy: 0.8426 - val_loss: 0.3956 - val_accuracy: 0.8704\n",
            "Epoch 164/200\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.3915 - accuracy: 0.8565 - val_loss: 0.4143 - val_accuracy: 0.8889\n",
            "Epoch 165/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.3886 - accuracy: 0.8519 - val_loss: 0.4040 - val_accuracy: 0.8889\n",
            "Epoch 166/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4004 - accuracy: 0.8472 - val_loss: 0.4169 - val_accuracy: 0.8519\n",
            "Epoch 167/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.3843 - accuracy: 0.8519 - val_loss: 0.4148 - val_accuracy: 0.8704\n",
            "Epoch 168/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3763 - accuracy: 0.8704 - val_loss: 0.4031 - val_accuracy: 0.8889\n",
            "Epoch 169/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.3807 - accuracy: 0.8565 - val_loss: 0.4163 - val_accuracy: 0.8704\n",
            "Epoch 170/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3634 - accuracy: 0.8426 - val_loss: 0.4258 - val_accuracy: 0.8704\n",
            "Epoch 171/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.3897 - accuracy: 0.8565 - val_loss: 0.4371 - val_accuracy: 0.8704\n",
            "Epoch 172/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3805 - accuracy: 0.8472 - val_loss: 0.4030 - val_accuracy: 0.8519\n",
            "Epoch 173/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.3740 - accuracy: 0.8611 - val_loss: 0.3920 - val_accuracy: 0.8889\n",
            "Epoch 174/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3724 - accuracy: 0.8472 - val_loss: 0.3940 - val_accuracy: 0.8889\n",
            "Epoch 175/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.4181 - accuracy: 0.8287 - val_loss: 0.4008 - val_accuracy: 0.8889\n",
            "Epoch 176/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3840 - accuracy: 0.8472 - val_loss: 0.4068 - val_accuracy: 0.8333\n",
            "Epoch 177/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3881 - accuracy: 0.8565 - val_loss: 0.3999 - val_accuracy: 0.8704\n",
            "Epoch 178/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3571 - accuracy: 0.8657 - val_loss: 0.4042 - val_accuracy: 0.8704\n",
            "Epoch 179/200\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.3884 - accuracy: 0.8611 - val_loss: 0.4129 - val_accuracy: 0.8704\n",
            "Epoch 180/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3573 - accuracy: 0.8704 - val_loss: 0.3957 - val_accuracy: 0.8889\n",
            "Epoch 181/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3922 - accuracy: 0.8287 - val_loss: 0.4093 - val_accuracy: 0.8889\n",
            "Epoch 182/200\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.4238 - accuracy: 0.8472 - val_loss: 0.4098 - val_accuracy: 0.8889\n",
            "Epoch 183/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4100 - accuracy: 0.8519 - val_loss: 0.4020 - val_accuracy: 0.8889\n",
            "Epoch 184/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4038 - accuracy: 0.8565 - val_loss: 0.4032 - val_accuracy: 0.8704\n",
            "Epoch 185/200\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.3750 - accuracy: 0.8426 - val_loss: 0.3988 - val_accuracy: 0.8519\n",
            "Epoch 186/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.3734 - accuracy: 0.8657 - val_loss: 0.4080 - val_accuracy: 0.8704\n",
            "Epoch 187/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.4254 - accuracy: 0.8472 - val_loss: 0.4147 - val_accuracy: 0.8704\n",
            "Epoch 188/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.3748 - accuracy: 0.8750 - val_loss: 0.4008 - val_accuracy: 0.8889\n",
            "Epoch 189/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3875 - accuracy: 0.8611 - val_loss: 0.3849 - val_accuracy: 0.8519\n",
            "Epoch 190/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4023 - accuracy: 0.8519 - val_loss: 0.4087 - val_accuracy: 0.8704\n",
            "Epoch 191/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4106 - accuracy: 0.8472 - val_loss: 0.4047 - val_accuracy: 0.8704\n",
            "Epoch 192/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3789 - accuracy: 0.8519 - val_loss: 0.4084 - val_accuracy: 0.8519\n",
            "Epoch 193/200\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.4087 - accuracy: 0.8519 - val_loss: 0.4176 - val_accuracy: 0.8704\n",
            "Epoch 194/200\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.3910 - accuracy: 0.8333 - val_loss: 0.3991 - val_accuracy: 0.8333\n",
            "Epoch 195/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.3941 - accuracy: 0.8657 - val_loss: 0.4053 - val_accuracy: 0.8333\n",
            "Epoch 196/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3788 - accuracy: 0.8472 - val_loss: 0.4067 - val_accuracy: 0.8519\n",
            "Epoch 197/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3864 - accuracy: 0.8657 - val_loss: 0.4031 - val_accuracy: 0.8704\n",
            "Epoch 198/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4021 - accuracy: 0.8472 - val_loss: 0.3953 - val_accuracy: 0.8519\n",
            "Epoch 199/200\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3740 - accuracy: 0.8519 - val_loss: 0.4011 - val_accuracy: 0.8889\n",
            "Epoch 200/200\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.3820 - accuracy: 0.8333 - val_loss: 0.4066 - val_accuracy: 0.8704\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ici j'ai fait plein de tests pour voir combien de couches faire et deux couches de 32 neurones ont l'air d'être les meilleures,\n",
        "et adam est meilleur que rmsprop"
      ],
      "metadata": {
        "id": "x-nJUf4XctFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_f=keras.Sequential([\n",
        "    keras.layers.Dense(32, activation='elu',kernel_regularizer= keras.regularizers.l2(0.1)),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dropout(0.4),\n",
        "    keras.layers.Dense(32, activation='elu',kernel_regularizer= keras.regularizers.l2(0.1)),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dropout(0.4),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "model_f.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit model\n",
        "es = EarlyStopping(monitor='val_accuracy', mode='auto', verbose=0,patience=50)\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
        "original_hist_f = model_f.fit(trainX, trainy, validation_data=(testX, testy), epochs=500,callbacks=[es, mc],verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbynuvmFUIXs",
        "outputId": "d1837768-f2ee-4dc3-d852-980feecdda8a"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "1/7 [===>..........................] - ETA: 14s - loss: 6.1129 - accuracy: 0.4062\n",
            "Epoch 1: val_accuracy improved from -inf to 0.38889, saving model to best_model.h5\n",
            "7/7 [==============================] - 3s 49ms/step - loss: 6.0664 - accuracy: 0.4074 - val_loss: 5.6042 - val_accuracy: 0.3889\n",
            "Epoch 2/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 5.7073 - accuracy: 0.4375\n",
            "Epoch 2: val_accuracy improved from 0.38889 to 0.55556, saving model to best_model.h5\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 5.7375 - accuracy: 0.4815 - val_loss: 5.3054 - val_accuracy: 0.5556\n",
            "Epoch 3/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 5.5202 - accuracy: 0.5000\n",
            "Epoch 3: val_accuracy improved from 0.55556 to 0.72222, saving model to best_model.h5\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 5.3187 - accuracy: 0.6065 - val_loss: 5.0387 - val_accuracy: 0.7222\n",
            "Epoch 4/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 5.0258 - accuracy: 0.6875\n",
            "Epoch 4: val_accuracy improved from 0.72222 to 0.75926, saving model to best_model.h5\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 5.1363 - accuracy: 0.5833 - val_loss: 4.7939 - val_accuracy: 0.7593\n",
            "Epoch 5/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 4.9196 - accuracy: 0.5625\n",
            "Epoch 5: val_accuracy improved from 0.75926 to 0.77778, saving model to best_model.h5\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 4.9139 - accuracy: 0.6065 - val_loss: 4.5652 - val_accuracy: 0.7778\n",
            "Epoch 6/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 4.5485 - accuracy: 0.7500\n",
            "Epoch 6: val_accuracy did not improve from 0.77778\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 4.6449 - accuracy: 0.6528 - val_loss: 4.3532 - val_accuracy: 0.7778\n",
            "Epoch 7/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 4.5337 - accuracy: 0.6875\n",
            "Epoch 7: val_accuracy did not improve from 0.77778\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 4.5042 - accuracy: 0.6435 - val_loss: 4.1535 - val_accuracy: 0.7778\n",
            "Epoch 8/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 4.4162 - accuracy: 0.6250\n",
            "Epoch 8: val_accuracy improved from 0.77778 to 0.81481, saving model to best_model.h5\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 4.2813 - accuracy: 0.6852 - val_loss: 3.9636 - val_accuracy: 0.8148\n",
            "Epoch 9/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 3.9691 - accuracy: 0.7500\n",
            "Epoch 9: val_accuracy did not improve from 0.81481\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 3.9711 - accuracy: 0.6944 - val_loss: 3.7845 - val_accuracy: 0.8148\n",
            "Epoch 10/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 3.7335 - accuracy: 0.8438\n",
            "Epoch 10: val_accuracy improved from 0.81481 to 0.83333, saving model to best_model.h5\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 3.8119 - accuracy: 0.7130 - val_loss: 3.6142 - val_accuracy: 0.8333\n",
            "Epoch 11/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 3.6918 - accuracy: 0.7500\n",
            "Epoch 11: val_accuracy did not improve from 0.83333\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 3.6464 - accuracy: 0.7731 - val_loss: 3.4520 - val_accuracy: 0.8148\n",
            "Epoch 12/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 3.5861 - accuracy: 0.8438\n",
            "Epoch 12: val_accuracy did not improve from 0.83333\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 3.5044 - accuracy: 0.7454 - val_loss: 3.2963 - val_accuracy: 0.8148\n",
            "Epoch 13/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 3.4413 - accuracy: 0.7500\n",
            "Epoch 13: val_accuracy did not improve from 0.83333\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 3.3720 - accuracy: 0.7315 - val_loss: 3.1500 - val_accuracy: 0.8333\n",
            "Epoch 14/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 3.1573 - accuracy: 0.8438\n",
            "Epoch 14: val_accuracy did not improve from 0.83333\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 3.2104 - accuracy: 0.7361 - val_loss: 3.0106 - val_accuracy: 0.8333\n",
            "Epoch 15/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 3.0021 - accuracy: 0.8125\n",
            "Epoch 15: val_accuracy did not improve from 0.83333\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 3.0396 - accuracy: 0.7546 - val_loss: 2.8755 - val_accuracy: 0.8333\n",
            "Epoch 16/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 2.9698 - accuracy: 0.7500\n",
            "Epoch 16: val_accuracy did not improve from 0.83333\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 2.9728 - accuracy: 0.7454 - val_loss: 2.7482 - val_accuracy: 0.8333\n",
            "Epoch 17/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 2.8057 - accuracy: 0.7188\n",
            "Epoch 17: val_accuracy did not improve from 0.83333\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 2.7760 - accuracy: 0.7824 - val_loss: 2.6278 - val_accuracy: 0.8333\n",
            "Epoch 18/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 2.5215 - accuracy: 0.8750\n",
            "Epoch 18: val_accuracy did not improve from 0.83333\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 2.6441 - accuracy: 0.7870 - val_loss: 2.5119 - val_accuracy: 0.8333\n",
            "Epoch 19/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 2.6195 - accuracy: 0.7500\n",
            "Epoch 19: val_accuracy improved from 0.83333 to 0.87037, saving model to best_model.h5\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 2.5556 - accuracy: 0.7731 - val_loss: 2.4001 - val_accuracy: 0.8704\n",
            "Epoch 20/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 2.8759 - accuracy: 0.6875\n",
            "Epoch 20: val_accuracy did not improve from 0.87037\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 2.4354 - accuracy: 0.7917 - val_loss: 2.2965 - val_accuracy: 0.8704\n",
            "Epoch 21/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 2.3096 - accuracy: 0.7812\n",
            "Epoch 21: val_accuracy did not improve from 0.87037\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 2.3201 - accuracy: 0.7917 - val_loss: 2.1968 - val_accuracy: 0.8704\n",
            "Epoch 22/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 2.1939 - accuracy: 0.7500\n",
            "Epoch 22: val_accuracy did not improve from 0.87037\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 2.2300 - accuracy: 0.7917 - val_loss: 2.1006 - val_accuracy: 0.8519\n",
            "Epoch 23/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 2.0118 - accuracy: 0.9062\n",
            "Epoch 23: val_accuracy did not improve from 0.87037\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 2.0688 - accuracy: 0.8426 - val_loss: 2.0116 - val_accuracy: 0.8519\n",
            "Epoch 24/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 1.9558 - accuracy: 0.9062\n",
            "Epoch 24: val_accuracy did not improve from 0.87037\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 2.0253 - accuracy: 0.8333 - val_loss: 1.9273 - val_accuracy: 0.8519\n",
            "Epoch 25/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 1.8248 - accuracy: 0.8750\n",
            "Epoch 25: val_accuracy did not improve from 0.87037\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.9689 - accuracy: 0.7963 - val_loss: 1.8466 - val_accuracy: 0.8519\n",
            "Epoch 26/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 1.9150 - accuracy: 0.7188\n",
            "Epoch 26: val_accuracy did not improve from 0.87037\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 1.8780 - accuracy: 0.8009 - val_loss: 1.7739 - val_accuracy: 0.8519\n",
            "Epoch 27/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 1.6822 - accuracy: 0.8438\n",
            "Epoch 27: val_accuracy did not improve from 0.87037\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.8396 - accuracy: 0.7917 - val_loss: 1.7016 - val_accuracy: 0.8519\n",
            "Epoch 28/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 1.7137 - accuracy: 0.7812\n",
            "Epoch 28: val_accuracy did not improve from 0.87037\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.7587 - accuracy: 0.7407 - val_loss: 1.6329 - val_accuracy: 0.8704\n",
            "Epoch 29/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 1.5522 - accuracy: 0.9062\n",
            "Epoch 29: val_accuracy did not improve from 0.87037\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 1.6922 - accuracy: 0.8009 - val_loss: 1.5683 - val_accuracy: 0.8704\n",
            "Epoch 30/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 1.6906 - accuracy: 0.7812\n",
            "Epoch 30: val_accuracy did not improve from 0.87037\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 1.5698 - accuracy: 0.8287 - val_loss: 1.5079 - val_accuracy: 0.8704\n",
            "Epoch 31/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 1.4942 - accuracy: 0.8438\n",
            "Epoch 31: val_accuracy did not improve from 0.87037\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.5343 - accuracy: 0.8148 - val_loss: 1.4523 - val_accuracy: 0.8704\n",
            "Epoch 32/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 1.4874 - accuracy: 0.8438\n",
            "Epoch 32: val_accuracy did not improve from 0.87037\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 1.4493 - accuracy: 0.8148 - val_loss: 1.3977 - val_accuracy: 0.8704\n",
            "Epoch 33/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 1.4549 - accuracy: 0.8125\n",
            "Epoch 33: val_accuracy did not improve from 0.87037\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.4326 - accuracy: 0.8194 - val_loss: 1.3459 - val_accuracy: 0.8704\n",
            "Epoch 34/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 1.3773 - accuracy: 0.7500\n",
            "Epoch 34: val_accuracy did not improve from 0.87037\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 1.3267 - accuracy: 0.8102 - val_loss: 1.2958 - val_accuracy: 0.8704\n",
            "Epoch 35/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 1.3494 - accuracy: 0.8125\n",
            "Epoch 35: val_accuracy did not improve from 0.87037\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 1.2562 - accuracy: 0.8426 - val_loss: 1.2503 - val_accuracy: 0.8704\n",
            "Epoch 36/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 1.1438 - accuracy: 0.9375\n",
            "Epoch 36: val_accuracy did not improve from 0.87037\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 1.2086 - accuracy: 0.8380 - val_loss: 1.2073 - val_accuracy: 0.8704\n",
            "Epoch 37/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 1.0485 - accuracy: 0.9062\n",
            "Epoch 37: val_accuracy did not improve from 0.87037\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 1.1770 - accuracy: 0.8472 - val_loss: 1.1636 - val_accuracy: 0.8704\n",
            "Epoch 38/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.9903 - accuracy: 0.9062\n",
            "Epoch 38: val_accuracy improved from 0.87037 to 0.88889, saving model to best_model.h5\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 1.1357 - accuracy: 0.8241 - val_loss: 1.1218 - val_accuracy: 0.8889\n",
            "Epoch 39/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 1.1195 - accuracy: 0.7500\n",
            "Epoch 39: val_accuracy did not improve from 0.88889\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 1.1238 - accuracy: 0.8056 - val_loss: 1.0821 - val_accuracy: 0.8889\n",
            "Epoch 40/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 1.0610 - accuracy: 0.8125\n",
            "Epoch 40: val_accuracy did not improve from 0.88889\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 1.1029 - accuracy: 0.8241 - val_loss: 1.0514 - val_accuracy: 0.8704\n",
            "Epoch 41/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 1.1629 - accuracy: 0.7812\n",
            "Epoch 41: val_accuracy did not improve from 0.88889\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 1.0705 - accuracy: 0.8102 - val_loss: 1.0202 - val_accuracy: 0.8704\n",
            "Epoch 42/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 1.1615 - accuracy: 0.8125\n",
            "Epoch 42: val_accuracy did not improve from 0.88889\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.9855 - accuracy: 0.8472 - val_loss: 0.9906 - val_accuracy: 0.8704\n",
            "Epoch 43/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 1.0402 - accuracy: 0.8125\n",
            "Epoch 43: val_accuracy did not improve from 0.88889\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.9342 - accuracy: 0.8380 - val_loss: 0.9605 - val_accuracy: 0.8704\n",
            "Epoch 44/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.9665 - accuracy: 0.8125\n",
            "Epoch 44: val_accuracy did not improve from 0.88889\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.9583 - accuracy: 0.8426 - val_loss: 0.9283 - val_accuracy: 0.8889\n",
            "Epoch 45/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.9683 - accuracy: 0.8438\n",
            "Epoch 45: val_accuracy did not improve from 0.88889\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.9941 - accuracy: 0.8287 - val_loss: 0.9046 - val_accuracy: 0.8704\n",
            "Epoch 46/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.7818 - accuracy: 0.8438\n",
            "Epoch 46: val_accuracy did not improve from 0.88889\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.9364 - accuracy: 0.8194 - val_loss: 0.8808 - val_accuracy: 0.8889\n",
            "Epoch 47/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 1.0842 - accuracy: 0.7500\n",
            "Epoch 47: val_accuracy did not improve from 0.88889\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.8483 - accuracy: 0.8565 - val_loss: 0.8545 - val_accuracy: 0.8889\n",
            "Epoch 48/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.8207 - accuracy: 0.8750\n",
            "Epoch 48: val_accuracy did not improve from 0.88889\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.8000 - accuracy: 0.8611 - val_loss: 0.8300 - val_accuracy: 0.8889\n",
            "Epoch 49/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.7654 - accuracy: 0.9062\n",
            "Epoch 49: val_accuracy did not improve from 0.88889\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.8308 - accuracy: 0.8287 - val_loss: 0.8065 - val_accuracy: 0.8889\n",
            "Epoch 50/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.7521 - accuracy: 0.8750\n",
            "Epoch 50: val_accuracy did not improve from 0.88889\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.8888 - accuracy: 0.8056 - val_loss: 0.7895 - val_accuracy: 0.8704\n",
            "Epoch 51/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.7733 - accuracy: 0.8125\n",
            "Epoch 51: val_accuracy did not improve from 0.88889\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.7818 - accuracy: 0.8287 - val_loss: 0.7705 - val_accuracy: 0.8889\n",
            "Epoch 52/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.6688 - accuracy: 0.9062\n",
            "Epoch 52: val_accuracy did not improve from 0.88889\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.7609 - accuracy: 0.8426 - val_loss: 0.7577 - val_accuracy: 0.8889\n",
            "Epoch 53/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.9058 - accuracy: 0.7812\n",
            "Epoch 53: val_accuracy did not improve from 0.88889\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.7062 - accuracy: 0.8333 - val_loss: 0.7398 - val_accuracy: 0.8704\n",
            "Epoch 54/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.7793 - accuracy: 0.7500\n",
            "Epoch 54: val_accuracy did not improve from 0.88889\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.7628 - accuracy: 0.8056 - val_loss: 0.7276 - val_accuracy: 0.8704\n",
            "Epoch 55/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.5711 - accuracy: 0.9062\n",
            "Epoch 55: val_accuracy did not improve from 0.88889\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.7006 - accuracy: 0.8611 - val_loss: 0.7158 - val_accuracy: 0.8704\n",
            "Epoch 56/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.9257 - accuracy: 0.6562\n",
            "Epoch 56: val_accuracy did not improve from 0.88889\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.7172 - accuracy: 0.8148 - val_loss: 0.7063 - val_accuracy: 0.8889\n",
            "Epoch 57/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.5656 - accuracy: 0.8750\n",
            "Epoch 57: val_accuracy did not improve from 0.88889\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.6765 - accuracy: 0.8241 - val_loss: 0.6852 - val_accuracy: 0.8704\n",
            "Epoch 58/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.5713 - accuracy: 0.9062\n",
            "Epoch 58: val_accuracy did not improve from 0.88889\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.7007 - accuracy: 0.8287 - val_loss: 0.6706 - val_accuracy: 0.8704\n",
            "Epoch 59/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.3620 - accuracy: 1.0000\n",
            "Epoch 59: val_accuracy did not improve from 0.88889\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.6281 - accuracy: 0.8519 - val_loss: 0.6564 - val_accuracy: 0.8704\n",
            "Epoch 60/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.5548 - accuracy: 0.8750\n",
            "Epoch 60: val_accuracy did not improve from 0.88889\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.6799 - accuracy: 0.8333 - val_loss: 0.6469 - val_accuracy: 0.8704\n",
            "Epoch 61/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.4489 - accuracy: 0.9062\n",
            "Epoch 61: val_accuracy did not improve from 0.88889\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.6324 - accuracy: 0.8056 - val_loss: 0.6343 - val_accuracy: 0.8519\n",
            "Epoch 62/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.5171 - accuracy: 0.9062\n",
            "Epoch 62: val_accuracy did not improve from 0.88889\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.6402 - accuracy: 0.8148 - val_loss: 0.6229 - val_accuracy: 0.8704\n",
            "Epoch 63/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.5907 - accuracy: 0.9062\n",
            "Epoch 63: val_accuracy did not improve from 0.88889\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.6193 - accuracy: 0.8657 - val_loss: 0.6192 - val_accuracy: 0.8704\n",
            "Epoch 64/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.7061 - accuracy: 0.7500\n",
            "Epoch 64: val_accuracy did not improve from 0.88889\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.6400 - accuracy: 0.7917 - val_loss: 0.6105 - val_accuracy: 0.8704\n",
            "Epoch 65/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.4038 - accuracy: 0.9062\n",
            "Epoch 65: val_accuracy did not improve from 0.88889\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.6136 - accuracy: 0.8287 - val_loss: 0.6037 - val_accuracy: 0.8889\n",
            "Epoch 66/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.6603 - accuracy: 0.8438\n",
            "Epoch 66: val_accuracy did not improve from 0.88889\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.5939 - accuracy: 0.8426 - val_loss: 0.6033 - val_accuracy: 0.8704\n",
            "Epoch 67/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.4965 - accuracy: 0.8750\n",
            "Epoch 67: val_accuracy did not improve from 0.88889\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.5569 - accuracy: 0.8102 - val_loss: 0.6001 - val_accuracy: 0.8889\n",
            "Epoch 68/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.3797 - accuracy: 0.9375\n",
            "Epoch 68: val_accuracy improved from 0.88889 to 0.90741, saving model to best_model.h5\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.5670 - accuracy: 0.8426 - val_loss: 0.5858 - val_accuracy: 0.9074\n",
            "Epoch 69/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.5305 - accuracy: 0.7812\n",
            "Epoch 69: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.5363 - accuracy: 0.8565 - val_loss: 0.5680 - val_accuracy: 0.9074\n",
            "Epoch 70/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.6917 - accuracy: 0.8438\n",
            "Epoch 70: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.5193 - accuracy: 0.8565 - val_loss: 0.5562 - val_accuracy: 0.9074\n",
            "Epoch 71/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.3202 - accuracy: 0.9375\n",
            "Epoch 71: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4928 - accuracy: 0.8472 - val_loss: 0.5370 - val_accuracy: 0.9074\n",
            "Epoch 72/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.5688 - accuracy: 0.8125\n",
            "Epoch 72: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.5354 - accuracy: 0.8519 - val_loss: 0.5221 - val_accuracy: 0.9074\n",
            "Epoch 73/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.5973 - accuracy: 0.8125\n",
            "Epoch 73: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.4821 - accuracy: 0.8657 - val_loss: 0.5200 - val_accuracy: 0.9074\n",
            "Epoch 74/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.7118 - accuracy: 0.7500\n",
            "Epoch 74: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.5656 - accuracy: 0.8148 - val_loss: 0.5191 - val_accuracy: 0.9074\n",
            "Epoch 75/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.5157 - accuracy: 0.8438\n",
            "Epoch 75: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.5033 - accuracy: 0.8519 - val_loss: 0.5217 - val_accuracy: 0.8889\n",
            "Epoch 76/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.4466 - accuracy: 0.9062\n",
            "Epoch 76: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4920 - accuracy: 0.8565 - val_loss: 0.5146 - val_accuracy: 0.8704\n",
            "Epoch 77/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.4696 - accuracy: 0.8125\n",
            "Epoch 77: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.5039 - accuracy: 0.8333 - val_loss: 0.5072 - val_accuracy: 0.8889\n",
            "Epoch 78/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.5827 - accuracy: 0.8125\n",
            "Epoch 78: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.5056 - accuracy: 0.8519 - val_loss: 0.5039 - val_accuracy: 0.8519\n",
            "Epoch 79/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.4641 - accuracy: 0.8750\n",
            "Epoch 79: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4830 - accuracy: 0.8472 - val_loss: 0.4963 - val_accuracy: 0.8519\n",
            "Epoch 80/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.4180 - accuracy: 0.9688\n",
            "Epoch 80: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4784 - accuracy: 0.8704 - val_loss: 0.4833 - val_accuracy: 0.8704\n",
            "Epoch 81/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.4510 - accuracy: 0.8438\n",
            "Epoch 81: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4362 - accuracy: 0.8704 - val_loss: 0.4799 - val_accuracy: 0.8704\n",
            "Epoch 82/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.4870 - accuracy: 0.9062\n",
            "Epoch 82: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4966 - accuracy: 0.8241 - val_loss: 0.4789 - val_accuracy: 0.8519\n",
            "Epoch 83/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.4909 - accuracy: 0.8438\n",
            "Epoch 83: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4407 - accuracy: 0.8704 - val_loss: 0.4791 - val_accuracy: 0.8519\n",
            "Epoch 84/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.4569 - accuracy: 0.7812\n",
            "Epoch 84: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4950 - accuracy: 0.8102 - val_loss: 0.4792 - val_accuracy: 0.8519\n",
            "Epoch 85/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.4056 - accuracy: 0.9062\n",
            "Epoch 85: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4416 - accuracy: 0.8519 - val_loss: 0.4794 - val_accuracy: 0.8704\n",
            "Epoch 86/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.4768 - accuracy: 0.8750\n",
            "Epoch 86: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4601 - accuracy: 0.8611 - val_loss: 0.4750 - val_accuracy: 0.8704\n",
            "Epoch 87/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.4326 - accuracy: 0.9062\n",
            "Epoch 87: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4853 - accuracy: 0.8657 - val_loss: 0.4674 - val_accuracy: 0.8704\n",
            "Epoch 88/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.5658 - accuracy: 0.7812\n",
            "Epoch 88: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 0.4673 - accuracy: 0.8426 - val_loss: 0.4612 - val_accuracy: 0.8519\n",
            "Epoch 89/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.5823 - accuracy: 0.8125\n",
            "Epoch 89: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4449 - accuracy: 0.8472 - val_loss: 0.4604 - val_accuracy: 0.8704\n",
            "Epoch 90/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.4016 - accuracy: 0.8750\n",
            "Epoch 90: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.4761 - accuracy: 0.8194 - val_loss: 0.4651 - val_accuracy: 0.8889\n",
            "Epoch 91/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.5872 - accuracy: 0.7500\n",
            "Epoch 91: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4196 - accuracy: 0.8565 - val_loss: 0.4664 - val_accuracy: 0.8889\n",
            "Epoch 92/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.3199 - accuracy: 0.9062\n",
            "Epoch 92: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4344 - accuracy: 0.8472 - val_loss: 0.4589 - val_accuracy: 0.8704\n",
            "Epoch 93/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.5813 - accuracy: 0.7500\n",
            "Epoch 93: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.4710 - accuracy: 0.8241 - val_loss: 0.4546 - val_accuracy: 0.8704\n",
            "Epoch 94/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.4222 - accuracy: 0.8125\n",
            "Epoch 94: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4157 - accuracy: 0.8519 - val_loss: 0.4572 - val_accuracy: 0.8704\n",
            "Epoch 95/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.3232 - accuracy: 0.8750\n",
            "Epoch 95: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3938 - accuracy: 0.8519 - val_loss: 0.4582 - val_accuracy: 0.8704\n",
            "Epoch 96/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.4115 - accuracy: 0.8438\n",
            "Epoch 96: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4132 - accuracy: 0.8750 - val_loss: 0.4457 - val_accuracy: 0.8519\n",
            "Epoch 97/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.3816 - accuracy: 0.9375\n",
            "Epoch 97: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4160 - accuracy: 0.8565 - val_loss: 0.4347 - val_accuracy: 0.8519\n",
            "Epoch 98/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.3336 - accuracy: 0.8750\n",
            "Epoch 98: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 0.4053 - accuracy: 0.8750 - val_loss: 0.4304 - val_accuracy: 0.8519\n",
            "Epoch 99/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.6216 - accuracy: 0.7812\n",
            "Epoch 99: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4181 - accuracy: 0.8519 - val_loss: 0.4327 - val_accuracy: 0.8704\n",
            "Epoch 100/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.3290 - accuracy: 0.9062\n",
            "Epoch 100: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3789 - accuracy: 0.8519 - val_loss: 0.4385 - val_accuracy: 0.8519\n",
            "Epoch 101/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.6211 - accuracy: 0.7812\n",
            "Epoch 101: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4121 - accuracy: 0.8565 - val_loss: 0.4354 - val_accuracy: 0.8519\n",
            "Epoch 102/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.4235 - accuracy: 0.8438\n",
            "Epoch 102: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.3798 - accuracy: 0.8565 - val_loss: 0.4293 - val_accuracy: 0.8519\n",
            "Epoch 103/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.3906 - accuracy: 0.8750\n",
            "Epoch 103: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3861 - accuracy: 0.8519 - val_loss: 0.4209 - val_accuracy: 0.8519\n",
            "Epoch 104/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.3967 - accuracy: 0.8438\n",
            "Epoch 104: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4106 - accuracy: 0.8611 - val_loss: 0.4300 - val_accuracy: 0.8704\n",
            "Epoch 105/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.3011 - accuracy: 0.9062\n",
            "Epoch 105: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.4506 - accuracy: 0.8194 - val_loss: 0.4386 - val_accuracy: 0.8704\n",
            "Epoch 106/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.2167 - accuracy: 0.9688\n",
            "Epoch 106: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.3738 - accuracy: 0.8611 - val_loss: 0.4356 - val_accuracy: 0.8704\n",
            "Epoch 107/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.3330 - accuracy: 0.8750\n",
            "Epoch 107: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.3737 - accuracy: 0.8565 - val_loss: 0.4285 - val_accuracy: 0.8704\n",
            "Epoch 108/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.3364 - accuracy: 0.8750\n",
            "Epoch 108: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4264 - accuracy: 0.8241 - val_loss: 0.4237 - val_accuracy: 0.8704\n",
            "Epoch 109/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.4435 - accuracy: 0.8438\n",
            "Epoch 109: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4137 - accuracy: 0.8657 - val_loss: 0.4253 - val_accuracy: 0.8704\n",
            "Epoch 110/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.4536 - accuracy: 0.8125\n",
            "Epoch 110: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.3937 - accuracy: 0.8657 - val_loss: 0.4123 - val_accuracy: 0.8889\n",
            "Epoch 111/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.6562 - accuracy: 0.7812\n",
            "Epoch 111: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.4139 - accuracy: 0.8472 - val_loss: 0.4128 - val_accuracy: 0.8889\n",
            "Epoch 112/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.2699 - accuracy: 0.9062\n",
            "Epoch 112: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.3769 - accuracy: 0.8796 - val_loss: 0.4172 - val_accuracy: 0.8704\n",
            "Epoch 113/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.4300 - accuracy: 0.8125\n",
            "Epoch 113: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4161 - accuracy: 0.8565 - val_loss: 0.4183 - val_accuracy: 0.8519\n",
            "Epoch 114/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.4359 - accuracy: 0.8125\n",
            "Epoch 114: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.3921 - accuracy: 0.8657 - val_loss: 0.4197 - val_accuracy: 0.8519\n",
            "Epoch 115/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.3662 - accuracy: 0.8750\n",
            "Epoch 115: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4184 - accuracy: 0.8426 - val_loss: 0.4186 - val_accuracy: 0.8519\n",
            "Epoch 116/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.4286 - accuracy: 0.7500\n",
            "Epoch 116: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 0.3692 - accuracy: 0.8657 - val_loss: 0.4115 - val_accuracy: 0.8704\n",
            "Epoch 117/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.2954 - accuracy: 0.8750\n",
            "Epoch 117: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.4260 - accuracy: 0.8380 - val_loss: 0.4041 - val_accuracy: 0.8519\n",
            "Epoch 118/500\n",
            "1/7 [===>..........................] - ETA: 0s - loss: 0.2005 - accuracy: 0.9375\n",
            "Epoch 118: val_accuracy did not improve from 0.90741\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 0.4057 - accuracy: 0.8426 - val_loss: 0.4053 - val_accuracy: 0.8519\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "le meilleur modèle est obtenu à l'époque 68\n",
        "val_loss=0.5858\n",
        "val_accuracy=0.9074"
      ],
      "metadata": {
        "id": "51mH1Cscczgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "saved_model = load_model('best_model.h5')"
      ],
      "metadata": {
        "id": "vrZd1n1sV_fG"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def smooth_curve(points, factor=0.9):\n",
        "  smoothed_points = []\n",
        "  for point in points:\n",
        "    if smoothed_points:\n",
        "      previous = smoothed_points[-1]\n",
        "      smoothed_points.append(previous * factor + point * (1 - factor))\n",
        "    else:\n",
        "      smoothed_points.append(point)\n",
        "  return smoothed_points"
      ],
      "metadata": {
        "id": "23YlAq2jWdpU"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(smooth_curve(original_hist_f.history['loss']), label='train')\n",
        "plt.plot(smooth_curve(original_hist_f.history['val_loss']), label='test')\n",
        "plt.title('loss graph', pad=-80)\n",
        "plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "vVFtULitXC8R",
        "outputId": "7d3e4883-db45-4433-c7c1-c9ff80945c71"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fde6647ae50>"
            ]
          },
          "metadata": {},
          "execution_count": 123
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(smooth_curve(original_hist_f.history['accuracy']), label='train')\n",
        "plt.plot(smooth_curve(original_hist_f.history['val_accuracy']), label='test')\n",
        "plt.title('accuracy graph', pad=-80)\n",
        "plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "9pNYNnyRXYwJ",
        "outputId": "16a0c387-ac9f-4755-cfac-d667a194ba50"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fde74b832d0>"
            ]
          },
          "metadata": {},
          "execution_count": 124
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "sur les deux graphes, on peut remarquer qu'il n'y a pas d'over fitting car le training n'a pas de meilleur résultat en gros écart avec le test"
      ],
      "metadata": {
        "id": "HQFEjlj8c4p2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prédiction finale"
      ],
      "metadata": {
        "id": "kdK-9RZzcPvC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "val_loss=0.5858\n",
        "val_accuracy=0.9074\n",
        "(val_loss=0.4041 à l'époque 117)"
      ],
      "metadata": {
        "id": "gyhTbcauc8nF"
      }
    }
  ]
}